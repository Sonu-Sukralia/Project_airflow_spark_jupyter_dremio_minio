[2025-12-09T22:13:35.080+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: amount_data_s3_processing_pipeline.wait_for_json_files_in_minio manual__2025-12-09T22:13:33.168137+00:00 [queued]>
[2025-12-09T22:13:35.089+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: amount_data_s3_processing_pipeline.wait_for_json_files_in_minio manual__2025-12-09T22:13:33.168137+00:00 [queued]>
[2025-12-09T22:13:35.089+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 3
[2025-12-09T22:13:35.098+0000] {taskinstance.py:1380} INFO - Executing <Task(PythonSensor): wait_for_json_files_in_minio> on 2025-12-09 22:13:33.168137+00:00
[2025-12-09T22:13:35.103+0000] {standard_task_runner.py:57} INFO - Started process 300 to run task
[2025-12-09T22:13:35.106+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'amount_data_s3_processing_pipeline', 'wait_for_json_files_in_minio', 'manual__2025-12-09T22:13:33.168137+00:00', '--job-id', '61', '--raw', '--subdir', 'DAGS_FOLDER/amount_data_s3_processing_pipeline.py', '--cfg-path', '/tmp/tmpddahlzjt']
[2025-12-09T22:13:35.109+0000] {standard_task_runner.py:85} INFO - Job 61: Subtask wait_for_json_files_in_minio
[2025-12-09T22:13:35.157+0000] {task_command.py:415} INFO - Running <TaskInstance: amount_data_s3_processing_pipeline.wait_for_json_files_in_minio manual__2025-12-09T22:13:33.168137+00:00 [running]> on host 9a461450159a
[2025-12-09T22:13:35.232+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='amount_data_s3_processing_pipeline' AIRFLOW_CTX_TASK_ID='wait_for_json_files_in_minio' AIRFLOW_CTX_EXECUTION_DATE='2025-12-09T22:13:33.168137+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-12-09T22:13:33.168137+00:00'
[2025-12-09T22:13:35.233+0000] {python.py:72} INFO - Poking callable: <function check_for_new_json_files at 0x79f91bf35820>
[2025-12-09T22:13:35.345+0000] {amount_data_s3_processing_pipeline.py:79} INFO - ⏳ No new JSON files found (all files have timestamps)
[2025-12-09T22:13:35.345+0000] {base.py:323} INFO - new poke interval is 25
[2025-12-09T22:14:00.368+0000] {python.py:72} INFO - Poking callable: <function check_for_new_json_files at 0x79f91bf35820>
[2025-12-09T22:14:00.378+0000] {amount_data_s3_processing_pipeline.py:79} INFO - ⏳ No new JSON files found (all files have timestamps)
[2025-12-09T22:14:00.379+0000] {base.py:323} INFO - new poke interval is 41
[2025-12-09T22:14:41.413+0000] {python.py:72} INFO - Poking callable: <function check_for_new_json_files at 0x79f91bf35820>
[2025-12-09T22:14:41.595+0000] {amount_data_s3_processing_pipeline.py:79} INFO - ⏳ No new JSON files found (all files have timestamps)
[2025-12-09T22:14:41.595+0000] {base.py:323} INFO - new poke interval is 62
[2025-12-09T22:15:41.494+0000] {local_task_job_runner.py:294} WARNING - State of this instance has been externally set to restarting. Terminating instance.
[2025-12-09T22:15:41.496+0000] {process_utils.py:131} INFO - Sending 15 to group 300. PIDs of all processes in the group: [300]
[2025-12-09T22:15:41.496+0000] {process_utils.py:86} INFO - Sending the signal 15 to group 300
[2025-12-09T22:15:41.497+0000] {taskinstance.py:1630} ERROR - Received SIGTERM. Terminating subprocesses.
[2025-12-09T22:15:41.509+0000] {taskinstance.py:1935} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/sensors/base.py", line 285, in execute
    time.sleep(self._get_next_poke_interval(started_at, run_duration, try_number))
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/models/taskinstance.py", line 1632, in signal_handler
    raise AirflowException("Task received SIGTERM signal")
airflow.exceptions.AirflowException: Task received SIGTERM signal
[2025-12-09T22:15:41.512+0000] {taskinstance.py:1398} INFO - Marking task as UP_FOR_RETRY. dag_id=amount_data_s3_processing_pipeline, task_id=wait_for_json_files_in_minio, execution_date=20251209T221333, start_date=20251209T221335, end_date=20251209T221541
[2025-12-09T22:15:41.524+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 61 for task wait_for_json_files_in_minio (Task received SIGTERM signal; 300)
[2025-12-09T22:15:41.549+0000] {process_utils.py:79} INFO - Process psutil.Process(pid=300, status='terminated', exitcode=1, started='22:13:34') (300) terminated with exit code 1
