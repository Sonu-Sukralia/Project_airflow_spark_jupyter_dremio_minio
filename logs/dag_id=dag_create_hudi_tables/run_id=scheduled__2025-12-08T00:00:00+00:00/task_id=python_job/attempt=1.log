[2025-12-09T19:59:58.989+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: dag_create_hudi_tables.python_job scheduled__2025-12-08T00:00:00+00:00 [queued]>
[2025-12-09T19:59:58.997+0000] {taskinstance.py:1157} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: dag_create_hudi_tables.python_job scheduled__2025-12-08T00:00:00+00:00 [queued]>
[2025-12-09T19:59:58.998+0000] {taskinstance.py:1359} INFO - Starting attempt 1 of 1
[2025-12-09T19:59:59.009+0000] {taskinstance.py:1380} INFO - Executing <Task(SparkSubmitOperator): python_job> on 2025-12-08 00:00:00+00:00
[2025-12-09T19:59:59.013+0000] {standard_task_runner.py:57} INFO - Started process 967 to run task
[2025-12-09T19:59:59.017+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'dag_create_hudi_tables', 'python_job', 'scheduled__2025-12-08T00:00:00+00:00', '--job-id', '11', '--raw', '--subdir', 'DAGS_FOLDER/my_dag.py', '--cfg-path', '/tmp/tmpxga53avd']
[2025-12-09T19:59:59.019+0000] {standard_task_runner.py:85} INFO - Job 11: Subtask python_job
[2025-12-09T19:59:59.073+0000] {task_command.py:415} INFO - Running <TaskInstance: dag_create_hudi_tables.python_job scheduled__2025-12-08T00:00:00+00:00 [running]> on host ead6510418b0
[2025-12-09T19:59:59.154+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='Soumil Shah' AIRFLOW_CTX_DAG_ID='dag_create_hudi_tables' AIRFLOW_CTX_TASK_ID='python_job' AIRFLOW_CTX_EXECUTION_DATE='2025-12-08T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-12-08T00:00:00+00:00'
[2025-12-09T19:59:59.166+0000] {base.py:73} INFO - Using connection ID 'spark-conn' for task execution.
[2025-12-09T19:59:59.167+0000] {spark_submit.py:340} INFO - Spark-Submit cmd: spark-submit --master spark://spark-master:7077 --conf spark.driver.memory=1g --conf spark.executor.memory=1g --conf spark.executor.instances=1 --packages org.apache.hudi:hudi-spark3.4-bundle_2.12:0.14.0,org.apache.hadoop:hadoop-aws:3.3.2 --name arrow-spark jobs/my_dag_spark.py
[2025-12-09T20:00:00.878+0000] {spark_submit.py:491} INFO - :: loading settings :: url = jar:file:/home/***/.local/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml
[2025-12-09T20:00:00.981+0000] {spark_submit.py:491} INFO - Ivy Default Cache set to: /root/.ivy2/cache
[2025-12-09T20:00:00.981+0000] {spark_submit.py:491} INFO - The jars for the packages stored in: /root/.ivy2/jars
[2025-12-09T20:00:00.986+0000] {spark_submit.py:491} INFO - org.apache.hudi#hudi-spark3.4-bundle_2.12 added as a dependency
[2025-12-09T20:00:00.986+0000] {spark_submit.py:491} INFO - org.apache.hadoop#hadoop-aws added as a dependency
[2025-12-09T20:00:00.987+0000] {spark_submit.py:491} INFO - :: resolving dependencies :: org.apache.spark#spark-submit-parent-d4aa96ce-0ed6-40b6-9880-d7199fd6d9e2;1.0
[2025-12-09T20:00:00.988+0000] {spark_submit.py:491} INFO - confs: [default]
[2025-12-09T20:00:01.178+0000] {spark_submit.py:491} INFO - found org.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.0 in central
[2025-12-09T20:00:01.221+0000] {spark_submit.py:491} INFO - found org.apache.hadoop#hadoop-aws;3.3.2 in central
[2025-12-09T20:00:01.248+0000] {spark_submit.py:491} INFO - found com.amazonaws#aws-java-sdk-bundle;1.11.1026 in central
[2025-12-09T20:00:01.277+0000] {spark_submit.py:491} INFO - found org.wildfly.openssl#wildfly-openssl;1.0.7.Final in central
[2025-12-09T20:00:01.300+0000] {spark_submit.py:491} INFO - :: resolution report :: resolve 302ms :: artifacts dl 11ms
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - :: modules in use:
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - com.amazonaws#aws-java-sdk-bundle;1.11.1026 from central in [default]
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - org.apache.hadoop#hadoop-aws;3.3.2 from central in [default]
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - org.apache.hudi#hudi-spark3.4-bundle_2.12;0.14.0 from central in [default]
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - org.wildfly.openssl#wildfly-openssl;1.0.7.Final from central in [default]
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - |                  |            modules            ||   artifacts   |
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - |      default     |   4   |   0   |   0   |   0   ||   4   |   0   |
[2025-12-09T20:00:01.301+0000] {spark_submit.py:491} INFO - ---------------------------------------------------------------------
[2025-12-09T20:00:01.308+0000] {spark_submit.py:491} INFO - :: retrieving :: org.apache.spark#spark-submit-parent-d4aa96ce-0ed6-40b6-9880-d7199fd6d9e2
[2025-12-09T20:00:01.308+0000] {spark_submit.py:491} INFO - confs: [default]
[2025-12-09T20:00:01.314+0000] {spark_submit.py:491} INFO - 0 artifacts copied, 4 already retrieved (0kB/6ms)
[2025-12-09T20:00:01.530+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:01 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-12-09T20:00:02.751+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO SparkContext: Running Spark version 3.4.0
[2025-12-09T20:00:02.772+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO ResourceUtils: ==============================================================
[2025-12-09T20:00:02.772+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO ResourceUtils: No custom resources configured for spark.driver.
[2025-12-09T20:00:02.772+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO ResourceUtils: ==============================================================
[2025-12-09T20:00:02.773+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO SparkContext: Submitted application: arrow-spark
[2025-12-09T20:00:02.795+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
[2025-12-09T20:00:02.801+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO ResourceProfile: Limiting resource is cpu
[2025-12-09T20:00:02.801+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO ResourceProfileManager: Added ResourceProfile id: 0
[2025-12-09T20:00:02.860+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO SecurityManager: Changing view acls to: root
[2025-12-09T20:00:02.861+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO SecurityManager: Changing modify acls to: root
[2025-12-09T20:00:02.862+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO SecurityManager: Changing view acls groups to:
[2025-12-09T20:00:02.862+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO SecurityManager: Changing modify acls groups to:
[2025-12-09T20:00:02.862+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: root; groups with view permissions: EMPTY; users with modify permissions: root; groups with modify permissions: EMPTY
[2025-12-09T20:00:03.064+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO Utils: Successfully started service 'sparkDriver' on port 35511.
[2025-12-09T20:00:03.094+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkEnv: Registering MapOutputTracker
[2025-12-09T20:00:03.143+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkEnv: Registering BlockManagerMaster
[2025-12-09T20:00:03.158+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
[2025-12-09T20:00:03.159+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
[2025-12-09T20:00:03.167+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
[2025-12-09T20:00:03.180+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-adb958b0-d433-4dbd-865b-dc99c40d3a47
[2025-12-09T20:00:03.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
[2025-12-09T20:00:03.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkEnv: Registering OutputCommitCoordinator
[2025-12-09T20:00:03.333+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO JettyUtils: Start Jetty 0.0.0.0:4040 for SparkUI
[2025-12-09T20:00:03.393+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
[2025-12-09T20:00:03.423+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar at spark://ead6510418b0:35511/jars/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar with timestamp 1765310402744
[2025-12-09T20:00:03.424+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.2.jar at spark://ead6510418b0:35511/jars/org.apache.hadoop_hadoop-aws-3.3.2.jar with timestamp 1765310402744
[2025-12-09T20:00:03.425+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar at spark://ead6510418b0:35511/jars/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar with timestamp 1765310402744
[2025-12-09T20:00:03.425+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkContext: Added JAR file:///root/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://ead6510418b0:35511/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1765310402744
[2025-12-09T20:00:03.427+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar at spark://ead6510418b0:35511/files/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar with timestamp 1765310402744
[2025-12-09T20:00:03.427+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar to /tmp/spark-e4d45da3-19dc-4870-aa7a-566ca7d7d0b9/userFiles-100ce72b-999d-4f5a-9799-4618e22240ad/org.apache.hudi_hudi-spark3.4-bundle_2.12-0.14.0.jar
[2025-12-09T20:00:03.621+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkContext: Added file file:///root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.2.jar at spark://ead6510418b0:35511/files/org.apache.hadoop_hadoop-aws-3.3.2.jar with timestamp 1765310402744
[2025-12-09T20:00:03.621+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO Utils: Copying /root/.ivy2/jars/org.apache.hadoop_hadoop-aws-3.3.2.jar to /tmp/spark-e4d45da3-19dc-4870-aa7a-566ca7d7d0b9/userFiles-100ce72b-999d-4f5a-9799-4618e22240ad/org.apache.hadoop_hadoop-aws-3.3.2.jar
[2025-12-09T20:00:03.628+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO SparkContext: Added file file:///root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar at spark://ead6510418b0:35511/files/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar with timestamp 1765310402744
[2025-12-09T20:00:03.628+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:03 INFO Utils: Copying /root/.ivy2/jars/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar to /tmp/spark-e4d45da3-19dc-4870-aa7a-566ca7d7d0b9/userFiles-100ce72b-999d-4f5a-9799-4618e22240ad/com.amazonaws_aws-java-sdk-bundle-1.11.1026.jar
[2025-12-09T20:00:04.076+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO SparkContext: Added file file:///root/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar at spark://ead6510418b0:35511/files/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar with timestamp 1765310402744
[2025-12-09T20:00:04.076+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO Utils: Copying /root/.ivy2/jars/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar to /tmp/spark-e4d45da3-19dc-4870-aa7a-566ca7d7d0b9/userFiles-100ce72b-999d-4f5a-9799-4618e22240ad/org.wildfly.openssl_wildfly-openssl-1.0.7.Final.jar
[2025-12-09T20:00:04.138+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://spark-master:7077...
[2025-12-09T20:00:04.172+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO TransportClientFactory: Successfully created connection to spark-master/172.19.0.5:7077 after 18 ms (0 ms spent in bootstraps)
[2025-12-09T20:00:04.246+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20251209200004-0003
[2025-12-09T20:00:04.248+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20251209200004-0003/0 on worker-20251209193211-spark-worker-44899 (spark-worker:44899) with 4 core(s)
[2025-12-09T20:00:04.251+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20251209200004-0003/0 on hostPort spark-worker:44899 with 4 core(s), 1024.0 MiB RAM
[2025-12-09T20:00:04.253+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35171.
[2025-12-09T20:00:04.253+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO NettyBlockTransferService: Server created on ead6510418b0:35171
[2025-12-09T20:00:04.255+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
[2025-12-09T20:00:04.261+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, ead6510418b0, 35171, None)
[2025-12-09T20:00:04.264+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO BlockManagerMasterEndpoint: Registering block manager ead6510418b0:35171 with 434.4 MiB RAM, BlockManagerId(driver, ead6510418b0, 35171, None)
[2025-12-09T20:00:04.265+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, ead6510418b0, 35171, None)
[2025-12-09T20:00:04.266+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, ead6510418b0, 35171, None)
[2025-12-09T20:00:04.300+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20251209200004-0003/0 is now RUNNING
[2025-12-09T20:00:04.466+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:04 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
[2025-12-09T20:00:05.099+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:05 INFO SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.
[2025-12-09T20:00:05.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:05 INFO SharedState: Warehouse path is 'file:/opt/***/spark-warehouse'.
[2025-12-09T20:00:06.958+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:06 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (172.19.0.9:46230) with ID 0,  ResourceProfileId 0
[2025-12-09T20:00:07.027+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:07 INFO BlockManagerMasterEndpoint: Registering block manager spark-worker:33701 with 434.4 MiB RAM, BlockManagerId(0, spark-worker, 33701, None)
[2025-12-09T20:00:07.972+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:07 INFO CodeGenerator: Code generated in 195.457219 ms
[2025-12-09T20:00:08.001+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2025-12-09T20:00:08.011+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO DAGScheduler: Got job 0 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-12-09T20:00:08.011+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO DAGScheduler: Final stage: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0)
[2025-12-09T20:00:08.011+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:08.012+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:08.014+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-12-09T20:00:08.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 13.6 KiB, free 434.4 MiB)
[2025-12-09T20:00:08.247+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 6.9 KiB, free 434.4 MiB)
[2025-12-09T20:00:08.252+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on ead6510418b0:35171 (size: 6.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:08.255+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:08.279+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:08.282+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:08 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks resource profile 0
[2025-12-09T20:00:15.998+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:15 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 11861 bytes)
[2025-12-09T20:00:16.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:16 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on spark-worker:33701 (size: 6.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:17.660+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1671 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:17.661+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool
[2025-12-09T20:00:17.674+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO PythonAccumulatorV2: Connected to AccumulatorServer at host: 127.0.0.1 port: 33213
[2025-12-09T20:00:17.678+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: ResultStage 0 (showString at NativeMethodAccessorImpl.java:0) finished in 9.647 s
[2025-12-09T20:00:17.681+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:17.682+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO TaskSchedulerImpl: Killing all running tasks in stage 0: Stage finished
[2025-12-09T20:00:17.684+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: Job 0 finished: showString at NativeMethodAccessorImpl.java:0, took 9.682701 s
[2025-12-09T20:00:17.743+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO CodeGenerator: Code generated in 26.526425 ms
[2025-12-09T20:00:17.756+0000] {spark_submit.py:491} INFO - +--------------------+-----------------+--------------+-----------------+--------------------+--------------------+--------------------+
[2025-12-09T20:00:17.757+0000] {spark_submit.py:491} INFO - |         customer_id|             name|         state|             city|               email|          created_at|             address|
[2025-12-09T20:00:17.757+0000] {spark_submit.py:491} INFO - +--------------------+-----------------+--------------+-----------------+--------------------+--------------------+--------------------+
[2025-12-09T20:00:17.757+0000] {spark_submit.py:491} INFO - |dea07d35-f6a5-463...|   Brett Jennings|      Delaware|       Port Carol|  wevans@example.com|2025-12-09T20:00:...|254 Byrd Ports\nM...|
[2025-12-09T20:00:17.758+0000] {spark_submit.py:491} INFO - |5a88d295-0a13-43c...|      Sarah Allen|          Ohio|         Troyfort| sheri98@example.net|2025-12-09T20:00:...|294 Long Mount Su...|
[2025-12-09T20:00:17.758+0000] {spark_submit.py:491} INFO - |ce6c029f-a785-4fd...|     Bryan Monroe|       Florida|  West Evelynstad|charles35@example...|2025-12-09T20:00:...|22137 Fuentes Gro...|
[2025-12-09T20:00:17.758+0000] {spark_submit.py:491} INFO - |84494e61-6669-45b...|    Carla Carroll|        Oregon| West Ronaldmouth|johnnyfox@example...|2025-12-09T20:00:...|307 Jones Branch ...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |0428a8c6-4fa9-481...|      April Jones|  North Dakota|      Wintersport|michael38@example...|2025-12-09T20:00:...|PSC 3475, Box 625...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |f3faec6d-9218-477...|     Dustin Evans|  South Dakota|North Connieburgh| sarah07@example.net|2025-12-09T20:00:...|51919 Nelson Heig...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |3db28d85-0a13-4f5...|Jessica Hernandez|  North Dakota|  New Ronaldmouth| wanda99@example.net|2025-12-09T20:00:...|775 Hall Loop\nEa...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |4d128a98-395c-41d...|    Tyrone Barker|         Maine|      Wesleymouth|bernardjoshua@exa...|2025-12-09T20:00:...|288 Bowers Square...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |6aabbc90-a11a-4f2...|    Robert Newman|    New Mexico|   East Derekstad|christopherharper...|2025-12-09T20:00:...|Unit 9913 Box 219...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |d7c9db50-6705-46a...|  Stephen Michael|      Oklahoma|       Princeland|katrina62@example...|2025-12-09T20:00:...|916 King Meadows ...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |c50b7f02-65c8-416...|        Jesse Lin|       Vermont|         Kingtown|moorenicholas@exa...|2025-12-09T20:00:...|812 Francis Gatew...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |d66a95ad-bae5-46e...|  Jorge Rodriguez|   Connecticut|     Barnettshire|oharrison@example...|2025-12-09T20:00:...|75379 Eric Fords\...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |a7848a20-d01e-470...|    Joanna Moreno|      Maryland|         Ericfurt| oflores@example.org|2025-12-09T20:00:...|183 Ferguson Turn...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |b0e5d0fd-3c61-462...|      Brent Burns|        Oregon|  Port Angelaland|sweeneyvictoria@e...|2025-12-09T20:00:...|390 Gonzalez Flat...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |792c9b1c-9fb2-444...|    Alyssa Thomas|South Carolina|     Jenniferstad|  rick14@example.org|2025-12-09T20:00:...|26356 Kelly Turnp...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |9e1af632-c42b-4ab...|   Johnny Fuentes|North Carolina|       Lindaville|velasquezjamie@ex...|2025-12-09T20:00:...|86620 Murphy Cove...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |316500a9-4d3c-48a...|     Kathy Bowers|  Rhode Island|     Lake Katelyn|rsimpson@example.net|2025-12-09T20:00:...|307 Danielle Lake...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |2d167d51-df1b-41f...|   Ernest Higgins|        Oregon|      Fuentesfurt|richardwells@exam...|2025-12-09T20:00:...|40843 Bell Summit...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |e90dea0d-e3fe-42d...|      Mary Nguyen|       Montana|      East Daniel|samantha79@exampl...|2025-12-09T20:00:...|75211 Anthony Kno...|
[2025-12-09T20:00:17.759+0000] {spark_submit.py:491} INFO - |f19ad738-6e74-4bf...|    Sierra Lowery|         Maine| New Matthewville|aliciaedwards@exa...|2025-12-09T20:00:...|200 Rebecca View ...|
[2025-12-09T20:00:17.760+0000] {spark_submit.py:491} INFO - +--------------------+-----------------+--------------+-----------------+--------------------+--------------------+--------------------+
[2025-12-09T20:00:17.760+0000] {spark_submit.py:491} INFO - only showing top 20 rows
[2025-12-09T20:00:17.760+0000] {spark_submit.py:491} INFO - 
[2025-12-09T20:00:17.850+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO CodeGenerator: Code generated in 12.376897 ms
[2025-12-09T20:00:17.856+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO SparkContext: Starting job: showString at NativeMethodAccessorImpl.java:0
[2025-12-09T20:00:17.858+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-12-09T20:00:17.858+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)
[2025-12-09T20:00:17.859+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:17.859+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:17.859+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-12-09T20:00:17.863+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 13.3 KiB, free 434.4 MiB)
[2025-12-09T20:00:17.872+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 6.8 KiB, free 434.4 MiB)
[2025-12-09T20:00:17.873+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on ead6510418b0:35171 (size: 6.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:17.874+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:17.875+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[13] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:17.875+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks resource profile 0
[2025-12-09T20:00:17.877+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 10194 bytes)
[2025-12-09T20:00:17.896+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:17 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on spark-worker:33701 (size: 6.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:18.000+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 123 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:18.001+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool
[2025-12-09T20:00:18.002+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 0.141 s
[2025-12-09T20:00:18.002+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:18.002+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 1: Stage finished
[2025-12-09T20:00:18.003+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 0.146512 s
[2025-12-09T20:00:18.020+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO CodeGenerator: Code generated in 12.28753 ms
[2025-12-09T20:00:18.023+0000] {spark_submit.py:491} INFO - +--------------------+--------------------+-----------+--------+----------+--------------------+
[2025-12-09T20:00:18.024+0000] {spark_submit.py:491} INFO - |            order_id|                name|order_value|priority|order_date|         customer_id|
[2025-12-09T20:00:18.024+0000] {spark_submit.py:491} INFO - +--------------------+--------------------+-----------+--------+----------+--------------------+
[2025-12-09T20:00:18.024+0000] {spark_submit.py:491} INFO - |bcf97ec3-fdb0-412...|   Radio age source.|         54|    HIGH|2025-12-07|2d167d51-df1b-41f...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |d6576da7-553a-451...| Read ask write low.|        597|     LOW|2025-12-04|6ecd66ef-deae-4e3...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |4fe9b9bb-31d3-480...|Message interesting.|        725|    HIGH|2025-11-18|c50b7f02-65c8-416...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |25e22e8b-de27-4a0...|  Point capital per.|        319|    HIGH|2025-12-07|27087161-4404-47e...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |3f3dbf7c-73cc-462...|  Nature rather eye.|        802|    HIGH|2025-12-02|e608cd74-9d8d-4a8...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |d780112c-5966-4ed...|        Prepare far.|        275|     LOW|2025-11-26|a7848a20-d01e-470...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |37c024f5-a212-482...|Reason course color.|        599|    HIGH|2025-11-18|ba50d3fa-9dec-44e...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |bb8c2350-ebe9-451...|       Member reach.|        436|    HIGH|2025-11-23|454bdb32-da98-464...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |2fb2c874-1710-4ca...|    Likely her less.|        351|    HIGH|2025-12-06|d66a95ad-bae5-46e...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |cd66acd8-1705-4d2...|     Election cover.|        526|  MEDIUM|2025-11-15|9e1af632-c42b-4ab...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |cb320e25-e8c3-467...|  Current including.|        140|     LOW|2025-11-29|b0e5d0fd-3c61-462...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |82309106-fd0e-48f...|   Camera sell your.|        620|    HIGH|2025-11-15|ef79e0c7-3413-409...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |53aff8b4-8d91-424...|    Poor least beat.|        627|  MEDIUM|2025-11-20|f2ab2984-588d-4d1...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |fa33476f-344d-4c2...| Under natural able.|        626|    HIGH|2025-11-26|ef1d7787-804b-45a...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |81963449-6a17-4de...| Perhaps and charge.|        514|     LOW|2025-11-12|f9f4c7a2-cb8c-41a...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |84e1fee3-a4d1-48e...|       Concern line.|        824|  MEDIUM|2025-11-20|ef79e0c7-3413-409...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |9a1f7f73-8716-4c2...|   Image seem alone.|        320|     LOW|2025-11-15|5a88d295-0a13-43c...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |3284831f-399d-4a5...| Rather toward half.|        439|  MEDIUM|2025-11-10|f3faec6d-9218-477...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |db1d5caf-902e-403...|  Fear soon because.|        642|     LOW|2025-11-18|f3faec6d-9218-477...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - |5ff7d14b-3fdc-41d...|       Particularly.|        421|     LOW|2025-12-01|53a4045e-4854-4f5...|
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - +--------------------+--------------------+-----------+--------+----------+--------------------+
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - only showing top 20 rows
[2025-12-09T20:00:18.025+0000] {spark_submit.py:491} INFO - 
[2025-12-09T20:00:18.166+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO CodeGenerator: Code generated in 10.015591 ms
[2025-12-09T20:00:18.199+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Registering RDD 15 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0
[2025-12-09T20:00:18.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 2 output partitions
[2025-12-09T20:00:18.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)
[2025-12-09T20:00:18.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:18.204+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:18.206+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-12-09T20:00:18.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 15.9 KiB, free 434.3 MiB)
[2025-12-09T20:00:18.239+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.3 MiB)
[2025-12-09T20:00:18.241+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on ead6510418b0:35171 (size: 8.4 KiB, free: 434.4 MiB)
[2025-12-09T20:00:18.242+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:18.244+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[15] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:18.244+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSchedulerImpl: Adding task set 2.0 with 2 tasks resource profile 0
[2025-12-09T20:00:18.246+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 11850 bytes)
[2025-12-09T20:00:18.247+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSetManager: Starting task 1.0 in stage 2.0 (TID 3) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 11859 bytes)
[2025-12-09T20:00:18.271+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on spark-worker:33701 (size: 8.4 KiB, free: 434.4 MiB)
[2025-12-09T20:00:18.444+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 198 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:18.447+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSetManager: Finished task 1.0 in stage 2.0 (TID 3) in 201 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:18.448+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool
[2025-12-09T20:00:18.449+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.238 s
[2025-12-09T20:00:18.449+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:18.450+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:18.450+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: waiting: Set()
[2025-12-09T20:00:18.450+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:18.492+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO CodeGenerator: Code generated in 13.860591 ms
[2025-12-09T20:00:18.535+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
[2025-12-09T20:00:18.537+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-12-09T20:00:18.538+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Final stage: ResultStage 4 (count at NativeMethodAccessorImpl.java:0)
[2025-12-09T20:00:18.538+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
[2025-12-09T20:00:18.538+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:18.539+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-12-09T20:00:18.546+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 12.1 KiB, free 434.3 MiB)
[2025-12-09T20:00:18.551+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.3 MiB)
[2025-12-09T20:00:18.552+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on ead6510418b0:35171 (size: 5.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:18.553+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:18.554+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:18.554+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks resource profile 0
[2025-12-09T20:00:18.558+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 4) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2025-12-09T20:00:18.579+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on spark-worker:33701 (size: 5.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:18.597+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 172.19.0.9:46230
[2025-12-09T20:00:18.710+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 4) in 153 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:18.711+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool
[2025-12-09T20:00:18.712+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: ResultStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.168 s
[2025-12-09T20:00:18.712+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:18.712+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO TaskSchedulerImpl: Killing all running tasks in stage 4: Stage finished
[2025-12-09T20:00:18.713+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO DAGScheduler: Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.177548 s
[2025-12-09T20:00:18.977+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 WARN MetricsConfig: Cannot locate configuration: tried hadoop-metrics2-s3a-file-system.properties,hadoop-metrics2.properties
[2025-12-09T20:00:18.988+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO MetricsSystemImpl: Scheduled Metric snapshot period at 10 second(s).
[2025-12-09T20:00:18.988+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:18 INFO MetricsSystemImpl: s3a-file-system metrics system started
[2025-12-09T20:00:19.803+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:19 WARN DFSPropertiesConfiguration: Cannot find HUDI_CONF_DIR, please set it as the dir of hudi-defaults.conf
[2025-12-09T20:00:19.831+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:19 WARN DFSPropertiesConfiguration: Properties file file:/etc/hudi/conf/hudi-defaults.conf not found. Ignoring to load props file
[2025-12-09T20:00:19.903+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:19 INFO HoodieTableMetaClient: Initializing s3a://huditest/silver/table_name=customers/ as hoodie table s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:20.389+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:20.403+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:20.431+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:20.432+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:20.488+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:20.677+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO EmbeddedTimelineService: Starting Timeline service !!
[2025-12-09T20:00:20.678+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO EmbeddedTimelineService: Overriding hostIp to (ead6510418b0) found in spark-conf. It was null
[2025-12-09T20:00:20.685+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:20.686+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:20.703+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO log: Logging initialized @21291ms to org.apache.hudi.org.apache.jetty.util.log.Slf4jLog
[2025-12-09T20:00:20.803+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on ead6510418b0:35171 in memory (size: 6.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:20.810+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO BlockManagerInfo: Removed broadcast_1_piece0 on spark-worker:33701 in memory (size: 6.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:20.821+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on ead6510418b0:35171 in memory (size: 6.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:20.824+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO BlockManagerInfo: Removed broadcast_0_piece0 on spark-worker:33701 in memory (size: 6.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:20.831+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on ead6510418b0:35171 in memory (size: 5.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:20.834+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO BlockManagerInfo: Removed broadcast_3_piece0 on spark-worker:33701 in memory (size: 5.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:20.847+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on ead6510418b0:35171 in memory (size: 8.4 KiB, free: 434.4 MiB)
[2025-12-09T20:00:20.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO BlockManagerInfo: Removed broadcast_2_piece0 on spark-worker:33701 in memory (size: 8.4 KiB, free: 434.4 MiB)
[2025-12-09T20:00:20.878+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO Javalin:
[2025-12-09T20:00:20.879+0000] {spark_submit.py:491} INFO - __                      __ _            __ __
[2025-12-09T20:00:20.879+0000] {spark_submit.py:491} INFO - / /____ _ _   __ ____ _ / /(_)____      / // /
[2025-12-09T20:00:20.879+0000] {spark_submit.py:491} INFO - __  / // __ `/| | / // __ `// // // __ \    / // /_
[2025-12-09T20:00:20.879+0000] {spark_submit.py:491} INFO - / /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
[2025-12-09T20:00:20.879+0000] {spark_submit.py:491} INFO - \____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/
[2025-12-09T20:00:20.879+0000] {spark_submit.py:491} INFO - 
[2025-12-09T20:00:20.879+0000] {spark_submit.py:491} INFO - https://javalin.io/documentation
[2025-12-09T20:00:20.879+0000] {spark_submit.py:491} INFO - 
[2025-12-09T20:00:20.880+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO Javalin: Starting Javalin ...
[2025-12-09T20:00:20.891+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 1142 days old. Consider checking for a newer version.).
[2025-12-09T20:00:20.967+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:20 INFO Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 17.0.17+10-Debian-1deb11u1
[2025-12-09T20:00:21.028+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO Server: Started @21616ms
[2025-12-09T20:00:21.029+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO Javalin: Listening on http://localhost:36343/
[2025-12-09T20:00:21.029+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO Javalin: Javalin started in 150ms \o/
[2025-12-09T20:00:21.029+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO TimelineService: Starting Timeline server on port :36343
[2025-12-09T20:00:21.029+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO EmbeddedTimelineService: Started embedded timeline server at ead6510418b0:36343
[2025-12-09T20:00:21.038+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO BaseHoodieClient: Timeline Server already running. Not restarting the service
[2025-12-09T20:00:21.038+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-12-09T20:00:21.038+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-12-09T20:00:21.077+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.085+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:21.093+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.094+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.098+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:21.103+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO CleanerUtils: Cleaned failed attempts if any
[2025-12-09T20:00:21.105+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.116+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:21.123+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.124+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.131+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:21.140+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.148+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:21.154+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.166+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:21.173+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:21.173+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:21.176+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO BaseHoodieWriteClient: Generate a new instant time: 20251209200020437 action: commit
[2025-12-09T20:00:21.177+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieActiveTimeline: Creating a new instant [==>20251209200020437__commit__REQUESTED]
[2025-12-09T20:00:21.199+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:21.213+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.213+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.219+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__REQUESTED__20251209200021187]}
[2025-12-09T20:00:21.229+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.236+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:21.245+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:21.280+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
[2025-12-09T20:00:21.287+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__REQUESTED__20251209200021187]}
[2025-12-09T20:00:21.289+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Initializing s3a://huditest/silver/table_name=customers//.hoodie/metadata as hoodie table s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:21.515+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:21.525+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:21.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:21.534+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:21.581+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
[2025-12-09T20:00:21.582+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO DAGScheduler: Got job 4 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
[2025-12-09T20:00:21.582+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO DAGScheduler: Final stage: ResultStage 5 (collect at HoodieSparkEngineContext.java:116)
[2025-12-09T20:00:21.582+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:21.583+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:21.583+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[25] at map at HoodieSparkEngineContext.java:116), which has no missing parents
[2025-12-09T20:00:21.589+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 101.1 KiB, free 434.3 MiB)
[2025-12-09T20:00:21.600+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.3 MiB)
[2025-12-09T20:00:21.601+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on ead6510418b0:35171 (size: 36.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:21.602+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:21.602+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[25] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:21.602+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO TaskSchedulerImpl: Adding task set 5.0 with 1 tasks resource profile 0
[2025-12-09T20:00:21.608+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 5) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7371 bytes)
[2025-12-09T20:00:21.646+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:21 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on spark-worker:33701 (size: 36.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:22.875+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 1272 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:22.875+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool
[2025-12-09T20:00:22.876+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: ResultStage 5 (collect at HoodieSparkEngineContext.java:116) finished in 1.292 s
[2025-12-09T20:00:22.876+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:22.876+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO TaskSchedulerImpl: Killing all running tasks in stage 5: Stage finished
[2025-12-09T20:00:22.877+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: Job 4 finished: collect at HoodieSparkEngineContext.java:116, took 1.295864 s
[2025-12-09T20:00:22.890+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:22.891+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
[2025-12-09T20:00:22.891+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
[2025-12-09T20:00:22.903+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO SparkContext: Starting job: count at HoodieJavaRDD.java:115
[2025-12-09T20:00:22.904+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: Got job 5 (count at HoodieJavaRDD.java:115) with 1 output partitions
[2025-12-09T20:00:22.904+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: Final stage: ResultStage 6 (count at HoodieJavaRDD.java:115)
[2025-12-09T20:00:22.905+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:22.905+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:22.905+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: Submitting ResultStage 6 (ParallelCollectionRDD[26] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
[2025-12-09T20:00:22.909+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 3.0 KiB, free 434.3 MiB)
[2025-12-09T20:00:22.914+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.3 MiB)
[2025-12-09T20:00:22.916+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on ead6510418b0:35171 (size: 1866.0 B, free: 434.4 MiB)
[2025-12-09T20:00:22.917+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:22.919+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (ParallelCollectionRDD[26] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:22.919+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks resource profile 0
[2025-12-09T20:00:22.928+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:22 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 6) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7405 bytes)
[2025-12-09T20:00:23.057+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on spark-worker:33701 (size: 1866.0 B, free: 434.4 MiB)
[2025-12-09T20:00:23.075+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 6) in 153 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:23.075+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool
[2025-12-09T20:00:23.076+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: ResultStage 6 (count at HoodieJavaRDD.java:115) finished in 0.169 s
[2025-12-09T20:00:23.076+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Job 5 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:23.076+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 6: Stage finished
[2025-12-09T20:00:23.076+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Job 5 finished: count at HoodieJavaRDD.java:115, took 0.173055 s
[2025-12-09T20:00:23.077+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
[2025-12-09T20:00:23.090+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
[2025-12-09T20:00:23.157+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
[2025-12-09T20:00:23.159+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Got job 6 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
[2025-12-09T20:00:23.159+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Final stage: ResultStage 7 (foreach at HoodieSparkEngineContext.java:155)
[2025-12-09T20:00:23.159+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:23.159+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:23.159+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Submitting ResultStage 7 (ParallelCollectionRDD[27] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
[2025-12-09T20:00:23.179+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 383.8 KiB, free 433.9 MiB)
[2025-12-09T20:00:23.184+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 126.6 KiB, free 433.8 MiB)
[2025-12-09T20:00:23.185+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on ead6510418b0:35171 (size: 126.6 KiB, free: 434.2 MiB)
[2025-12-09T20:00:23.186+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:23.187+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (ParallelCollectionRDD[27] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:23.187+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks resource profile 0
[2025-12-09T20:00:23.188+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 7) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7260 bytes)
[2025-12-09T20:00:23.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on spark-worker:33701 (size: 126.6 KiB, free: 434.2 MiB)
[2025-12-09T20:00:23.545+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 7) in 358 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:23.545+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool
[2025-12-09T20:00:23.546+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: ResultStage 7 (foreach at HoodieSparkEngineContext.java:155) finished in 0.385 s
[2025-12-09T20:00:23.546+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:23.546+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 7: Stage finished
[2025-12-09T20:00:23.546+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Job 6 finished: foreach at HoodieSparkEngineContext.java:155, took 0.388947 s
[2025-12-09T20:00:23.551+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:23.566+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:23.566+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
[2025-12-09T20:00:23.566+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:23.566+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:23.566+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO AbstractTableFileSystemView: Building file system view for partition (files)
[2025-12-09T20:00:23.586+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-12-09T20:00:23.587+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-12-09T20:00:23.587+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.597+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:23.604+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.604+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.610+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:23.619+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:23.625+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:23.625+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:23.626+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
[2025-12-09T20:00:23.626+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.632+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:23.640+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.641+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.648+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:23.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO CleanerUtils: Cleaned failed attempts if any
[2025-12-09T20:00:23.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.660+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:23.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.669+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.674+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:23.682+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:23.689+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:23.690+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:23.690+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
[2025-12-09T20:00:23.690+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
[2025-12-09T20:00:23.714+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.722+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:23.729+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.730+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:23.736+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20251209200023701]}
[2025-12-09T20:00:23.745+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:23.751+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:23.751+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:23.754+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
[2025-12-09T20:00:23.754+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-12-09T20:00:23.764+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.requested
[2025-12-09T20:00:23.783+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
[2025-12-09T20:00:23.808+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
[2025-12-09T20:00:23.811+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Registering RDD 31 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) as input to shuffle 1
[2025-12-09T20:00:23.811+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Got job 7 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
[2025-12-09T20:00:23.812+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Final stage: ResultStage 9 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
[2025-12-09T20:00:23.812+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 8)
[2025-12-09T20:00:23.812+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 8)
[2025-12-09T20:00:23.812+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Submitting ShuffleMapStage 8 (MapPartitionsRDD[31] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
[2025-12-09T20:00:23.817+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 8.5 KiB, free 433.8 MiB)
[2025-12-09T20:00:23.821+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.7 MiB)
[2025-12-09T20:00:23.823+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on ead6510418b0:35171 (size: 4.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:23.824+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:23.825+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 8 (MapPartitionsRDD[31] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:23.825+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks resource profile 0
[2025-12-09T20:00:23.828+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 8) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7503 bytes)
[2025-12-09T20:00:23.848+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on spark-worker:33701 (size: 4.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:23.886+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 8) in 59 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:23.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool
[2025-12-09T20:00:23.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: ShuffleMapStage 8 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.073 s
[2025-12-09T20:00:23.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:23.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:23.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: waiting: Set(ResultStage 9)
[2025-12-09T20:00:23.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:23.888+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Submitting ResultStage 9 (MapPartitionsRDD[34] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
[2025-12-09T20:00:23.891+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 7.3 KiB, free 433.7 MiB)
[2025-12-09T20:00:23.895+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.7 MiB)
[2025-12-09T20:00:23.897+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on ead6510418b0:35171 (size: 3.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:23.897+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:23.898+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 9 (MapPartitionsRDD[34] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:23.898+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Adding task set 9.0 with 1 tasks resource profile 0
[2025-12-09T20:00:23.900+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 9) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:23.922+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on spark-worker:33701 (size: 3.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:23.934+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 172.19.0.9:46230
[2025-12-09T20:00:23.974+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 9) in 74 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:23.975+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool
[2025-12-09T20:00:23.975+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: ResultStage 9 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.086 s
[2025-12-09T20:00:23.976+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:23.976+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO TaskSchedulerImpl: Killing all running tasks in stage 9: Stage finished
[2025-12-09T20:00:23.976+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:23 INFO DAGScheduler: Job 7 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.168189 s
[2025-12-09T20:00:24.009+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-12-09T20:00:24.009+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
[2025-12-09T20:00:24.037+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
[2025-12-09T20:00:24.038+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Got job 8 (collect at HoodieJavaRDD.java:177) with 1 output partitions
[2025-12-09T20:00:24.039+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Final stage: ResultStage 11 (collect at HoodieJavaRDD.java:177)
[2025-12-09T20:00:24.039+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 10)
[2025-12-09T20:00:24.042+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:24.043+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Submitting ResultStage 11 (MapPartitionsRDD[37] at map at HoodieJavaRDD.java:125), which has no missing parents
[2025-12-09T20:00:24.058+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 357.5 KiB, free 433.4 MiB)
[2025-12-09T20:00:24.063+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 123.6 KiB, free 433.3 MiB)
[2025-12-09T20:00:24.065+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on ead6510418b0:35171 (size: 123.6 KiB, free: 434.1 MiB)
[2025-12-09T20:00:24.065+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:24.066+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 11 (MapPartitionsRDD[37] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:24.066+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSchedulerImpl: Adding task set 11.0 with 1 tasks resource profile 0
[2025-12-09T20:00:24.067+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 10) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:24.086+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on spark-worker:33701 (size: 123.6 KiB, free: 434.1 MiB)
[2025-12-09T20:00:24.821+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Added rdd_36_0 in memory on spark-worker:33701 (size: 290.0 B, free: 434.1 MiB)
[2025-12-09T20:00:24.841+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 10) in 773 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:24.841+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool
[2025-12-09T20:00:24.842+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: ResultStage 11 (collect at HoodieJavaRDD.java:177) finished in 0.798 s
[2025-12-09T20:00:24.842+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Job 8 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:24.842+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 11: Stage finished
[2025-12-09T20:00:24.843+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Job 8 finished: collect at HoodieJavaRDD.java:177, took 0.805792 s
[2025-12-09T20:00:24.845+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
[2025-12-09T20:00:24.845+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
[2025-12-09T20:00:24.872+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_9_piece0 on ead6510418b0:35171 in memory (size: 123.6 KiB, free: 434.2 MiB)
[2025-12-09T20:00:24.877+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_9_piece0 on spark-worker:33701 in memory (size: 123.6 KiB, free: 434.2 MiB)
[2025-12-09T20:00:24.882+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-12-09T20:00:24.883+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Got job 9 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-12-09T20:00:24.884+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Final stage: ResultStage 12 (collect at HoodieSparkEngineContext.java:150)
[2025-12-09T20:00:24.884+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:24.884+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:24.884+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[39] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-12-09T20:00:24.885+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_7_piece0 on ead6510418b0:35171 in memory (size: 4.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:24.889+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 100.7 KiB, free 433.7 MiB)
[2025-12-09T20:00:24.889+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_7_piece0 on spark-worker:33701 in memory (size: 4.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:24.895+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.6 MiB)
[2025-12-09T20:00:24.896+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on ead6510418b0:35171 (size: 35.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:24.896+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:24.897+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[39] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:24.897+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks resource profile 0
[2025-12-09T20:00:24.898+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_6_piece0 on ead6510418b0:35171 in memory (size: 126.6 KiB, free: 434.3 MiB)
[2025-12-09T20:00:24.899+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 11) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2025-12-09T20:00:24.902+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_6_piece0 on spark-worker:33701 in memory (size: 126.6 KiB, free: 434.4 MiB)
[2025-12-09T20:00:24.909+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_4_piece0 on ead6510418b0:35171 in memory (size: 36.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:24.913+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_4_piece0 on spark-worker:33701 in memory (size: 36.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:24.914+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on spark-worker:33701 (size: 35.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:24.919+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_8_piece0 on ead6510418b0:35171 in memory (size: 3.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:24.922+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_8_piece0 on spark-worker:33701 in memory (size: 3.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:24.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on ead6510418b0:35171 in memory (size: 1866.0 B, free: 434.4 MiB)
[2025-12-09T20:00:24.934+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO BlockManagerInfo: Removed broadcast_5_piece0 on spark-worker:33701 in memory (size: 1866.0 B, free: 434.4 MiB)
[2025-12-09T20:00:24.953+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 11) in 54 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:24.953+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool
[2025-12-09T20:00:24.954+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: ResultStage 12 (collect at HoodieSparkEngineContext.java:150) finished in 0.068 s
[2025-12-09T20:00:24.954+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:24.954+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO TaskSchedulerImpl: Killing all running tasks in stage 12: Stage finished
[2025-12-09T20:00:24.954+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO DAGScheduler: Job 9 finished: collect at HoodieSparkEngineContext.java:150, took 0.071829 s
[2025-12-09T20:00:24.976+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
[2025-12-09T20:00:24.977+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:24 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
[2025-12-09T20:00:25.001+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
[2025-12-09T20:00:25.002+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
[2025-12-09T20:00:25.002+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
[2025-12-09T20:00:25.034+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-12-09T20:00:25.035+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Got job 10 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
[2025-12-09T20:00:25.036+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Final stage: ResultStage 13 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-12-09T20:00:25.036+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:25.036+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:25.037+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[41] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-12-09T20:00:25.041+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 100.9 KiB, free 434.2 MiB)
[2025-12-09T20:00:25.045+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.1 MiB)
[2025-12-09T20:00:25.046+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on ead6510418b0:35171 (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:25.047+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:25.047+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[41] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:25.048+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO TaskSchedulerImpl: Adding task set 13.0 with 1 tasks resource profile 0
[2025-12-09T20:00:25.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 12) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2025-12-09T20:00:25.067+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on spark-worker:33701 (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:25.125+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 12) in 77 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:25.125+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool
[2025-12-09T20:00:25.126+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: ResultStage 13 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.090 s
[2025-12-09T20:00:25.126+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:25.126+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO TaskSchedulerImpl: Killing all running tasks in stage 13: Stage finished
[2025-12-09T20:00:25.127+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Job 10 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.092453 s
[2025-12-09T20:00:25.155+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/.temp/00000000000000010
[2025-12-09T20:00:25.155+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.165+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.174+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.175+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.183+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.189+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.195+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:25.195+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:25.201+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.210+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.325+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: MDT s3a://huditest/silver/table_name=customers partition FILES has been enabled
[2025-12-09T20:00:25.326+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.332+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:25.338+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.339+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.346+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.351+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.355+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.355+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:25.355+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:25.355+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 2278 in ms
[2025-12-09T20:00:25.356+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.363+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.374+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.382+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.390+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:25.391+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:25.391+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.397+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.403+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.404+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.410+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.419+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.427+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:25.427+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:25.433+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.434+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
[2025-12-09T20:00:25.440+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.441+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.445+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.451+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.451+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.456+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.463+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.467+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:25.467+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:25.467+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BaseHoodieWriteClient: Cleaner started
[2025-12-09T20:00:25.468+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.473+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.479+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.479+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.485+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.493+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.498+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:25.499+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:25.499+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time :00000000000000010002
[2025-12-09T20:00:25.504+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.505+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:25.505+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:25.507+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
[2025-12-09T20:00:25.507+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
[2025-12-09T20:00:25.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.532+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.538+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.544+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.545+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.550+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.557+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.563+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:25.563+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:25.569+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__REQUESTED__20251209200021187]}
[2025-12-09T20:00:25.570+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
[2025-12-09T20:00:25.570+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.577+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.583+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.583+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.588+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.594+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.601+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:25.601+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:25.601+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BaseHoodieWriteClient: Scheduling compaction at instant time :00000000000000010001
[2025-12-09T20:00:25.606+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.610+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.617+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:25.628+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.634+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.638+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:25.639+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:25.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.650+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.655+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:25.659+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.663+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__REQUESTED__20251209200021187]}
[2025-12-09T20:00:25.666+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTimelineArchiver: No Instants to archive
[2025-12-09T20:00:25.666+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
[2025-12-09T20:00:25.667+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.672+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:25.676+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.677+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.682+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.686+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.691+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.692+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:25.692+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:25.692+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:25.693+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:25.693+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO AsyncCleanerService: Starting async clean service with instant time 20251209200025692...
[2025-12-09T20:00:25.694+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-12-09T20:00:25.695+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.702+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:25.709+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.709+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.714+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__REQUESTED__20251209200021187]}
[2025-12-09T20:00:25.715+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.722+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:25.729+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.729+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.736+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.741+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-12-09T20:00:25.743+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.747+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Registering RDD 42 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 3
[2025-12-09T20:00:25.748+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Registering RDD 46 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 2
[2025-12-09T20:00:25.748+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Got job 11 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
[2025-12-09T20:00:25.748+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Final stage: ResultStage 16 (countByKey at HoodieJavaPairRDD.java:105)
[2025-12-09T20:00:25.748+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 15)
[2025-12-09T20:00:25.748+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 15)
[2025-12-09T20:00:25.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Submitting ShuffleMapStage 14 (MapPartitionsRDD[42] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-12-09T20:00:25.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:25.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:25.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:25.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:25.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BaseHoodieWriteClient: Cleaner started
[2025-12-09T20:00:25.750+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.755+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 76.8 KiB, free 434.1 MiB)
[2025-12-09T20:00:25.757+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 27.7 KiB, free 434.0 MiB)
[2025-12-09T20:00:25.758+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:25.758+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on ead6510418b0:35171 (size: 27.7 KiB, free: 434.3 MiB)
[2025-12-09T20:00:25.759+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:25.759+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 14 (MapPartitionsRDD[42] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:25.759+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO TaskSchedulerImpl: Adding task set 14.0 with 2 tasks resource profile 0
[2025-12-09T20:00:25.761+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 13) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 11850 bytes)
[2025-12-09T20:00:25.761+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO TaskSetManager: Starting task 1.0 in stage 14.0 (TID 14) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 11859 bytes)
[2025-12-09T20:00:25.766+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.766+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.774+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__REQUESTED__20251209200021187]}
[2025-12-09T20:00:25.775+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.779+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on spark-worker:33701 (size: 27.7 KiB, free: 434.3 MiB)
[2025-12-09T20:00:25.783+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:25.790+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:25.790+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.798+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:25.806+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:25.816+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:25.816+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:25.816+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:25.816+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:25.816+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:25.816+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time :20251209200025692
[2025-12-09T20:00:25.825+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:25 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__REQUESTED__20251209200021187]}
[2025-12-09T20:00:26.015+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 13) in 255 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:26.019+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 1.0 in stage 14.0 (TID 14) in 258 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:26.020+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool
[2025-12-09T20:00:26.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: ShuffleMapStage 14 (mapToPair at HoodieJavaRDD.java:149) finished in 0.271 s
[2025-12-09T20:00:26.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:26.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:26.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: waiting: Set(ShuffleMapStage 15, ResultStage 16)
[2025-12-09T20:00:26.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:26.022+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:26.025+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 27.1 KiB, free 434.0 MiB)
[2025-12-09T20:00:26.030+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 434.0 MiB)
[2025-12-09T20:00:26.032+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on ead6510418b0:35171 (size: 12.9 KiB, free: 434.3 MiB)
[2025-12-09T20:00:26.032+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:26.033+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[46] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:26.033+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Adding task set 15.0 with 2 tasks resource profile 0
[2025-12-09T20:00:26.035+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 15) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:26.035+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 1.0 in stage 15.0 (TID 16) (spark-worker, executor 0, partition 1, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:26.053+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on spark-worker:33701 (size: 12.9 KiB, free: 434.3 MiB)
[2025-12-09T20:00:26.072+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.19.0.9:46230
[2025-12-09T20:00:26.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added rdd_44_1 in memory on spark-worker:33701 (size: 4.5 KiB, free: 434.3 MiB)
[2025-12-09T20:00:26.102+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added rdd_44_0 in memory on spark-worker:33701 (size: 3.6 KiB, free: 434.3 MiB)
[2025-12-09T20:00:26.116+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 15) in 82 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:26.119+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 1.0 in stage 15.0 (TID 16) in 84 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:26.120+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool
[2025-12-09T20:00:26.120+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: ShuffleMapStage 15 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.098 s
[2025-12-09T20:00:26.120+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:26.121+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:26.121+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: waiting: Set(ResultStage 16)
[2025-12-09T20:00:26.121+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:26.121+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Submitting ResultStage 16 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:26.123+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 5.5 KiB, free 434.0 MiB)
[2025-12-09T20:00:26.128+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.0 MiB)
[2025-12-09T20:00:26.128+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on ead6510418b0:35171 (size: 3.1 KiB, free: 434.3 MiB)
[2025-12-09T20:00:26.129+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:26.129+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 16 (ShuffledRDD[47] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:26.130+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks resource profile 0
[2025-12-09T20:00:26.132+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 17) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:26.132+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 18) (spark-worker, executor 0, partition 1, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:26.145+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on spark-worker:33701 (size: 3.1 KiB, free: 434.3 MiB)
[2025-12-09T20:00:26.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 172.19.0.9:46230
[2025-12-09T20:00:26.168+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 18) in 36 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:26.171+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 17) in 40 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:26.172+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool
[2025-12-09T20:00:26.172+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: ResultStage 16 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.050 s
[2025-12-09T20:00:26.172+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Job 11 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:26.172+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 16: Stage finished
[2025-12-09T20:00:26.173+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Job 11 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.430980 s
[2025-12-09T20:00:26.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-12-09T20:00:26.229+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Got job 12 (collect at HoodieSparkEngineContext.java:150) with 31 output partitions
[2025-12-09T20:00:26.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Final stage: ResultStage 17 (collect at HoodieSparkEngineContext.java:150)
[2025-12-09T20:00:26.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:26.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:26.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[49] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-12-09T20:00:26.243+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 453.7 KiB, free 433.5 MiB)
[2025-12-09T20:00:26.246+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 158.2 KiB, free 433.4 MiB)
[2025-12-09T20:00:26.247+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on ead6510418b0:35171 (size: 158.2 KiB, free: 434.1 MiB)
[2025-12-09T20:00:26.248+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:26.248+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Submitting 31 missing tasks from ResultStage 17 (MapPartitionsRDD[49] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-12-09T20:00:26.249+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Adding task set 17.0 with 31 tasks resource profile 0
[2025-12-09T20:00:26.250+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 19) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:26.250+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 1.0 in stage 17.0 (TID 20) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7264 bytes)
[2025-12-09T20:00:26.250+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 2.0 in stage 17.0 (TID 21) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7265 bytes)
[2025-12-09T20:00:26.250+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 3.0 in stage 17.0 (TID 22) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7260 bytes)
[2025-12-09T20:00:26.269+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on spark-worker:33701 (size: 158.2 KiB, free: 434.1 MiB)
[2025-12-09T20:00:26.332+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 4.0 in stage 17.0 (TID 23) (spark-worker, executor 0, partition 4, PROCESS_LOCAL, 7263 bytes)
[2025-12-09T20:00:26.333+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 19) in 83 ms on spark-worker (executor 0) (1/31)
[2025-12-09T20:00:26.337+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 5.0 in stage 17.0 (TID 24) (spark-worker, executor 0, partition 5, PROCESS_LOCAL, 7258 bytes)
[2025-12-09T20:00:26.338+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 2.0 in stage 17.0 (TID 21) in 88 ms on spark-worker (executor 0) (2/31)
[2025-12-09T20:00:26.342+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 6.0 in stage 17.0 (TID 25) (spark-worker, executor 0, partition 6, PROCESS_LOCAL, 7266 bytes)
[2025-12-09T20:00:26.342+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 1.0 in stage 17.0 (TID 20) in 92 ms on spark-worker (executor 0) (3/31)
[2025-12-09T20:00:26.345+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 7.0 in stage 17.0 (TID 26) (spark-worker, executor 0, partition 7, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:26.345+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 3.0 in stage 17.0 (TID 22) in 95 ms on spark-worker (executor 0) (4/31)
[2025-12-09T20:00:26.387+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 8.0 in stage 17.0 (TID 27) (spark-worker, executor 0, partition 8, PROCESS_LOCAL, 7268 bytes)
[2025-12-09T20:00:26.388+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 4.0 in stage 17.0 (TID 23) in 56 ms on spark-worker (executor 0) (5/31)
[2025-12-09T20:00:26.392+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 9.0 in stage 17.0 (TID 28) (spark-worker, executor 0, partition 9, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:26.393+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 6.0 in stage 17.0 (TID 25) in 53 ms on spark-worker (executor 0) (6/31)
[2025-12-09T20:00:26.396+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 10.0 in stage 17.0 (TID 29) (spark-worker, executor 0, partition 10, PROCESS_LOCAL, 7259 bytes)
[2025-12-09T20:00:26.397+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 5.0 in stage 17.0 (TID 24) in 64 ms on spark-worker (executor 0) (7/31)
[2025-12-09T20:00:26.403+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 11.0 in stage 17.0 (TID 30) (spark-worker, executor 0, partition 11, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:26.404+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 7.0 in stage 17.0 (TID 26) in 60 ms on spark-worker (executor 0) (8/31)
[2025-12-09T20:00:26.440+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 12.0 in stage 17.0 (TID 31) (spark-worker, executor 0, partition 12, PROCESS_LOCAL, 7266 bytes)
[2025-12-09T20:00:26.441+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 9.0 in stage 17.0 (TID 28) in 52 ms on spark-worker (executor 0) (9/31)
[2025-12-09T20:00:26.444+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 13.0 in stage 17.0 (TID 32) (spark-worker, executor 0, partition 13, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:26.444+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 11.0 in stage 17.0 (TID 30) in 46 ms on spark-worker (executor 0) (10/31)
[2025-12-09T20:00:26.448+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 14.0 in stage 17.0 (TID 33) (spark-worker, executor 0, partition 14, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:26.449+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 10.0 in stage 17.0 (TID 29) in 55 ms on spark-worker (executor 0) (11/31)
[2025-12-09T20:00:26.453+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 15.0 in stage 17.0 (TID 34) (spark-worker, executor 0, partition 15, PROCESS_LOCAL, 7259 bytes)
[2025-12-09T20:00:26.454+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 8.0 in stage 17.0 (TID 27) in 70 ms on spark-worker (executor 0) (12/31)
[2025-12-09T20:00:26.494+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 16.0 in stage 17.0 (TID 35) (spark-worker, executor 0, partition 16, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:26.495+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 12.0 in stage 17.0 (TID 31) in 58 ms on spark-worker (executor 0) (13/31)
[2025-12-09T20:00:26.509+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 17.0 in stage 17.0 (TID 36) (spark-worker, executor 0, partition 17, PROCESS_LOCAL, 7264 bytes)
[2025-12-09T20:00:26.510+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 13.0 in stage 17.0 (TID 32) in 69 ms on spark-worker (executor 0) (14/31)
[2025-12-09T20:00:26.552+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 18.0 in stage 17.0 (TID 37) (spark-worker, executor 0, partition 18, PROCESS_LOCAL, 7260 bytes)
[2025-12-09T20:00:26.553+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 14.0 in stage 17.0 (TID 33) in 107 ms on spark-worker (executor 0) (15/31)
[2025-12-09T20:00:26.579+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 19.0 in stage 17.0 (TID 38) (spark-worker, executor 0, partition 19, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:26.580+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 15.0 in stage 17.0 (TID 34) in 130 ms on spark-worker (executor 0) (16/31)
[2025-12-09T20:00:26.595+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 20.0 in stage 17.0 (TID 39) (spark-worker, executor 0, partition 20, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:26.596+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 17.0 in stage 17.0 (TID 36) in 93 ms on spark-worker (executor 0) (17/31)
[2025-12-09T20:00:26.601+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 21.0 in stage 17.0 (TID 40) (spark-worker, executor 0, partition 21, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:26.601+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 16.0 in stage 17.0 (TID 35) in 113 ms on spark-worker (executor 0) (18/31)
[2025-12-09T20:00:26.635+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 22.0 in stage 17.0 (TID 41) (spark-worker, executor 0, partition 22, PROCESS_LOCAL, 7258 bytes)
[2025-12-09T20:00:26.635+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 18.0 in stage 17.0 (TID 37) in 87 ms on spark-worker (executor 0) (19/31)
[2025-12-09T20:00:26.661+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 23.0 in stage 17.0 (TID 42) (spark-worker, executor 0, partition 23, PROCESS_LOCAL, 7268 bytes)
[2025-12-09T20:00:26.662+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 19.0 in stage 17.0 (TID 38) in 85 ms on spark-worker (executor 0) (20/31)
[2025-12-09T20:00:26.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 24.0 in stage 17.0 (TID 43) (spark-worker, executor 0, partition 24, PROCESS_LOCAL, 7258 bytes)
[2025-12-09T20:00:26.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 20.0 in stage 17.0 (TID 39) in 76 ms on spark-worker (executor 0) (21/31)
[2025-12-09T20:00:26.683+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 25.0 in stage 17.0 (TID 44) (spark-worker, executor 0, partition 25, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:26.684+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 21.0 in stage 17.0 (TID 40) in 87 ms on spark-worker (executor 0) (22/31)
[2025-12-09T20:00:26.721+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 26.0 in stage 17.0 (TID 45) (spark-worker, executor 0, partition 26, PROCESS_LOCAL, 7267 bytes)
[2025-12-09T20:00:26.721+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 23.0 in stage 17.0 (TID 42) in 65 ms on spark-worker (executor 0) (23/31)
[2025-12-09T20:00:26.731+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 27.0 in stage 17.0 (TID 46) (spark-worker, executor 0, partition 27, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:26.731+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 22.0 in stage 17.0 (TID 41) in 99 ms on spark-worker (executor 0) (24/31)
[2025-12-09T20:00:26.743+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 28.0 in stage 17.0 (TID 47) (spark-worker, executor 0, partition 28, PROCESS_LOCAL, 7266 bytes)
[2025-12-09T20:00:26.744+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 24.0 in stage 17.0 (TID 43) in 79 ms on spark-worker (executor 0) (25/31)
[2025-12-09T20:00:26.774+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 29.0 in stage 17.0 (TID 48) (spark-worker, executor 0, partition 29, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:26.775+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 25.0 in stage 17.0 (TID 44) in 94 ms on spark-worker (executor 0) (26/31)
[2025-12-09T20:00:26.799+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 30.0 in stage 17.0 (TID 49) (spark-worker, executor 0, partition 30, PROCESS_LOCAL, 7263 bytes)
[2025-12-09T20:00:26.800+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 27.0 in stage 17.0 (TID 46) in 73 ms on spark-worker (executor 0) (27/31)
[2025-12-09T20:00:26.812+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 26.0 in stage 17.0 (TID 45) in 94 ms on spark-worker (executor 0) (28/31)
[2025-12-09T20:00:26.822+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 28.0 in stage 17.0 (TID 47) in 83 ms on spark-worker (executor 0) (29/31)
[2025-12-09T20:00:26.861+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 29.0 in stage 17.0 (TID 48) in 91 ms on spark-worker (executor 0) (30/31)
[2025-12-09T20:00:26.873+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Finished task 30.0 in stage 17.0 (TID 49) in 76 ms on spark-worker (executor 0) (31/31)
[2025-12-09T20:00:26.873+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool
[2025-12-09T20:00:26.874+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: ResultStage 17 (collect at HoodieSparkEngineContext.java:150) finished in 0.643 s
[2025-12-09T20:00:26.874+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:26.875+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Killing all running tasks in stage 17: Stage finished
[2025-12-09T20:00:26.875+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Job 12 finished: collect at HoodieSparkEngineContext.java:150, took 0.645819 s
[2025-12-09T20:00:26.913+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_13_piece0 on ead6510418b0:35171 in memory (size: 12.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:26.916+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_13_piece0 on spark-worker:33701 in memory (size: 12.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:26.923+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
[2025-12-09T20:00:26.925+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Got job 13 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
[2025-12-09T20:00:26.926+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Final stage: ResultStage 18 (collect at HoodieSparkEngineContext.java:116)
[2025-12-09T20:00:26.926+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:26.926+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:26.927+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Submitting ResultStage 18 (MapPartitionsRDD[51] at map at HoodieSparkEngineContext.java:116), which has no missing parents
[2025-12-09T20:00:26.928+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_11_piece0 on ead6510418b0:35171 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2025-12-09T20:00:26.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_11_piece0 on spark-worker:33701 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2025-12-09T20:00:26.939+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 453.4 KiB, free 433.1 MiB)
[2025-12-09T20:00:26.942+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_15_piece0 on ead6510418b0:35171 in memory (size: 158.2 KiB, free: 434.3 MiB)
[2025-12-09T20:00:26.944+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 158.0 KiB, free 433.6 MiB)
[2025-12-09T20:00:26.945+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on ead6510418b0:35171 (size: 158.0 KiB, free: 434.2 MiB)
[2025-12-09T20:00:26.945+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_15_piece0 on spark-worker:33701 in memory (size: 158.2 KiB, free: 434.3 MiB)
[2025-12-09T20:00:26.945+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:26.946+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 18 (MapPartitionsRDD[51] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:26.946+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSchedulerImpl: Adding task set 18.0 with 1 tasks resource profile 0
[2025-12-09T20:00:26.948+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 50) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7246 bytes)
[2025-12-09T20:00:26.954+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on ead6510418b0:35171 in memory (size: 35.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:26.957+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_10_piece0 on spark-worker:33701 in memory (size: 35.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:26.964+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on spark-worker:33701 (size: 158.0 KiB, free: 434.2 MiB)
[2025-12-09T20:00:26.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManager: Removing RDD 36
[2025-12-09T20:00:26.986+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_12_piece0 on ead6510418b0:35171 in memory (size: 27.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:26.988+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_12_piece0 on spark-worker:33701 in memory (size: 27.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:26.997+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_14_piece0 on ead6510418b0:35171 in memory (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:26.998+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:26 INFO BlockManagerInfo: Removed broadcast_14_piece0 on spark-worker:33701 in memory (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.014+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 50) in 66 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:27.014+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool
[2025-12-09T20:00:27.014+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: ResultStage 18 (collect at HoodieSparkEngineContext.java:116) finished in 0.087 s
[2025-12-09T20:00:27.014+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:27.015+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 18: Stage finished
[2025-12-09T20:00:27.015+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Job 13 finished: collect at HoodieSparkEngineContext.java:116, took 0.091818 s
[2025-12-09T20:00:27.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkHoodieBloomIndexHelper: Input parallelism: 2, Index parallelism: 2
[2025-12-09T20:00:27.029+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Starting job: countByKey at SparkHoodieBloomIndexHelper.java:197
[2025-12-09T20:00:27.030+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Registering RDD 54 (countByKey at SparkHoodieBloomIndexHelper.java:197) as input to shuffle 4
[2025-12-09T20:00:27.031+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Got job 14 (countByKey at SparkHoodieBloomIndexHelper.java:197) with 2 output partitions
[2025-12-09T20:00:27.031+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Final stage: ResultStage 21 (countByKey at SparkHoodieBloomIndexHelper.java:197)
[2025-12-09T20:00:27.031+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
[2025-12-09T20:00:27.031+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
[2025-12-09T20:00:27.032+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[54] at countByKey at SparkHoodieBloomIndexHelper.java:197), which has no missing parents
[2025-12-09T20:00:27.034+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 28.8 KiB, free 433.8 MiB)
[2025-12-09T20:00:27.037+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 433.8 MiB)
[2025-12-09T20:00:27.038+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on ead6510418b0:35171 (size: 13.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.039+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:27.040+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[54] at countByKey at SparkHoodieBloomIndexHelper.java:197) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:27.040+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks resource profile 0
[2025-12-09T20:00:27.043+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 51) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7174 bytes)
[2025-12-09T20:00:27.043+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 52) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7174 bytes)
[2025-12-09T20:00:27.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on spark-worker:33701 (size: 13.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.077+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 51) in 35 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:27.081+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 52) in 38 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:27.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool
[2025-12-09T20:00:27.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: ShuffleMapStage 20 (countByKey at SparkHoodieBloomIndexHelper.java:197) finished in 0.050 s
[2025-12-09T20:00:27.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:27.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:27.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: waiting: Set(ResultStage 21)
[2025-12-09T20:00:27.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:27.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting ResultStage 21 (ShuffledRDD[55] at countByKey at SparkHoodieBloomIndexHelper.java:197), which has no missing parents
[2025-12-09T20:00:27.084+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 5.5 KiB, free 433.8 MiB)
[2025-12-09T20:00:27.087+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.8 MiB)
[2025-12-09T20:00:27.088+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on ead6510418b0:35171 (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.089+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:27.089+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 21 (ShuffledRDD[55] at countByKey at SparkHoodieBloomIndexHelper.java:197) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:27.089+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Adding task set 21.0 with 2 tasks resource profile 0
[2025-12-09T20:00:27.090+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 53) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:27.091+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 1.0 in stage 21.0 (TID 54) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:27.103+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on spark-worker:33701 (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.110+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 172.19.0.9:46230
[2025-12-09T20:00:27.119+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 1.0 in stage 21.0 (TID 54) in 28 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:27.121+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 53) in 31 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:27.122+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool
[2025-12-09T20:00:27.122+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: ResultStage 21 (countByKey at SparkHoodieBloomIndexHelper.java:197) finished in 0.039 s
[2025-12-09T20:00:27.122+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Job 14 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:27.122+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 21: Stage finished
[2025-12-09T20:00:27.123+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Job 14 finished: countByKey at SparkHoodieBloomIndexHelper.java:197, took 0.093286 s
[2025-12-09T20:00:27.125+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BucketizedBloomCheckPartitioner: TotalBuckets 0, min_buckets/partition 1
[2025-12-09T20:00:27.181+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MapPartitionsRDD: Removing RDD 44 from persistence list
[2025-12-09T20:00:27.183+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MapPartitionsRDD: Removing RDD 62 from persistence list
[2025-12-09T20:00:27.184+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManager: Removing RDD 44
[2025-12-09T20:00:27.185+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:27.186+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManager: Removing RDD 62
[2025-12-09T20:00:27.195+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:27.212+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-12-09T20:00:27.214+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Registering RDD 56 (mapToPair at SparkHoodieBloomIndexHelper.java:166) as input to shuffle 8
[2025-12-09T20:00:27.215+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Registering RDD 62 (flatMapToPair at SparkHoodieBloomIndexHelper.java:177) as input to shuffle 6
[2025-12-09T20:00:27.215+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Registering RDD 63 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 5
[2025-12-09T20:00:27.215+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Registering RDD 72 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 7
[2025-12-09T20:00:27.216+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Got job 15 (countByKey at HoodieJavaPairRDD.java:105) with 2 output partitions
[2025-12-09T20:00:27.216+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Final stage: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105)
[2025-12-09T20:00:27.216+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 26)
[2025-12-09T20:00:27.217+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 26)
[2025-12-09T20:00:27.218+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[63] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-12-09T20:00:27.220+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 26.8 KiB, free 433.7 MiB)
[2025-12-09T20:00:27.223+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 433.7 MiB)
[2025-12-09T20:00:27.225+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on ead6510418b0:35171 (size: 12.8 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.226+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:27.226+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[63] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:27.226+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Adding task set 25.0 with 2 tasks resource profile 0
[2025-12-09T20:00:27.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 55) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:27.229+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 56) (spark-worker, executor 0, partition 1, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:27.246+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on spark-worker:33701 (size: 12.8 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.256+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 172.19.0.9:46230
[2025-12-09T20:00:27.280+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 55) in 52 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:27.281+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 56) in 53 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:27.282+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool
[2025-12-09T20:00:27.282+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: ShuffleMapStage 25 (mapToPair at HoodieJavaRDD.java:149) finished in 0.064 s
[2025-12-09T20:00:27.282+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:27.283+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:27.283+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: waiting: Set(ResultStage 27, ShuffleMapStage 26)
[2025-12-09T20:00:27.283+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:27.283+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting ShuffleMapStage 26 (MapPartitionsRDD[72] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:27.287+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 10.0 KiB, free 433.7 MiB)
[2025-12-09T20:00:27.290+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 5.2 KiB, free 433.7 MiB)
[2025-12-09T20:00:27.292+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on ead6510418b0:35171 (size: 5.2 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.293+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:27.293+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 26 (MapPartitionsRDD[72] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:27.293+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Adding task set 26.0 with 2 tasks resource profile 0
[2025-12-09T20:00:27.296+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 57) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:27.296+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 1.0 in stage 26.0 (TID 58) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:27.312+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on spark-worker:33701 (size: 5.2 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.333+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 172.19.0.9:46230
[2025-12-09T20:00:27.340+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 172.19.0.9:46230
[2025-12-09T20:00:27.369+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added rdd_70_1 in memory on spark-worker:33701 (size: 4.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.370+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added rdd_70_0 in memory on spark-worker:33701 (size: 3.6 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.382+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 1.0 in stage 26.0 (TID 58) in 85 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:27.384+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 57) in 90 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:27.384+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool
[2025-12-09T20:00:27.385+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: ShuffleMapStage 26 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.100 s
[2025-12-09T20:00:27.385+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:27.385+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:27.385+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: waiting: Set(ResultStage 27)
[2025-12-09T20:00:27.385+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:27.385+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting ResultStage 27 (ShuffledRDD[73] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:27.387+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 5.5 KiB, free 433.7 MiB)
[2025-12-09T20:00:27.390+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.7 MiB)
[2025-12-09T20:00:27.392+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on ead6510418b0:35171 (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.392+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:27.392+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 27 (ShuffledRDD[73] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:27.393+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Adding task set 27.0 with 2 tasks resource profile 0
[2025-12-09T20:00:27.394+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 59) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:27.394+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 1.0 in stage 27.0 (TID 60) (spark-worker, executor 0, partition 1, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:27.410+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on spark-worker:33701 (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.415+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 172.19.0.9:46230
[2025-12-09T20:00:27.426+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_19_piece0 on ead6510418b0:35171 in memory (size: 12.8 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.431+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_19_piece0 on spark-worker:33701 in memory (size: 12.8 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.448+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_17_piece0 on ead6510418b0:35171 in memory (size: 13.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.450+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_17_piece0 on spark-worker:33701 in memory (size: 13.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.452+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 59) in 58 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:27.456+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 1.0 in stage 27.0 (TID 60) in 62 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:27.456+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool
[2025-12-09T20:00:27.457+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: ResultStage 27 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.071 s
[2025-12-09T20:00:27.457+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:27.458+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 27: Stage finished
[2025-12-09T20:00:27.458+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Job 15 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.245282 s
[2025-12-09T20:00:27.461+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO UpsertPartitioner: AvgRecordSize => 1024
[2025-12-09T20:00:27.462+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_16_piece0 on ead6510418b0:35171 in memory (size: 158.0 KiB, free: 434.4 MiB)
[2025-12-09T20:00:27.466+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_16_piece0 on spark-worker:33701 in memory (size: 158.0 KiB, free: 434.4 MiB)
[2025-12-09T20:00:27.477+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_20_piece0 on ead6510418b0:35171 in memory (size: 5.2 KiB, free: 434.4 MiB)
[2025-12-09T20:00:27.481+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_20_piece0 on spark-worker:33701 in memory (size: 5.2 KiB, free: 434.4 MiB)
[2025-12-09T20:00:27.488+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_18_piece0 on ead6510418b0:35171 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:27.491+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Removed broadcast_18_piece0 on spark-worker:33701 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:27.514+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
[2025-12-09T20:00:27.515+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Got job 16 (collectAsMap at UpsertPartitioner.java:282) with 31 output partitions
[2025-12-09T20:00:27.515+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Final stage: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282)
[2025-12-09T20:00:27.515+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:27.516+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:27.516+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting ResultStage 28 (MapPartitionsRDD[75] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
[2025-12-09T20:00:27.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 457.0 KiB, free 433.9 MiB)
[2025-12-09T20:00:27.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 159.1 KiB, free 433.8 MiB)
[2025-12-09T20:00:27.534+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on ead6510418b0:35171 (size: 159.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.534+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:27.535+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting 31 missing tasks from ResultStage 28 (MapPartitionsRDD[75] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-12-09T20:00:27.535+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Adding task set 28.0 with 31 tasks resource profile 0
[2025-12-09T20:00:27.536+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 61) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7267 bytes)
[2025-12-09T20:00:27.536+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 62) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7266 bytes)
[2025-12-09T20:00:27.537+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 2.0 in stage 28.0 (TID 63) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:27.537+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 3.0 in stage 28.0 (TID 64) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:27.551+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on spark-worker:33701 (size: 159.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:27.619+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 4.0 in stage 28.0 (TID 65) (spark-worker, executor 0, partition 4, PROCESS_LOCAL, 7263 bytes)
[2025-12-09T20:00:27.619+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 3.0 in stage 28.0 (TID 64) in 82 ms on spark-worker (executor 0) (1/31)
[2025-12-09T20:00:27.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 5.0 in stage 28.0 (TID 66) (spark-worker, executor 0, partition 5, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:27.626+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 6.0 in stage 28.0 (TID 67) (spark-worker, executor 0, partition 6, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:27.627+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 62) in 91 ms on spark-worker (executor 0) (2/31)
[2025-12-09T20:00:27.630+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 7.0 in stage 28.0 (TID 68) (spark-worker, executor 0, partition 7, PROCESS_LOCAL, 7268 bytes)
[2025-12-09T20:00:27.630+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 61) in 94 ms on spark-worker (executor 0) (3/31)
[2025-12-09T20:00:27.631+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 2.0 in stage 28.0 (TID 63) in 95 ms on spark-worker (executor 0) (4/31)
[2025-12-09T20:00:27.636+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 8.0 in stage 28.0 (TID 69) (spark-worker, executor 0, partition 8, PROCESS_LOCAL, 7258 bytes)
[2025-12-09T20:00:27.636+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 4.0 in stage 28.0 (TID 65) in 18 ms on spark-worker (executor 0) (5/31)
[2025-12-09T20:00:27.642+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 9.0 in stage 28.0 (TID 70) (spark-worker, executor 0, partition 9, PROCESS_LOCAL, 7266 bytes)
[2025-12-09T20:00:27.643+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 5.0 in stage 28.0 (TID 66) in 22 ms on spark-worker (executor 0) (6/31)
[2025-12-09T20:00:27.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 10.0 in stage 28.0 (TID 71) (spark-worker, executor 0, partition 10, PROCESS_LOCAL, 7266 bytes)
[2025-12-09T20:00:27.653+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 11.0 in stage 28.0 (TID 72) (spark-worker, executor 0, partition 11, PROCESS_LOCAL, 7259 bytes)
[2025-12-09T20:00:27.654+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 6.0 in stage 28.0 (TID 67) in 31 ms on spark-worker (executor 0) (7/31)
[2025-12-09T20:00:27.655+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 12.0 in stage 28.0 (TID 73) (spark-worker, executor 0, partition 12, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:27.655+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 7.0 in stage 28.0 (TID 68) in 28 ms on spark-worker (executor 0) (8/31)
[2025-12-09T20:00:27.658+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 13.0 in stage 28.0 (TID 74) (spark-worker, executor 0, partition 13, PROCESS_LOCAL, 7260 bytes)
[2025-12-09T20:00:27.659+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 8.0 in stage 28.0 (TID 69) in 25 ms on spark-worker (executor 0) (9/31)
[2025-12-09T20:00:27.659+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 9.0 in stage 28.0 (TID 70) in 21 ms on spark-worker (executor 0) (10/31)
[2025-12-09T20:00:27.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 14.0 in stage 28.0 (TID 75) (spark-worker, executor 0, partition 14, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:27.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 10.0 in stage 28.0 (TID 71) in 22 ms on spark-worker (executor 0) (11/31)
[2025-12-09T20:00:27.673+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 15.0 in stage 28.0 (TID 76) (spark-worker, executor 0, partition 15, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:27.679+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 16.0 in stage 28.0 (TID 77) (spark-worker, executor 0, partition 16, PROCESS_LOCAL, 7258 bytes)
[2025-12-09T20:00:27.679+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 11.0 in stage 28.0 (TID 72) in 28 ms on spark-worker (executor 0) (12/31)
[2025-12-09T20:00:27.681+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 17.0 in stage 28.0 (TID 78) (spark-worker, executor 0, partition 17, PROCESS_LOCAL, 7268 bytes)
[2025-12-09T20:00:27.682+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 13.0 in stage 28.0 (TID 74) in 25 ms on spark-worker (executor 0) (13/31)
[2025-12-09T20:00:27.682+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 12.0 in stage 28.0 (TID 73) in 28 ms on spark-worker (executor 0) (14/31)
[2025-12-09T20:00:27.689+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 18.0 in stage 28.0 (TID 79) (spark-worker, executor 0, partition 18, PROCESS_LOCAL, 7263 bytes)
[2025-12-09T20:00:27.692+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 14.0 in stage 28.0 (TID 75) in 26 ms on spark-worker (executor 0) (15/31)
[2025-12-09T20:00:27.695+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 19.0 in stage 28.0 (TID 80) (spark-worker, executor 0, partition 19, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:27.698+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 20.0 in stage 28.0 (TID 81) (spark-worker, executor 0, partition 20, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:27.699+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 15.0 in stage 28.0 (TID 76) in 30 ms on spark-worker (executor 0) (16/31)
[2025-12-09T20:00:27.699+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 16.0 in stage 28.0 (TID 77) in 25 ms on spark-worker (executor 0) (17/31)
[2025-12-09T20:00:27.703+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 21.0 in stage 28.0 (TID 82) (spark-worker, executor 0, partition 21, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:27.703+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 17.0 in stage 28.0 (TID 78) in 24 ms on spark-worker (executor 0) (18/31)
[2025-12-09T20:00:27.725+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 22.0 in stage 28.0 (TID 83) (spark-worker, executor 0, partition 22, PROCESS_LOCAL, 7260 bytes)
[2025-12-09T20:00:27.726+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 18.0 in stage 28.0 (TID 79) in 39 ms on spark-worker (executor 0) (19/31)
[2025-12-09T20:00:27.733+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 23.0 in stage 28.0 (TID 84) (spark-worker, executor 0, partition 23, PROCESS_LOCAL, 7258 bytes)
[2025-12-09T20:00:27.733+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 21.0 in stage 28.0 (TID 82) in 33 ms on spark-worker (executor 0) (20/31)
[2025-12-09T20:00:27.755+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 24.0 in stage 28.0 (TID 85) (spark-worker, executor 0, partition 24, PROCESS_LOCAL, 7264 bytes)
[2025-12-09T20:00:27.755+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 19.0 in stage 28.0 (TID 80) in 63 ms on spark-worker (executor 0) (21/31)
[2025-12-09T20:00:27.759+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 25.0 in stage 28.0 (TID 86) (spark-worker, executor 0, partition 25, PROCESS_LOCAL, 7259 bytes)
[2025-12-09T20:00:27.762+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 26.0 in stage 28.0 (TID 87) (spark-worker, executor 0, partition 26, PROCESS_LOCAL, 7264 bytes)
[2025-12-09T20:00:27.762+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 22.0 in stage 28.0 (TID 83) in 40 ms on spark-worker (executor 0) (22/31)
[2025-12-09T20:00:27.764+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 27.0 in stage 28.0 (TID 88) (spark-worker, executor 0, partition 27, PROCESS_LOCAL, 7265 bytes)
[2025-12-09T20:00:27.765+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 23.0 in stage 28.0 (TID 84) in 35 ms on spark-worker (executor 0) (23/31)
[2025-12-09T20:00:27.767+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 20.0 in stage 28.0 (TID 81) in 71 ms on spark-worker (executor 0) (24/31)
[2025-12-09T20:00:27.783+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 28.0 in stage 28.0 (TID 89) (spark-worker, executor 0, partition 28, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:27.784+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 24.0 in stage 28.0 (TID 85) in 32 ms on spark-worker (executor 0) (25/31)
[2025-12-09T20:00:27.826+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 29.0 in stage 28.0 (TID 90) (spark-worker, executor 0, partition 29, PROCESS_LOCAL, 7262 bytes)
[2025-12-09T20:00:27.827+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 25.0 in stage 28.0 (TID 86) in 71 ms on spark-worker (executor 0) (26/31)
[2025-12-09T20:00:27.844+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 30.0 in stage 28.0 (TID 91) (spark-worker, executor 0, partition 30, PROCESS_LOCAL, 7261 bytes)
[2025-12-09T20:00:27.846+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 26.0 in stage 28.0 (TID 87) in 85 ms on spark-worker (executor 0) (27/31)
[2025-12-09T20:00:27.848+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 27.0 in stage 28.0 (TID 88) in 84 ms on spark-worker (executor 0) (28/31)
[2025-12-09T20:00:27.860+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 28.0 in stage 28.0 (TID 89) in 78 ms on spark-worker (executor 0) (29/31)
[2025-12-09T20:00:27.862+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 29.0 in stage 28.0 (TID 90) in 39 ms on spark-worker (executor 0) (30/31)
[2025-12-09T20:00:27.866+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Finished task 30.0 in stage 28.0 (TID 91) in 25 ms on spark-worker (executor 0) (31/31)
[2025-12-09T20:00:27.867+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool
[2025-12-09T20:00:27.867+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: ResultStage 28 (collectAsMap at UpsertPartitioner.java:282) finished in 0.350 s
[2025-12-09T20:00:27.867+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:27.867+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Killing all running tasks in stage 28: Stage finished
[2025-12-09T20:00:27.868+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Job 16 finished: collectAsMap at UpsertPartitioner.java:282, took 0.353572 s
[2025-12-09T20:00:27.869+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:27.875+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:27.878+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO UpsertPartitioner: Total Buckets: 31
[2025-12-09T20:00:27.882+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/20251209200020437.commit.requested
[2025-12-09T20:00:27.912+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/20251209200020437.inflight
[2025-12-09T20:00:27.934+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-12-09T20:00:27.934+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BaseCommitActionExecutor: Auto commit disabled for 20251209200020437
[2025-12-09T20:00:27.941+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Starting job: count at HoodieSparkSqlWriter.scala:1050
[2025-12-09T20:00:27.943+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Registering RDD 76 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 9
[2025-12-09T20:00:27.943+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Got job 17 (count at HoodieSparkSqlWriter.scala:1050) with 31 output partitions
[2025-12-09T20:00:27.943+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Final stage: ResultStage 34 (count at HoodieSparkSqlWriter.scala:1050)
[2025-12-09T20:00:27.943+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 33)
[2025-12-09T20:00:27.944+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 33)
[2025-12-09T20:00:27.945+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[76] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-12-09T20:00:27.959+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 465.9 KiB, free 433.3 MiB)
[2025-12-09T20:00:27.963+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 160.9 KiB, free 433.2 MiB)
[2025-12-09T20:00:27.964+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on ead6510418b0:35171 (size: 160.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:27.966+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:27.966+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[76] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:27.967+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSchedulerImpl: Adding task set 33.0 with 2 tasks resource profile 0
[2025-12-09T20:00:27.969+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 0.0 in stage 33.0 (TID 92) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:27.969+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO TaskSetManager: Starting task 1.0 in stage 33.0 (TID 93) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:27.981+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:27 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on spark-worker:33701 (size: 160.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:28.067+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO TaskSetManager: Finished task 0.0 in stage 33.0 (TID 92) in 98 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:28.077+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO TaskSetManager: Finished task 1.0 in stage 33.0 (TID 93) in 108 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:28.077+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO TaskSchedulerImpl: Removed TaskSet 33.0, whose tasks have all completed, from pool
[2025-12-09T20:00:28.078+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO DAGScheduler: ShuffleMapStage 33 (mapToPair at HoodieJavaRDD.java:149) finished in 0.131 s
[2025-12-09T20:00:28.078+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:28.078+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:28.078+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO DAGScheduler: waiting: Set(ResultStage 34)
[2025-12-09T20:00:28.078+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:28.079+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO DAGScheduler: Submitting ResultStage 34 (MapPartitionsRDD[81] at filter at HoodieSparkSqlWriter.scala:1050), which has no missing parents
[2025-12-09T20:00:28.094+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 483.5 KiB, free 432.7 MiB)
[2025-12-09T20:00:28.097+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 171.2 KiB, free 432.5 MiB)
[2025-12-09T20:00:28.098+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on ead6510418b0:35171 (size: 171.2 KiB, free: 433.9 MiB)
[2025-12-09T20:00:28.099+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:28.099+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO DAGScheduler: Submitting 31 missing tasks from ResultStage 34 (MapPartitionsRDD[81] at filter at HoodieSparkSqlWriter.scala:1050) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-12-09T20:00:28.099+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO TaskSchedulerImpl: Adding task set 34.0 with 31 tasks resource profile 0
[2025-12-09T20:00:28.100+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO TaskSetManager: Starting task 0.0 in stage 34.0 (TID 94) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:28.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO TaskSetManager: Starting task 1.0 in stage 34.0 (TID 95) (spark-worker, executor 0, partition 1, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:28.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO TaskSetManager: Starting task 2.0 in stage 34.0 (TID 96) (spark-worker, executor 0, partition 2, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:28.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO TaskSetManager: Starting task 3.0 in stage 34.0 (TID 97) (spark-worker, executor 0, partition 3, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:28.114+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on spark-worker:33701 (size: 171.2 KiB, free: 433.9 MiB)
[2025-12-09T20:00:28.165+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 172.19.0.9:46230
[2025-12-09T20:00:28.557+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO MarkerHandler: Request: create marker: state=Rhode Island/9debb6d9-b222-485a-a198-8d19a479df3f-0_1-34-95_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:28.557+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO MarkerHandler: Request: create marker: state=West Virginia/3aa5361b-ab50-4d63-ba10-d611d2e12341-0_0-34-94_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:28.558+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO MarkerHandler: Request: create marker: state=Georgia/d29b29b6-c167-4989-86b7-fec041926c89-0_3-34-97_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:28.558+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:28 INFO MarkerHandler: Request: create marker: state=Vermont/beb0489a-dbdc-467a-aa85-799a6d3b0dd0-0_2-34-96_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.363+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_0 in memory on spark-worker:33701 (size: 334.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.371+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 4.0 in stage 34.0 (TID 98) (spark-worker, executor 0, partition 4, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.372+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 0.0 in stage 34.0 (TID 94) in 1272 ms on spark-worker (executor 0) (1/31)
[2025-12-09T20:00:29.381+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_1 in memory on spark-worker:33701 (size: 333.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.382+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_3 in memory on spark-worker:33701 (size: 327.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.387+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 5.0 in stage 34.0 (TID 99) (spark-worker, executor 0, partition 5, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.388+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 6.0 in stage 34.0 (TID 100) (spark-worker, executor 0, partition 6, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.390+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 1.0 in stage 34.0 (TID 95) in 1289 ms on spark-worker (executor 0) (2/31)
[2025-12-09T20:00:29.390+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 3.0 in stage 34.0 (TID 97) in 1289 ms on spark-worker (executor 0) (3/31)
[2025-12-09T20:00:29.409+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_2 in memory on spark-worker:33701 (size: 330.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.414+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 7.0 in stage 34.0 (TID 101) (spark-worker, executor 0, partition 7, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.417+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 2.0 in stage 34.0 (TID 96) in 1316 ms on spark-worker (executor 0) (4/31)
[2025-12-09T20:00:29.504+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO MarkerHandler: Request: create marker: state=Tennessee/143e2748-6698-40e3-a254-dcf11cbe3f20-0_4-34-98_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.516+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO MarkerHandler: Request: create marker: state=Alabama/05fd355c-1062-4a12-a4ac-568fe702ba62-0_5-34-99_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.535+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO MarkerHandler: Request: create marker: state=North Carolina/0a62ac59-b3ea-42d2-8208-3a034579237f-0_7-34-101_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.540+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO MarkerHandler: Request: create marker: state=Wyoming/29804e8e-2df8-4070-9889-21fe05f538d3-0_6-34-100_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.621+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_4 in memory on spark-worker:33701 (size: 326.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.633+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 8.0 in stage 34.0 (TID 102) (spark-worker, executor 0, partition 8, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.636+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 4.0 in stage 34.0 (TID 98) in 265 ms on spark-worker (executor 0) (5/31)
[2025-12-09T20:00:29.651+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_5 in memory on spark-worker:33701 (size: 326.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.656+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 9.0 in stage 34.0 (TID 103) (spark-worker, executor 0, partition 9, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.660+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 5.0 in stage 34.0 (TID 99) in 273 ms on spark-worker (executor 0) (6/31)
[2025-12-09T20:00:29.669+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_7 in memory on spark-worker:33701 (size: 336.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.686+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 10.0 in stage 34.0 (TID 104) (spark-worker, executor 0, partition 10, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.688+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_6 in memory on spark-worker:33701 (size: 327.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.690+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 7.0 in stage 34.0 (TID 101) in 276 ms on spark-worker (executor 0) (7/31)
[2025-12-09T20:00:29.695+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 11.0 in stage 34.0 (TID 105) (spark-worker, executor 0, partition 11, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.698+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 6.0 in stage 34.0 (TID 100) in 310 ms on spark-worker (executor 0) (8/31)
[2025-12-09T20:00:29.788+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO MarkerHandler: Request: create marker: state=Ohio/4a551826-608d-4b71-afb0-34f45caf60f8-0_8-34-102_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.803+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO MarkerHandler: Request: create marker: state=North Dakota/0d505583-460c-40d6-a0f2-7e5322840cf4-0_9-34-103_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.822+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO MarkerHandler: Request: create marker: state=South Dakota/48501371-4d2c-4b4b-815e-f7ee85edfcfb-0_10-34-104_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.826+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO MarkerHandler: Request: create marker: state=Idaho/e056d200-dd44-4fee-b7c5-b15d181dfbbc-0_11-34-105_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:29.916+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_8 in memory on spark-worker:33701 (size: 325.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.917+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_11 in memory on spark-worker:33701 (size: 326.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.922+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 12.0 in stage 34.0 (TID 106) (spark-worker, executor 0, partition 12, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.923+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 13.0 in stage 34.0 (TID 107) (spark-worker, executor 0, partition 13, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.925+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 11.0 in stage 34.0 (TID 105) in 231 ms on spark-worker (executor 0) (9/31)
[2025-12-09T20:00:29.926+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 8.0 in stage 34.0 (TID 102) in 293 ms on spark-worker (executor 0) (10/31)
[2025-12-09T20:00:29.927+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_10 in memory on spark-worker:33701 (size: 333.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 14.0 in stage 34.0 (TID 108) (spark-worker, executor 0, partition 14, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.936+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 10.0 in stage 34.0 (TID 104) in 250 ms on spark-worker (executor 0) (11/31)
[2025-12-09T20:00:29.953+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO BlockManagerInfo: Added rdd_80_9 in memory on spark-worker:33701 (size: 333.0 B, free: 433.9 MiB)
[2025-12-09T20:00:29.957+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Starting task 15.0 in stage 34.0 (TID 109) (spark-worker, executor 0, partition 15, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:29.960+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:29 INFO TaskSetManager: Finished task 9.0 in stage 34.0 (TID 103) in 305 ms on spark-worker (executor 0) (12/31)
[2025-12-09T20:00:30.050+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Oklahoma/22af776c-f274-4220-8ded-5c9394e0f2aa-0_12-34-106_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.051+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Nevada/e4580fef-c736-42c9-b61c-933fbd786c1f-0_13-34-107_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.082+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Virginia/5c241463-a437-44ea-8066-ca819afdb704-0_15-34-109_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.092+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Montana/a6164128-bfa9-4778-b610-1ad78a450607-0_14-34-108_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.174+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_12 in memory on spark-worker:33701 (size: 329.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.176+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_13 in memory on spark-worker:33701 (size: 328.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.180+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 16.0 in stage 34.0 (TID 110) (spark-worker, executor 0, partition 16, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.183+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 17.0 in stage 34.0 (TID 111) (spark-worker, executor 0, partition 17, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.184+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 12.0 in stage 34.0 (TID 106) in 262 ms on spark-worker (executor 0) (13/31)
[2025-12-09T20:00:30.186+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 13.0 in stage 34.0 (TID 107) in 264 ms on spark-worker (executor 0) (14/31)
[2025-12-09T20:00:30.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_15 in memory on spark-worker:33701 (size: 330.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.201+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 18.0 in stage 34.0 (TID 112) (spark-worker, executor 0, partition 18, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 15.0 in stage 34.0 (TID 109) in 248 ms on spark-worker (executor 0) (15/31)
[2025-12-09T20:00:30.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_14 in memory on spark-worker:33701 (size: 328.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.233+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 19.0 in stage 34.0 (TID 113) (spark-worker, executor 0, partition 19, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.236+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 14.0 in stage 34.0 (TID 108) in 303 ms on spark-worker (executor 0) (16/31)
[2025-12-09T20:00:30.300+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Iowa/c0693171-63f2-4303-860e-26ea4e131fc5-0_16-34-110_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.327+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Minnesota/38223a6a-4c06-453f-a571-90760bb43552-0_18-34-112_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.331+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=South Carolina/e1b0e50f-e8a9-4938-8b57-1e7323317b79-0_17-34-111_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.354+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Nebraska/cb1f5f51-d19a-48ec-8323-134f02c3a25c-0_19-34-113_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.400+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_18 in memory on spark-worker:33701 (size: 330.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.405+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 20.0 in stage 34.0 (TID 114) (spark-worker, executor 0, partition 20, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.408+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 18.0 in stage 34.0 (TID 112) in 206 ms on spark-worker (executor 0) (17/31)
[2025-12-09T20:00:30.438+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_16 in memory on spark-worker:33701 (size: 326.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.443+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 21.0 in stage 34.0 (TID 115) (spark-worker, executor 0, partition 21, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.444+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_17 in memory on spark-worker:33701 (size: 336.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.445+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 16.0 in stage 34.0 (TID 110) in 266 ms on spark-worker (executor 0) (18/31)
[2025-12-09T20:00:30.448+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 22.0 in stage 34.0 (TID 116) (spark-worker, executor 0, partition 22, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.449+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_19 in memory on spark-worker:33701 (size: 329.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.450+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 17.0 in stage 34.0 (TID 111) in 268 ms on spark-worker (executor 0) (19/31)
[2025-12-09T20:00:30.454+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 23.0 in stage 34.0 (TID 117) (spark-worker, executor 0, partition 23, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.456+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 19.0 in stage 34.0 (TID 113) in 223 ms on spark-worker (executor 0) (20/31)
[2025-12-09T20:00:30.531+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Maryland/0a79c8a6-d2e5-4817-81b2-81a413fd4700-0_20-34-114_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.574+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Utah/4bbc463b-b57e-4284-b034-c486d0dc93f4-0_23-34-117_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.577+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Oregon/290f7824-0994-4efc-906b-13629b925841-0_22-34-116_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.619+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_20 in memory on spark-worker:33701 (size: 329.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.624+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 24.0 in stage 34.0 (TID 118) (spark-worker, executor 0, partition 24, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.627+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 20.0 in stage 34.0 (TID 114) in 223 ms on spark-worker (executor 0) (21/31)
[2025-12-09T20:00:30.636+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Delaware/d50d1a72-3611-474f-bfd1-7e76a46e97c1-0_21-34-115_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.670+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_22 in memory on spark-worker:33701 (size: 328.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.676+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 25.0 in stage 34.0 (TID 119) (spark-worker, executor 0, partition 25, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.678+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 22.0 in stage 34.0 (TID 116) in 231 ms on spark-worker (executor 0) (22/31)
[2025-12-09T20:00:30.687+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_23 in memory on spark-worker:33701 (size: 326.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.693+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 26.0 in stage 34.0 (TID 120) (spark-worker, executor 0, partition 26, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.696+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 23.0 in stage 34.0 (TID 117) in 243 ms on spark-worker (executor 0) (23/31)
[2025-12-09T20:00:30.756+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=New Mexico/864b3b8e-712e-4256-bb94-601942a1a262-0_24-34-118_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.772+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_21 in memory on spark-worker:33701 (size: 329.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.779+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 27.0 in stage 34.0 (TID 121) (spark-worker, executor 0, partition 27, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.781+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Maine/32164e54-0bd2-4b6a-ba87-12d8244a9ea5-0_25-34-119_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.782+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 21.0 in stage 34.0 (TID 115) in 338 ms on spark-worker (executor 0) (24/31)
[2025-12-09T20:00:30.806+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=New Jersey/0f36ccc2-a72d-41d8-9d9e-502b002e69bd-0_26-34-120_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.880+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_24 in memory on spark-worker:33701 (size: 332.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.886+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 28.0 in stage 34.0 (TID 122) (spark-worker, executor 0, partition 28, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.888+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 24.0 in stage 34.0 (TID 118) in 265 ms on spark-worker (executor 0) (25/31)
[2025-12-09T20:00:30.899+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_25 in memory on spark-worker:33701 (size: 327.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.903+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO MarkerHandler: Request: create marker: state=Connecticut/d748cc5a-9c61-4969-8772-054aef92db41-0_27-34-121_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:30.903+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 29.0 in stage 34.0 (TID 123) (spark-worker, executor 0, partition 29, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.906+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 25.0 in stage 34.0 (TID 119) in 230 ms on spark-worker (executor 0) (26/31)
[2025-12-09T20:00:30.929+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO BlockManagerInfo: Added rdd_80_26 in memory on spark-worker:33701 (size: 331.0 B, free: 433.9 MiB)
[2025-12-09T20:00:30.934+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Starting task 30.0 in stage 34.0 (TID 124) (spark-worker, executor 0, partition 30, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:30.937+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:30 INFO TaskSetManager: Finished task 26.0 in stage 34.0 (TID 120) in 244 ms on spark-worker (executor 0) (27/31)
[2025-12-09T20:00:31.010+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO MarkerHandler: Request: create marker: state=Florida/4cca9850-f780-4165-aeaa-0afa4c4d6dc8-0_28-34-122_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:31.017+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO MarkerHandler: Request: create marker: state=New York/e4e75630-ff36-4496-b060-edd059cf9019-0_29-34-123_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:31.018+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Added rdd_80_27 in memory on spark-worker:33701 (size: 332.0 B, free: 433.9 MiB)
[2025-12-09T20:00:31.026+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 27.0 in stage 34.0 (TID 121) in 248 ms on spark-worker (executor 0) (28/31)
[2025-12-09T20:00:31.056+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO MarkerHandler: Request: create marker: state=Arizona/ce93fe6c-287c-42de-b603-fef4f86644ae-0_30-34-124_20251209200020437.parquet.marker.CREATE
[2025-12-09T20:00:31.112+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Added rdd_80_28 in memory on spark-worker:33701 (size: 328.0 B, free: 433.9 MiB)
[2025-12-09T20:00:31.112+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Added rdd_80_29 in memory on spark-worker:33701 (size: 330.0 B, free: 433.9 MiB)
[2025-12-09T20:00:31.118+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 28.0 in stage 34.0 (TID 122) in 233 ms on spark-worker (executor 0) (29/31)
[2025-12-09T20:00:31.119+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 29.0 in stage 34.0 (TID 123) in 216 ms on spark-worker (executor 0) (30/31)
[2025-12-09T20:00:31.183+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Added rdd_80_30 in memory on spark-worker:33701 (size: 328.0 B, free: 433.9 MiB)
[2025-12-09T20:00:31.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 30.0 in stage 34.0 (TID 124) in 258 ms on spark-worker (executor 0) (31/31)
[2025-12-09T20:00:31.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSchedulerImpl: Removed TaskSet 34.0, whose tasks have all completed, from pool
[2025-12-09T20:00:31.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: ResultStage 34 (count at HoodieSparkSqlWriter.scala:1050) finished in 3.112 s
[2025-12-09T20:00:31.194+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Job 17 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:31.195+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 34: Stage finished
[2025-12-09T20:00:31.195+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Job 17 finished: count at HoodieSparkSqlWriter.scala:1050, took 3.253475 s
[2025-12-09T20:00:31.195+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieSparkSqlWriter$: Proceeding to commit the write.
[2025-12-09T20:00:31.232+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
[2025-12-09T20:00:31.235+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Got job 18 (collect at SparkRDDWriteClient.java:103) with 31 output partitions
[2025-12-09T20:00:31.236+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Final stage: ResultStage 40 (collect at SparkRDDWriteClient.java:103)
[2025-12-09T20:00:31.236+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)
[2025-12-09T20:00:31.236+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:31.237+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[82] at map at SparkRDDWriteClient.java:103), which has no missing parents
[2025-12-09T20:00:31.254+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 484.2 KiB, free 432.1 MiB)
[2025-12-09T20:00:31.260+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 171.6 KiB, free 431.9 MiB)
[2025-12-09T20:00:31.261+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on ead6510418b0:35171 (size: 171.6 KiB, free: 433.7 MiB)
[2025-12-09T20:00:31.262+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:31.262+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Submitting 31 missing tasks from ResultStage 40 (MapPartitionsRDD[82] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14))
[2025-12-09T20:00:31.263+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSchedulerImpl: Adding task set 40.0 with 31 tasks resource profile 0
[2025-12-09T20:00:31.265+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 0.0 in stage 40.0 (TID 125) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.265+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 1.0 in stage 40.0 (TID 126) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.265+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 2.0 in stage 40.0 (TID 127) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.266+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 3.0 in stage 40.0 (TID 128) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.282+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on spark-worker:33701 (size: 171.6 KiB, free: 433.7 MiB)
[2025-12-09T20:00:31.324+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 4.0 in stage 40.0 (TID 129) (spark-worker, executor 0, partition 4, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.325+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 5.0 in stage 40.0 (TID 130) (spark-worker, executor 0, partition 5, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.326+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 1.0 in stage 40.0 (TID 126) in 60 ms on spark-worker (executor 0) (1/31)
[2025-12-09T20:00:31.326+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 6.0 in stage 40.0 (TID 131) (spark-worker, executor 0, partition 6, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.327+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 7.0 in stage 40.0 (TID 132) (spark-worker, executor 0, partition 7, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.330+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 3.0 in stage 40.0 (TID 128) in 64 ms on spark-worker (executor 0) (2/31)
[2025-12-09T20:00:31.331+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 0.0 in stage 40.0 (TID 125) in 66 ms on spark-worker (executor 0) (3/31)
[2025-12-09T20:00:31.332+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 2.0 in stage 40.0 (TID 127) in 66 ms on spark-worker (executor 0) (4/31)
[2025-12-09T20:00:31.350+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 8.0 in stage 40.0 (TID 133) (spark-worker, executor 0, partition 8, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.351+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 9.0 in stage 40.0 (TID 134) (spark-worker, executor 0, partition 9, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.352+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 10.0 in stage 40.0 (TID 135) (spark-worker, executor 0, partition 10, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.352+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 11.0 in stage 40.0 (TID 136) (spark-worker, executor 0, partition 11, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.365+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 5.0 in stage 40.0 (TID 130) in 39 ms on spark-worker (executor 0) (5/31)
[2025-12-09T20:00:31.365+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 7.0 in stage 40.0 (TID 132) in 39 ms on spark-worker (executor 0) (6/31)
[2025-12-09T20:00:31.366+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 4.0 in stage 40.0 (TID 129) in 43 ms on spark-worker (executor 0) (7/31)
[2025-12-09T20:00:31.366+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 6.0 in stage 40.0 (TID 131) in 40 ms on spark-worker (executor 0) (8/31)
[2025-12-09T20:00:31.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Removed broadcast_23_piece0 on ead6510418b0:35171 in memory (size: 160.9 KiB, free: 433.9 MiB)
[2025-12-09T20:00:31.372+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Removed broadcast_23_piece0 on spark-worker:33701 in memory (size: 160.9 KiB, free: 433.9 MiB)
[2025-12-09T20:00:31.381+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Removed broadcast_24_piece0 on ead6510418b0:35171 in memory (size: 171.2 KiB, free: 434.1 MiB)
[2025-12-09T20:00:31.385+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Removed broadcast_24_piece0 on spark-worker:33701 in memory (size: 171.2 KiB, free: 434.1 MiB)
[2025-12-09T20:00:31.386+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 12.0 in stage 40.0 (TID 137) (spark-worker, executor 0, partition 12, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.387+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 13.0 in stage 40.0 (TID 138) (spark-worker, executor 0, partition 13, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.388+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 14.0 in stage 40.0 (TID 139) (spark-worker, executor 0, partition 14, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.388+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 15.0 in stage 40.0 (TID 140) (spark-worker, executor 0, partition 15, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.391+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 8.0 in stage 40.0 (TID 133) in 40 ms on spark-worker (executor 0) (9/31)
[2025-12-09T20:00:31.391+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 9.0 in stage 40.0 (TID 134) in 39 ms on spark-worker (executor 0) (10/31)
[2025-12-09T20:00:31.392+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 10.0 in stage 40.0 (TID 135) in 40 ms on spark-worker (executor 0) (11/31)
[2025-12-09T20:00:31.394+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 11.0 in stage 40.0 (TID 136) in 40 ms on spark-worker (executor 0) (12/31)
[2025-12-09T20:00:31.394+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Removed broadcast_21_piece0 on ead6510418b0:35171 in memory (size: 3.1 KiB, free: 434.1 MiB)
[2025-12-09T20:00:31.397+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Removed broadcast_21_piece0 on spark-worker:33701 in memory (size: 3.1 KiB, free: 434.1 MiB)
[2025-12-09T20:00:31.404+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Removed broadcast_22_piece0 on ead6510418b0:35171 in memory (size: 159.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:31.406+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BlockManagerInfo: Removed broadcast_22_piece0 on spark-worker:33701 in memory (size: 159.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:31.411+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 16.0 in stage 40.0 (TID 141) (spark-worker, executor 0, partition 16, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.412+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 17.0 in stage 40.0 (TID 142) (spark-worker, executor 0, partition 17, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.412+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 18.0 in stage 40.0 (TID 143) (spark-worker, executor 0, partition 18, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.413+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 19.0 in stage 40.0 (TID 144) (spark-worker, executor 0, partition 19, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.414+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 14.0 in stage 40.0 (TID 139) in 26 ms on spark-worker (executor 0) (13/31)
[2025-12-09T20:00:31.415+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 12.0 in stage 40.0 (TID 137) in 28 ms on spark-worker (executor 0) (14/31)
[2025-12-09T20:00:31.416+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 13.0 in stage 40.0 (TID 138) in 28 ms on spark-worker (executor 0) (15/31)
[2025-12-09T20:00:31.417+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 15.0 in stage 40.0 (TID 140) in 28 ms on spark-worker (executor 0) (16/31)
[2025-12-09T20:00:31.433+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 20.0 in stage 40.0 (TID 145) (spark-worker, executor 0, partition 20, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.434+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 21.0 in stage 40.0 (TID 146) (spark-worker, executor 0, partition 21, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.436+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 22.0 in stage 40.0 (TID 147) (spark-worker, executor 0, partition 22, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.436+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 17.0 in stage 40.0 (TID 142) in 24 ms on spark-worker (executor 0) (17/31)
[2025-12-09T20:00:31.437+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 23.0 in stage 40.0 (TID 148) (spark-worker, executor 0, partition 23, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.437+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 16.0 in stage 40.0 (TID 141) in 27 ms on spark-worker (executor 0) (18/31)
[2025-12-09T20:00:31.438+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 18.0 in stage 40.0 (TID 143) in 25 ms on spark-worker (executor 0) (19/31)
[2025-12-09T20:00:31.441+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 19.0 in stage 40.0 (TID 144) in 28 ms on spark-worker (executor 0) (20/31)
[2025-12-09T20:00:31.456+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 24.0 in stage 40.0 (TID 149) (spark-worker, executor 0, partition 24, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.458+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 25.0 in stage 40.0 (TID 150) (spark-worker, executor 0, partition 25, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.459+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 26.0 in stage 40.0 (TID 151) (spark-worker, executor 0, partition 26, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.459+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 27.0 in stage 40.0 (TID 152) (spark-worker, executor 0, partition 27, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.460+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 22.0 in stage 40.0 (TID 147) in 25 ms on spark-worker (executor 0) (21/31)
[2025-12-09T20:00:31.460+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 20.0 in stage 40.0 (TID 145) in 27 ms on spark-worker (executor 0) (22/31)
[2025-12-09T20:00:31.460+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 21.0 in stage 40.0 (TID 146) in 26 ms on spark-worker (executor 0) (23/31)
[2025-12-09T20:00:31.461+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 23.0 in stage 40.0 (TID 148) in 25 ms on spark-worker (executor 0) (24/31)
[2025-12-09T20:00:31.478+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 28.0 in stage 40.0 (TID 153) (spark-worker, executor 0, partition 28, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.480+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 29.0 in stage 40.0 (TID 154) (spark-worker, executor 0, partition 29, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.481+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Starting task 30.0 in stage 40.0 (TID 155) (spark-worker, executor 0, partition 30, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:31.481+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 25.0 in stage 40.0 (TID 150) in 24 ms on spark-worker (executor 0) (25/31)
[2025-12-09T20:00:31.483+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 24.0 in stage 40.0 (TID 149) in 27 ms on spark-worker (executor 0) (26/31)
[2025-12-09T20:00:31.484+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 27.0 in stage 40.0 (TID 152) in 25 ms on spark-worker (executor 0) (27/31)
[2025-12-09T20:00:31.485+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 26.0 in stage 40.0 (TID 151) in 26 ms on spark-worker (executor 0) (28/31)
[2025-12-09T20:00:31.501+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 29.0 in stage 40.0 (TID 154) in 22 ms on spark-worker (executor 0) (29/31)
[2025-12-09T20:00:31.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 28.0 in stage 40.0 (TID 153) in 23 ms on spark-worker (executor 0) (30/31)
[2025-12-09T20:00:31.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSetManager: Finished task 30.0 in stage 40.0 (TID 155) in 22 ms on spark-worker (executor 0) (31/31)
[2025-12-09T20:00:31.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSchedulerImpl: Removed TaskSet 40.0, whose tasks have all completed, from pool
[2025-12-09T20:00:31.504+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: ResultStage 40 (collect at SparkRDDWriteClient.java:103) finished in 0.265 s
[2025-12-09T20:00:31.504+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:31.504+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO TaskSchedulerImpl: Killing all running tasks in stage 40: Stage finished
[2025-12-09T20:00:31.504+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO DAGScheduler: Job 18 finished: collect at SparkRDDWriteClient.java:103, took 0.272315 s
[2025-12-09T20:00:31.505+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BaseHoodieWriteClient: Committing 20251209200020437 action commit
[2025-12-09T20:00:31.505+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.517+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:31.526+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.527+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__INFLIGHT__20251209200027900]}
[2025-12-09T20:00:31.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.542+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:31.555+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.555+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:31.565+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:31.573+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:31.581+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:31.581+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:31.581+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:31.582+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:31.582+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:31.582+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:31 numReplaceFileIds:0
[2025-12-09T20:00:31.583+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.591+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:31.598+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.598+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.603+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__commit__INFLIGHT__20251209200027900]}
[2025-12-09T20:00:31.604+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.612+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:31.619+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.619+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:31.627+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:31.632+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:31.636+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:31.637+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:31.637+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:31.637+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:31.637+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:31.637+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BaseHoodieWriteClient: Committing 20251209200020437 action commit
[2025-12-09T20:00:31.799+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.806+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:31.814+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.818+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.827+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:31.834+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.841+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:31.841+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
[2025-12-09T20:00:31.841+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.848+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:31.858+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:31.859+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:31.866+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:31.874+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:31.881+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:31.881+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:31.882+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:31.891+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetadataUtil: Updating at 20251209200020437 from Commit/UPSERT. #partitions_updated=32, #files_added=31
[2025-12-09T20:00:31.915+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:31.916+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:31.916+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
[2025-12-09T20:00:31.916+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:31.916+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:31.916+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO AbstractTableFileSystemView: Building file system view for partition (files)
[2025-12-09T20:00:31.929+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-12-09T20:00:31.929+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-12-09T20:00:31.930+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.935+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:31.941+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.942+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.953+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:31.961+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:31.967+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:31.967+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:31.968+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieBackedTableMetadataWriter: New commit at 20251209200020437 being applied to MDT.
[2025-12-09T20:00:31.968+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.973+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:31.980+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.981+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.986+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:31.987+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO CleanerUtils: Cleaned failed attempts if any
[2025-12-09T20:00:31.987+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:31.997+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:31 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:32.006+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:32.006+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:32.013+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200024992]}
[2025-12-09T20:00:32.022+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:32.031+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:32.031+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:32.032+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BaseHoodieWriteClient: Generate a new instant time: 20251209200020437 action: deltacommit
[2025-12-09T20:00:32.032+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieActiveTimeline: Creating a new instant [==>20251209200020437__deltacommit__REQUESTED]
[2025-12-09T20:00:32.056+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:32.063+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:32.068+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:32.068+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:32.075+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200020437__deltacommit__REQUESTED__20251209200032044]}
[2025-12-09T20:00:32.081+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:32.088+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:32.088+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:32.088+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
[2025-12-09T20:00:32.088+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-12-09T20:00:32.090+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:32.091+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:32.099+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-12-09T20:00:32.100+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Registering RDD 93 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 10
[2025-12-09T20:00:32.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Got job 19 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
[2025-12-09T20:00:32.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Final stage: ResultStage 42 (countByKey at HoodieJavaPairRDD.java:105)
[2025-12-09T20:00:32.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 41)
[2025-12-09T20:00:32.101+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 41)
[2025-12-09T20:00:32.102+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting ShuffleMapStage 41 (MapPartitionsRDD[93] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:32.104+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 10.1 KiB, free 433.7 MiB)
[2025-12-09T20:00:32.107+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 433.7 MiB)
[2025-12-09T20:00:32.108+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on ead6510418b0:35171 (size: 5.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:32.109+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:32.109+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 41 (MapPartitionsRDD[93] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:32.109+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Adding task set 41.0 with 1 tasks resource profile 0
[2025-12-09T20:00:32.114+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Starting task 0.0 in stage 41.0 (TID 156) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 12094 bytes)
[2025-12-09T20:00:32.132+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on spark-worker:33701 (size: 5.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:32.141+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added rdd_91_0 in memory on spark-worker:33701 (size: 2.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:32.152+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Finished task 0.0 in stage 41.0 (TID 156) in 42 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:32.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Removed TaskSet 41.0, whose tasks have all completed, from pool
[2025-12-09T20:00:32.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: ShuffleMapStage 41 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.050 s
[2025-12-09T20:00:32.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:32.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:32.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: waiting: Set(ResultStage 42)
[2025-12-09T20:00:32.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:32.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting ResultStage 42 (ShuffledRDD[94] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:32.155+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 5.5 KiB, free 433.7 MiB)
[2025-12-09T20:00:32.157+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 433.7 MiB)
[2025-12-09T20:00:32.158+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on ead6510418b0:35171 (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:32.159+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:32.160+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 42 (ShuffledRDD[94] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:32.160+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Adding task set 42.0 with 1 tasks resource profile 0
[2025-12-09T20:00:32.162+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Starting task 0.0 in stage 42.0 (TID 157) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:32.176+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on spark-worker:33701 (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:32.181+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 172.19.0.9:46230
[2025-12-09T20:00:32.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Finished task 0.0 in stage 42.0 (TID 157) in 32 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:32.194+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Removed TaskSet 42.0, whose tasks have all completed, from pool
[2025-12-09T20:00:32.194+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: ResultStage 42 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.040 s
[2025-12-09T20:00:32.194+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:32.194+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 42: Stage finished
[2025-12-09T20:00:32.195+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Job 19 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.095080 s
[2025-12-09T20:00:32.196+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO UpsertPartitioner: AvgRecordSize => 1024
[2025-12-09T20:00:32.226+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
[2025-12-09T20:00:32.227+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Got job 20 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
[2025-12-09T20:00:32.227+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Final stage: ResultStage 43 (collectAsMap at UpsertPartitioner.java:282)
[2025-12-09T20:00:32.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:32.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:32.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting ResultStage 43 (MapPartitionsRDD[96] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
[2025-12-09T20:00:32.237+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 352.5 KiB, free 433.4 MiB)
[2025-12-09T20:00:32.241+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 123.6 KiB, free 433.3 MiB)
[2025-12-09T20:00:32.242+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on ead6510418b0:35171 (size: 123.6 KiB, free: 434.1 MiB)
[2025-12-09T20:00:32.243+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:32.243+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 43 (MapPartitionsRDD[96] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:32.243+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Adding task set 43.0 with 1 tasks resource profile 0
[2025-12-09T20:00:32.244+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Starting task 0.0 in stage 43.0 (TID 158) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7253 bytes)
[2025-12-09T20:00:32.255+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on spark-worker:33701 (size: 123.6 KiB, free: 434.1 MiB)
[2025-12-09T20:00:32.295+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Finished task 0.0 in stage 43.0 (TID 158) in 51 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:32.296+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Removed TaskSet 43.0, whose tasks have all completed, from pool
[2025-12-09T20:00:32.296+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: ResultStage 43 (collectAsMap at UpsertPartitioner.java:282) finished in 0.067 s
[2025-12-09T20:00:32.297+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Job 20 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:32.297+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Killing all running tasks in stage 43: Stage finished
[2025-12-09T20:00:32.297+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Job 20 finished: collectAsMap at UpsertPartitioner.java:282, took 0.070682 s
[2025-12-09T20:00:32.297+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:32.298+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:32.298+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO UpsertPartitioner: Total Buckets: 1
[2025-12-09T20:00:32.298+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/20251209200020437.deltacommit.requested
[2025-12-09T20:00:32.324+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/20251209200020437.deltacommit.inflight
[2025-12-09T20:00:32.348+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-12-09T20:00:32.348+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20251209200020437
[2025-12-09T20:00:32.373+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
[2025-12-09T20:00:32.375+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Registering RDD 97 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 11
[2025-12-09T20:00:32.376+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Got job 21 (collect at HoodieJavaRDD.java:177) with 1 output partitions
[2025-12-09T20:00:32.376+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Final stage: ResultStage 45 (collect at HoodieJavaRDD.java:177)
[2025-12-09T20:00:32.376+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 44)
[2025-12-09T20:00:32.377+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 44)
[2025-12-09T20:00:32.378+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting ShuffleMapStage 44 (MapPartitionsRDD[97] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-12-09T20:00:32.387+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 357.3 KiB, free 432.9 MiB)
[2025-12-09T20:00:32.391+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 123.9 KiB, free 432.8 MiB)
[2025-12-09T20:00:32.392+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on ead6510418b0:35171 (size: 123.9 KiB, free: 434.0 MiB)
[2025-12-09T20:00:32.393+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:32.393+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 44 (MapPartitionsRDD[97] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:32.393+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Adding task set 44.0 with 1 tasks resource profile 0
[2025-12-09T20:00:32.395+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Starting task 0.0 in stage 44.0 (TID 159) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 12094 bytes)
[2025-12-09T20:00:32.409+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on spark-worker:33701 (size: 123.9 KiB, free: 434.0 MiB)
[2025-12-09T20:00:32.427+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Finished task 0.0 in stage 44.0 (TID 159) in 33 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:32.427+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Removed TaskSet 44.0, whose tasks have all completed, from pool
[2025-12-09T20:00:32.428+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: ShuffleMapStage 44 (mapToPair at HoodieJavaRDD.java:149) finished in 0.049 s
[2025-12-09T20:00:32.428+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:32.428+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:32.429+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: waiting: Set(ResultStage 45)
[2025-12-09T20:00:32.429+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:32.429+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting ResultStage 45 (MapPartitionsRDD[102] at map at HoodieJavaRDD.java:125), which has no missing parents
[2025-12-09T20:00:32.442+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 459.4 KiB, free 432.4 MiB)
[2025-12-09T20:00:32.446+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 162.5 KiB, free 432.2 MiB)
[2025-12-09T20:00:32.447+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on ead6510418b0:35171 (size: 162.5 KiB, free: 433.8 MiB)
[2025-12-09T20:00:32.448+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:32.448+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 45 (MapPartitionsRDD[102] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:32.449+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSchedulerImpl: Adding task set 45.0 with 1 tasks resource profile 0
[2025-12-09T20:00:32.450+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO TaskSetManager: Starting task 0.0 in stage 45.0 (TID 160) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:32.464+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on spark-worker:33701 (size: 162.5 KiB, free: 433.8 MiB)
[2025-12-09T20:00:32.483+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:32 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 172.19.0.9:46230
[2025-12-09T20:00:33.552+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Added rdd_101_0 in memory on spark-worker:33701 (size: 367.0 B, free: 433.8 MiB)
[2025-12-09T20:00:33.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 0.0 in stage 45.0 (TID 160) in 1108 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:33.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Removed TaskSet 45.0, whose tasks have all completed, from pool
[2025-12-09T20:00:33.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: ResultStage 45 (collect at HoodieJavaRDD.java:177) finished in 1.130 s
[2025-12-09T20:00:33.560+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:33.560+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 45: Stage finished
[2025-12-09T20:00:33.560+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Job 21 finished: collect at HoodieJavaRDD.java:177, took 1.187267 s
[2025-12-09T20:00:33.561+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
[2025-12-09T20:00:33.562+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BaseSparkCommitActionExecutor: Committing 20251209200020437, action Type deltacommit, operation Type UPSERT_PREPPED
[2025-12-09T20:00:33.586+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-12-09T20:00:33.587+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Got job 22 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-12-09T20:00:33.587+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Final stage: ResultStage 46 (collect at HoodieSparkEngineContext.java:150)
[2025-12-09T20:00:33.588+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:33.588+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:33.588+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Submitting ResultStage 46 (MapPartitionsRDD[104] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-12-09T20:00:33.592+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 100.7 KiB, free 432.1 MiB)
[2025-12-09T20:00:33.595+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 432.1 MiB)
[2025-12-09T20:00:33.595+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on ead6510418b0:35171 (size: 35.9 KiB, free: 433.8 MiB)
[2025-12-09T20:00:33.596+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:33.597+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (MapPartitionsRDD[104] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:33.597+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Adding task set 46.0 with 1 tasks resource profile 0
[2025-12-09T20:00:33.598+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 0.0 in stage 46.0 (TID 161) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2025-12-09T20:00:33.610+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on spark-worker:33701 (size: 35.9 KiB, free: 433.8 MiB)
[2025-12-09T20:00:33.621+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 0.0 in stage 46.0 (TID 161) in 23 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:33.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Removed TaskSet 46.0, whose tasks have all completed, from pool
[2025-12-09T20:00:33.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: ResultStage 46 (collect at HoodieSparkEngineContext.java:150) finished in 0.034 s
[2025-12-09T20:00:33.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:33.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 46: Stage finished
[2025-12-09T20:00:33.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Job 22 finished: collect at HoodieSparkEngineContext.java:150, took 0.035978 s
[2025-12-09T20:00:33.630+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Marking instant complete [==>20251209200020437__deltacommit__INFLIGHT]
[2025-12-09T20:00:33.631+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/20251209200020437.deltacommit.inflight
[2025-12-09T20:00:33.653+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_25_piece0 on ead6510418b0:35171 in memory (size: 171.6 KiB, free: 434.0 MiB)
[2025-12-09T20:00:33.655+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_25_piece0 on spark-worker:33701 in memory (size: 171.6 KiB, free: 433.9 MiB)
[2025-12-09T20:00:33.661+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_29_piece0 on ead6510418b0:35171 in memory (size: 123.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:33.663+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_29_piece0 on spark-worker:33701 in memory (size: 123.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:33.665+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/20251209200020437.deltacommit
[2025-12-09T20:00:33.666+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Completed [==>20251209200020437__deltacommit__INFLIGHT]
[2025-12-09T20:00:33.666+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BaseSparkCommitActionExecutor: Committed 20251209200020437
[2025-12-09T20:00:33.666+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_31_piece0 on ead6510418b0:35171 in memory (size: 35.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:33.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_31_piece0 on spark-worker:33701 in memory (size: 35.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:33.672+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_30_piece0 on ead6510418b0:35171 in memory (size: 162.5 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.675+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_30_piece0 on spark-worker:33701 in memory (size: 162.5 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.680+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_26_piece0 on ead6510418b0:35171 in memory (size: 5.5 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.682+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_26_piece0 on spark-worker:33701 in memory (size: 5.5 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.686+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_27_piece0 on ead6510418b0:35171 in memory (size: 3.1 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.688+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_27_piece0 on spark-worker:33701 in memory (size: 3.1 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.692+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_28_piece0 on ead6510418b0:35171 in memory (size: 123.6 KiB, free: 434.4 MiB)
[2025-12-09T20:00:33.693+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-12-09T20:00:33.694+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Got job 23 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
[2025-12-09T20:00:33.694+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Final stage: ResultStage 47 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-12-09T20:00:33.694+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:33.694+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:33.694+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Removed broadcast_28_piece0 on spark-worker:33701 in memory (size: 123.6 KiB, free: 434.4 MiB)
[2025-12-09T20:00:33.694+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Submitting ResultStage 47 (MapPartitionsRDD[106] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-12-09T20:00:33.698+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 100.9 KiB, free 434.3 MiB)
[2025-12-09T20:00:33.700+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.3 MiB)
[2025-12-09T20:00:33.700+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on ead6510418b0:35171 (size: 36.0 KiB, free: 434.4 MiB)
[2025-12-09T20:00:33.701+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:33.701+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 47 (MapPartitionsRDD[106] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:33.701+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Adding task set 47.0 with 1 tasks resource profile 0
[2025-12-09T20:00:33.703+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 0.0 in stage 47.0 (TID 162) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7347 bytes)
[2025-12-09T20:00:33.713+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on spark-worker:33701 (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.748+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 0.0 in stage 47.0 (TID 162) in 46 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:33.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Removed TaskSet 47.0, whose tasks have all completed, from pool
[2025-12-09T20:00:33.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: ResultStage 47 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.054 s
[2025-12-09T20:00:33.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Job 23 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:33.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Killing all running tasks in stage 47: Stage finished
[2025-12-09T20:00:33.750+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Job 23 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.056481 s
[2025-12-09T20:00:33.775+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/.temp/20251209200020437
[2025-12-09T20:00:33.776+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:33.783+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:33.792+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:33.792+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers//.hoodie/metadata
[2025-12-09T20:00:33.799+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__deltacommit__COMPLETED__20251209200033653]}
[2025-12-09T20:00:33.809+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:33.817+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:33.818+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:33.824+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__deltacommit__COMPLETED__20251209200033653]}
[2025-12-09T20:00:33.826+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Marking instant complete [==>20251209200020437__commit__INFLIGHT]
[2025-12-09T20:00:33.826+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=customers/.hoodie/20251209200020437.inflight
[2025-12-09T20:00:33.853+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=customers/.hoodie/20251209200020437.commit
[2025-12-09T20:00:33.854+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO HoodieActiveTimeline: Completed [==>20251209200020437__commit__INFLIGHT]
[2025-12-09T20:00:33.885+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-12-09T20:00:33.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Got job 24 (collectAsMap at HoodieSparkEngineContext.java:164) with 13 output partitions
[2025-12-09T20:00:33.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Final stage: ResultStage 48 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-12-09T20:00:33.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:33.888+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:33.888+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Submitting ResultStage 48 (MapPartitionsRDD[108] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-12-09T20:00:33.893+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 100.9 KiB, free 434.2 MiB)
[2025-12-09T20:00:33.895+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.1 MiB)
[2025-12-09T20:00:33.896+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on ead6510418b0:35171 (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.896+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:33.896+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO DAGScheduler: Submitting 13 missing tasks from ResultStage 48 (MapPartitionsRDD[108] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12))
[2025-12-09T20:00:33.897+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSchedulerImpl: Adding task set 48.0 with 13 tasks resource profile 0
[2025-12-09T20:00:33.898+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 0.0 in stage 48.0 (TID 163) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7337 bytes)
[2025-12-09T20:00:33.898+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 1.0 in stage 48.0 (TID 164) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7333 bytes)
[2025-12-09T20:00:33.898+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 2.0 in stage 48.0 (TID 165) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7333 bytes)
[2025-12-09T20:00:33.899+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 3.0 in stage 48.0 (TID 166) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7334 bytes)
[2025-12-09T20:00:33.910+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on spark-worker:33701 (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:33.927+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 4.0 in stage 48.0 (TID 167) (spark-worker, executor 0, partition 4, PROCESS_LOCAL, 7334 bytes)
[2025-12-09T20:00:33.928+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 1.0 in stage 48.0 (TID 164) in 29 ms on spark-worker (executor 0) (1/13)
[2025-12-09T20:00:33.930+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 5.0 in stage 48.0 (TID 168) (spark-worker, executor 0, partition 5, PROCESS_LOCAL, 7334 bytes)
[2025-12-09T20:00:33.931+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 3.0 in stage 48.0 (TID 166) in 33 ms on spark-worker (executor 0) (2/13)
[2025-12-09T20:00:33.934+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 6.0 in stage 48.0 (TID 169) (spark-worker, executor 0, partition 6, PROCESS_LOCAL, 7334 bytes)
[2025-12-09T20:00:33.938+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 7.0 in stage 48.0 (TID 170) (spark-worker, executor 0, partition 7, PROCESS_LOCAL, 7334 bytes)
[2025-12-09T20:00:33.938+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 2.0 in stage 48.0 (TID 165) in 40 ms on spark-worker (executor 0) (3/13)
[2025-12-09T20:00:33.939+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 0.0 in stage 48.0 (TID 163) in 41 ms on spark-worker (executor 0) (4/13)
[2025-12-09T20:00:33.952+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 8.0 in stage 48.0 (TID 171) (spark-worker, executor 0, partition 8, PROCESS_LOCAL, 7333 bytes)
[2025-12-09T20:00:33.953+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 4.0 in stage 48.0 (TID 167) in 26 ms on spark-worker (executor 0) (5/13)
[2025-12-09T20:00:33.956+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 9.0 in stage 48.0 (TID 172) (spark-worker, executor 0, partition 9, PROCESS_LOCAL, 7333 bytes)
[2025-12-09T20:00:33.957+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 5.0 in stage 48.0 (TID 168) in 28 ms on spark-worker (executor 0) (6/13)
[2025-12-09T20:00:33.961+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 10.0 in stage 48.0 (TID 173) (spark-worker, executor 0, partition 10, PROCESS_LOCAL, 7333 bytes)
[2025-12-09T20:00:33.961+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 6.0 in stage 48.0 (TID 169) in 30 ms on spark-worker (executor 0) (7/13)
[2025-12-09T20:00:33.965+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 11.0 in stage 48.0 (TID 174) (spark-worker, executor 0, partition 11, PROCESS_LOCAL, 7333 bytes)
[2025-12-09T20:00:33.966+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 7.0 in stage 48.0 (TID 170) in 31 ms on spark-worker (executor 0) (8/13)
[2025-12-09T20:00:33.977+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Starting task 12.0 in stage 48.0 (TID 175) (spark-worker, executor 0, partition 12, PROCESS_LOCAL, 7333 bytes)
[2025-12-09T20:00:33.977+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 8.0 in stage 48.0 (TID 171) in 29 ms on spark-worker (executor 0) (9/13)
[2025-12-09T20:00:33.981+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 9.0 in stage 48.0 (TID 172) in 28 ms on spark-worker (executor 0) (10/13)
[2025-12-09T20:00:33.986+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 10.0 in stage 48.0 (TID 173) in 29 ms on spark-worker (executor 0) (11/13)
[2025-12-09T20:00:33.986+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:33 INFO TaskSetManager: Finished task 11.0 in stage 48.0 (TID 174) in 24 ms on spark-worker (executor 0) (12/13)
[2025-12-09T20:00:34.014+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Finished task 12.0 in stage 48.0 (TID 175) in 42 ms on spark-worker (executor 0) (13/13)
[2025-12-09T20:00:34.014+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSchedulerImpl: Removed TaskSet 48.0, whose tasks have all completed, from pool
[2025-12-09T20:00:34.015+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: ResultStage 48 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.125 s
[2025-12-09T20:00:34.016+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:34.016+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 48: Stage finished
[2025-12-09T20:00:34.016+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Job 24 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.130884 s
[2025-12-09T20:00:34.042+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=customers/.hoodie/.temp/20251209200020437
[2025-12-09T20:00:34.044+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BaseHoodieWriteClient: Committed 20251209200020437
[2025-12-09T20:00:34.048+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MapPartitionsRDD: Removing RDD 70 from persistence list
[2025-12-09T20:00:34.050+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManager: Removing RDD 70
[2025-12-09T20:00:34.050+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MapPartitionsRDD: Removing RDD 80 from persistence list
[2025-12-09T20:00:34.052+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManager: Removing RDD 80
[2025-12-09T20:00:34.052+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO UnionRDD: Removing RDD 91 from persistence list
[2025-12-09T20:00:34.053+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManager: Removing RDD 91
[2025-12-09T20:00:34.054+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MapPartitionsRDD: Removing RDD 101 from persistence list
[2025-12-09T20:00:34.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BaseHoodieWriteClient: Async cleaner has been spawned. Waiting for it to finish
[2025-12-09T20:00:34.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO AsyncCleanerService: Waiting for async clean service to finish
[2025-12-09T20:00:34.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BaseHoodieWriteClient: Async cleaner has finished
[2025-12-09T20:00:34.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManager: Removing RDD 101
[2025-12-09T20:00:34.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:34.062+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:34.068+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:34.068+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:34.072+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__commit__COMPLETED__20251209200033842]}
[2025-12-09T20:00:34.072+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:34.077+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:34.081+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:34.081+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:34.084+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:34.088+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:34.091+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__deltacommit__COMPLETED__20251209200033653]}
[2025-12-09T20:00:34.091+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:34.091+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:34.092+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:34.092+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:34.092+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BaseHoodieWriteClient: Start to archive synchronously.
[2025-12-09T20:00:34.096+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__commit__COMPLETED__20251209200033842]}
[2025-12-09T20:00:34.096+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:34.099+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/hoodie.properties
[2025-12-09T20:00:34.103+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=customers/
[2025-12-09T20:00:34.103+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:34.108+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=customers/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:34.111+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=customers/.hoodie/metadata
[2025-12-09T20:00:34.116+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__deltacommit__COMPLETED__20251209200033653]}
[2025-12-09T20:00:34.116+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:34.116+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:34.117+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
[2025-12-09T20:00:34.117+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTimelineArchiver: No Instants to archive
[2025-12-09T20:00:34.117+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FileSystemViewManager: Creating remote view for basePath s3a://huditest/silver/table_name=customers. Server=ead6510418b0:36343, Timeout=300
[2025-12-09T20:00:34.117+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://huditest/silver/table_name=customers
[2025-12-09T20:00:34.117+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:34.121+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:34.127+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__commit__COMPLETED__20251209200033842]}
[2025-12-09T20:00:34.128+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO RemoteHoodieTableFileSystemView: Sending request : (http://ead6510418b0:36343/v1/hoodie/view/refresh/?basepath=s3a%3A%2F%2Fhuditest%2Fsilver%2Ftable_name%3Dcustomers&lastinstantts=20251209200020437&timelinehash=8d7437b99368bcbeb625175cd7cc0c376852d97d4313c4a8e0a8012765f26565)
[2025-12-09T20:00:34.135+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__commit__COMPLETED__20251209200033842]}
[2025-12-09T20:00:34.135+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:34.140+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:34.147+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__commit__COMPLETED__20251209200033842]}
[2025-12-09T20:00:34.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200020437__deltacommit__COMPLETED__20251209200033653]}
[2025-12-09T20:00:34.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:34.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:34.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Commit 20251209200020437 successful!
[2025-12-09T20:00:34.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-12-09T20:00:34.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Compaction Scheduled is Optional.empty
[2025-12-09T20:00:34.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-12-09T20:00:34.151+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Clustering Scheduled is Optional.empty
[2025-12-09T20:00:34.152+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Is Async Compaction Enabled ? false
[2025-12-09T20:00:34.152+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-12-09T20:00:34.152+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-12-09T20:00:34.152+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Closing write client
[2025-12-09T20:00:34.152+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BaseHoodieClient: Stopping Timeline service !!
[2025-12-09T20:00:34.152+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO EmbeddedTimelineService: Closing Timeline server
[2025-12-09T20:00:34.152+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TimelineService: Closing Timeline Service
[2025-12-09T20:00:34.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Javalin: Stopping Javalin ...
[2025-12-09T20:00:34.165+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Javalin: Javalin has stopped
[2025-12-09T20:00:34.166+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TimelineService: Closed Timeline Service
[2025-12-09T20:00:34.166+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO EmbeddedTimelineService: Closed Timeline server
[2025-12-09T20:00:34.166+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO AsyncCleanerService: Shutting down async clean service...
[2025-12-09T20:00:34.198+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO CodeGenerator: Code generated in 7.378697 ms
[2025-12-09T20:00:34.202+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Registering RDD 110 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 12
[2025-12-09T20:00:34.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Got map stage job 25 (count at NativeMethodAccessorImpl.java:0) with 4 output partitions
[2025-12-09T20:00:34.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Final stage: ShuffleMapStage 49 (count at NativeMethodAccessorImpl.java:0)
[2025-12-09T20:00:34.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:34.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:34.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Submitting ShuffleMapStage 49 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-12-09T20:00:34.204+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 15.9 KiB, free 434.1 MiB)
[2025-12-09T20:00:34.206+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 8.4 KiB, free 434.1 MiB)
[2025-12-09T20:00:34.207+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on ead6510418b0:35171 (size: 8.4 KiB, free: 434.3 MiB)
[2025-12-09T20:00:34.208+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:34.208+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 49 (MapPartitionsRDD[110] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:34.208+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSchedulerImpl: Adding task set 49.0 with 4 tasks resource profile 0
[2025-12-09T20:00:34.209+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Starting task 0.0 in stage 49.0 (TID 176) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 10183 bytes)
[2025-12-09T20:00:34.209+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Starting task 1.0 in stage 49.0 (TID 177) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 10139 bytes)
[2025-12-09T20:00:34.210+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Starting task 2.0 in stage 49.0 (TID 178) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 9989 bytes)
[2025-12-09T20:00:34.210+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Starting task 3.0 in stage 49.0 (TID 179) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 10165 bytes)
[2025-12-09T20:00:34.218+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on spark-worker:33701 (size: 8.4 KiB, free: 434.3 MiB)
[2025-12-09T20:00:34.271+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Finished task 0.0 in stage 49.0 (TID 176) in 62 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:34.278+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Finished task 3.0 in stage 49.0 (TID 179) in 67 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:34.286+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Finished task 2.0 in stage 49.0 (TID 178) in 77 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:34.287+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Finished task 1.0 in stage 49.0 (TID 177) in 77 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:34.287+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSchedulerImpl: Removed TaskSet 49.0, whose tasks have all completed, from pool
[2025-12-09T20:00:34.288+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: ShuffleMapStage 49 (count at NativeMethodAccessorImpl.java:0) finished in 0.085 s
[2025-12-09T20:00:34.288+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:34.288+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:34.288+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: waiting: Set()
[2025-12-09T20:00:34.289+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:34.304+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO SparkContext: Starting job: count at NativeMethodAccessorImpl.java:0
[2025-12-09T20:00:34.305+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Got job 26 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions
[2025-12-09T20:00:34.306+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Final stage: ResultStage 51 (count at NativeMethodAccessorImpl.java:0)
[2025-12-09T20:00:34.306+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 50)
[2025-12-09T20:00:34.306+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:34.306+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Submitting ResultStage 51 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0), which has no missing parents
[2025-12-09T20:00:34.310+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 12.1 KiB, free 434.1 MiB)
[2025-12-09T20:00:34.314+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 5.8 KiB, free 434.1 MiB)
[2025-12-09T20:00:34.315+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on ead6510418b0:35171 (size: 5.8 KiB, free: 434.3 MiB)
[2025-12-09T20:00:34.316+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:34.316+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 51 (MapPartitionsRDD[113] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:34.316+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSchedulerImpl: Adding task set 51.0 with 1 tasks resource profile 0
[2025-12-09T20:00:34.318+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Starting task 0.0 in stage 51.0 (TID 180) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7367 bytes)
[2025-12-09T20:00:34.333+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on spark-worker:33701 (size: 5.8 KiB, free: 434.3 MiB)
[2025-12-09T20:00:34.339+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 172.19.0.9:46230
[2025-12-09T20:00:34.349+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Finished task 0.0 in stage 51.0 (TID 180) in 31 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:34.350+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSchedulerImpl: Removed TaskSet 51.0, whose tasks have all completed, from pool
[2025-12-09T20:00:34.350+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: ResultStage 51 (count at NativeMethodAccessorImpl.java:0) finished in 0.043 s
[2025-12-09T20:00:34.351+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Job 26 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:34.351+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSchedulerImpl: Killing all running tasks in stage 51: Stage finished
[2025-12-09T20:00:34.351+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Job 26 finished: count at NativeMethodAccessorImpl.java:0, took 0.046399 s
[2025-12-09T20:00:34.397+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Initializing s3a://huditest/silver/table_name=orders/ as hoodie table s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.589+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.595+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:34.599+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.600+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished initializing Table of type COPY_ON_WRITE from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.603+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:34.609+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO EmbeddedTimelineService: Starting Timeline service !!
[2025-12-09T20:00:34.610+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO EmbeddedTimelineService: Overriding hostIp to (ead6510418b0) found in spark-conf. It was null
[2025-12-09T20:00:34.610+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:34.610+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:34.612+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Javalin:
[2025-12-09T20:00:34.612+0000] {spark_submit.py:491} INFO - __                      __ _            __ __
[2025-12-09T20:00:34.612+0000] {spark_submit.py:491} INFO - / /____ _ _   __ ____ _ / /(_)____      / // /
[2025-12-09T20:00:34.612+0000] {spark_submit.py:491} INFO - __  / // __ `/| | / // __ `// // // __ \    / // /_
[2025-12-09T20:00:34.613+0000] {spark_submit.py:491} INFO - / /_/ // /_/ / | |/ // /_/ // // // / / /   /__  __/
[2025-12-09T20:00:34.613+0000] {spark_submit.py:491} INFO - \____/ \__,_/  |___/ \__,_//_//_//_/ /_/      /_/
[2025-12-09T20:00:34.613+0000] {spark_submit.py:491} INFO - 
[2025-12-09T20:00:34.613+0000] {spark_submit.py:491} INFO - https://javalin.io/documentation
[2025-12-09T20:00:34.613+0000] {spark_submit.py:491} INFO - 
[2025-12-09T20:00:34.613+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Javalin: Starting Javalin ...
[2025-12-09T20:00:34.613+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Javalin: You are running Javalin 4.6.7 (released October 24, 2022. Your Javalin version is 1142 days old. Consider checking for a newer version.).
[2025-12-09T20:00:34.613+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Server: jetty-9.4.48.v20220622; built: 2022-06-21T20:42:25.880Z; git: 6b67c5719d1f4371b33655ff2d047d24e171e49a; jvm 17.0.17+10-Debian-1deb11u1
[2025-12-09T20:00:34.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Server: Started @35212ms
[2025-12-09T20:00:34.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Javalin: Listening on http://localhost:35359/
[2025-12-09T20:00:34.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO Javalin: Javalin started in 10ms \o/
[2025-12-09T20:00:34.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TimelineService: Starting Timeline server on port :35359
[2025-12-09T20:00:34.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO EmbeddedTimelineService: Started embedded timeline server at ead6510418b0:35359
[2025-12-09T20:00:34.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BaseHoodieClient: Timeline Server already running. Not restarting the service
[2025-12-09T20:00:34.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-12-09T20:00:34.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-12-09T20:00:34.635+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.641+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:34.646+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.646+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:34.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO CleanerUtils: Cleaned failed attempts if any
[2025-12-09T20:00:34.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.653+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:34.656+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.657+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.659+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:34.660+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.663+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:34.667+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.670+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:34.674+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:34.674+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:34.675+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BaseHoodieWriteClient: Generate a new instant time: 20251209200034599 action: commit
[2025-12-09T20:00:34.675+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Creating a new instant [==>20251209200034599__commit__REQUESTED]
[2025-12-09T20:00:34.696+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.702+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:34.709+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.710+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.714+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__REQUESTED__20251209200034685]}
[2025-12-09T20:00:34.714+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.719+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:34.724+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:34.726+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: []
[2025-12-09T20:00:34.731+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__REQUESTED__20251209200034685]}
[2025-12-09T20:00:34.731+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Initializing s3a://huditest/silver/table_name=orders//.hoodie/metadata as hoodie table s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:34.937+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:34.944+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:34.949+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:34.950+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO HoodieTableMetaClient: Finished initializing Table of type MERGE_ON_READ from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:34.962+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
[2025-12-09T20:00:34.963+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Got job 27 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
[2025-12-09T20:00:34.964+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Final stage: ResultStage 52 (collect at HoodieSparkEngineContext.java:116)
[2025-12-09T20:00:34.964+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:34.964+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:34.964+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Submitting ResultStage 52 (MapPartitionsRDD[120] at map at HoodieSparkEngineContext.java:116), which has no missing parents
[2025-12-09T20:00:34.968+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MemoryStore: Block broadcast_36 stored as values in memory (estimated size 101.1 KiB, free 434.0 MiB)
[2025-12-09T20:00:34.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 36.1 KiB, free 434.0 MiB)
[2025-12-09T20:00:34.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on ead6510418b0:35171 (size: 36.1 KiB, free: 434.3 MiB)
[2025-12-09T20:00:34.972+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:34.972+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 52 (MapPartitionsRDD[120] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:34.972+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSchedulerImpl: Adding task set 52.0 with 1 tasks resource profile 0
[2025-12-09T20:00:34.976+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO TaskSetManager: Starting task 0.0 in stage 52.0 (TID 181) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7368 bytes)
[2025-12-09T20:00:34.989+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:34 INFO BlockManagerInfo: Added broadcast_36_piece0 in memory on spark-worker:33701 (size: 36.1 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.006+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Finished task 0.0 in stage 52.0 (TID 181) in 32 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:35.006+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Removed TaskSet 52.0, whose tasks have all completed, from pool
[2025-12-09T20:00:35.007+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: ResultStage 52 (collect at HoodieSparkEngineContext.java:116) finished in 0.042 s
[2025-12-09T20:00:35.008+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:35.009+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 52: Stage finished
[2025-12-09T20:00:35.009+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 27 finished: collect at HoodieSparkEngineContext.java:116, took 0.046525 s
[2025-12-09T20:00:35.018+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:35.019+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieBackedTableMetadataWriter: Initializing MDT partition FILES at instant 00000000000000010
[2025-12-09T20:00:35.019+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieBackedTableMetadataWriter: Committing total 0 partitions and 0 files to metadata
[2025-12-09T20:00:35.026+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Starting job: count at HoodieJavaRDD.java:115
[2025-12-09T20:00:35.027+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Got job 28 (count at HoodieJavaRDD.java:115) with 1 output partitions
[2025-12-09T20:00:35.028+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Final stage: ResultStage 53 (count at HoodieJavaRDD.java:115)
[2025-12-09T20:00:35.028+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:35.028+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:35.029+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting ResultStage 53 (ParallelCollectionRDD[121] at parallelize at HoodieSparkEngineContext.java:111), which has no missing parents
[2025-12-09T20:00:35.030+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_37 stored as values in memory (estimated size 3.0 KiB, free 434.0 MiB)
[2025-12-09T20:00:35.033+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 1866.0 B, free 434.0 MiB)
[2025-12-09T20:00:35.034+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on ead6510418b0:35171 (size: 1866.0 B, free: 434.3 MiB)
[2025-12-09T20:00:35.035+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:35.035+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 53 (ParallelCollectionRDD[121] at parallelize at HoodieSparkEngineContext.java:111) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:35.035+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Adding task set 53.0 with 1 tasks resource profile 0
[2025-12-09T20:00:35.037+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Starting task 0.0 in stage 53.0 (TID 182) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7405 bytes)
[2025-12-09T20:00:35.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_37_piece0 in memory on spark-worker:33701 (size: 1866.0 B, free: 434.3 MiB)
[2025-12-09T20:00:35.054+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Finished task 0.0 in stage 53.0 (TID 182) in 17 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:35.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Removed TaskSet 53.0, whose tasks have all completed, from pool
[2025-12-09T20:00:35.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: ResultStage 53 (count at HoodieJavaRDD.java:115) finished in 0.026 s
[2025-12-09T20:00:35.055+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:35.056+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 53: Stage finished
[2025-12-09T20:00:35.056+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 28 finished: count at HoodieJavaRDD.java:115, took 0.029713 s
[2025-12-09T20:00:35.056+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieBackedTableMetadataWriter: Initializing FILES index with 1 mappings and 1 file groups.
[2025-12-09T20:00:35.063+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieBackedTableMetadataWriter: Creating 1 file groups for partition files with base fileId files- at instant time 00000000000000010
[2025-12-09T20:00:35.093+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Starting job: foreach at HoodieSparkEngineContext.java:155
[2025-12-09T20:00:35.094+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Got job 29 (foreach at HoodieSparkEngineContext.java:155) with 1 output partitions
[2025-12-09T20:00:35.095+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Final stage: ResultStage 54 (foreach at HoodieSparkEngineContext.java:155)
[2025-12-09T20:00:35.095+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:35.095+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:35.095+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting ResultStage 54 (ParallelCollectionRDD[122] at parallelize at HoodieSparkEngineContext.java:155), which has no missing parents
[2025-12-09T20:00:35.104+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_38 stored as values in memory (estimated size 383.5 KiB, free 433.6 MiB)
[2025-12-09T20:00:35.116+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 126.9 KiB, free 433.5 MiB)
[2025-12-09T20:00:35.118+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on ead6510418b0:35171 (size: 126.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:35.119+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:35.121+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_32_piece0 on ead6510418b0:35171 in memory (size: 36.0 KiB, free: 434.2 MiB)
[2025-12-09T20:00:35.122+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 54 (ParallelCollectionRDD[122] at parallelize at HoodieSparkEngineContext.java:155) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:35.124+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Adding task set 54.0 with 1 tasks resource profile 0
[2025-12-09T20:00:35.126+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_32_piece0 on spark-worker:33701 in memory (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.126+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Starting task 0.0 in stage 54.0 (TID 183) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7260 bytes)
[2025-12-09T20:00:35.128+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManager: Removing RDD 91
[2025-12-09T20:00:35.132+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_37_piece0 on ead6510418b0:35171 in memory (size: 1866.0 B, free: 434.2 MiB)
[2025-12-09T20:00:35.134+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_37_piece0 on spark-worker:33701 in memory (size: 1866.0 B, free: 434.3 MiB)
[2025-12-09T20:00:35.138+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_36_piece0 on ead6510418b0:35171 in memory (size: 36.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:35.139+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_38_piece0 in memory on spark-worker:33701 (size: 126.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:35.140+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_36_piece0 on spark-worker:33701 in memory (size: 36.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:35.144+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManager: Removing RDD 101
[2025-12-09T20:00:35.148+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_33_piece0 on ead6510418b0:35171 in memory (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.150+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_33_piece0 on spark-worker:33701 in memory (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_35_piece0 on ead6510418b0:35171 in memory (size: 5.8 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.156+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_35_piece0 on spark-worker:33701 in memory (size: 5.8 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.162+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_34_piece0 on ead6510418b0:35171 in memory (size: 8.4 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.165+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Removed broadcast_34_piece0 on spark-worker:33701 in memory (size: 8.4 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.188+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Finished task 0.0 in stage 54.0 (TID 183) in 67 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:35.188+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Removed TaskSet 54.0, whose tasks have all completed, from pool
[2025-12-09T20:00:35.189+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: ResultStage 54 (foreach at HoodieSparkEngineContext.java:155) finished in 0.094 s
[2025-12-09T20:00:35.189+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 29 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:35.190+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 54: Stage finished
[2025-12-09T20:00:35.190+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 29 finished: foreach at HoodieSparkEngineContext.java:155, took 0.096376 s
[2025-12-09T20:00:35.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:35.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:35.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
[2025-12-09T20:00:35.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:35.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:35.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO AbstractTableFileSystemView: Building file system view for partition (files)
[2025-12-09T20:00:35.199+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-12-09T20:00:35.199+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-12-09T20:00:35.200+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.204+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.208+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.209+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.211+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:35.216+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.220+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:35.220+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:35.221+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieBackedTableMetadataWriter: New commit at 00000000000000010 being applied to MDT.
[2025-12-09T20:00:35.221+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.225+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.229+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.233+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:35.234+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO CleanerUtils: Cleaned failed attempts if any
[2025-12-09T20:00:35.234+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.238+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.243+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.243+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.245+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Optional.empty
[2025-12-09T20:00:35.250+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.254+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:35.254+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:35.255+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BaseHoodieWriteClient: Generate a new instant time: 00000000000000010 action: deltacommit
[2025-12-09T20:00:35.255+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Creating a new instant [==>00000000000000010__deltacommit__REQUESTED]
[2025-12-09T20:00:35.272+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.278+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.283+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.283+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.287+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>00000000000000010__deltacommit__REQUESTED__20251209200035263]}
[2025-12-09T20:00:35.294+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.298+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:35.299+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:35.299+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
[2025-12-09T20:00:35.299+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-12-09T20:00:35.299+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.requested
[2025-12-09T20:00:35.322+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
[2025-12-09T20:00:35.332+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Starting job: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95
[2025-12-09T20:00:35.333+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Registering RDD 126 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) as input to shuffle 13
[2025-12-09T20:00:35.334+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Got job 30 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) with 1 output partitions
[2025-12-09T20:00:35.334+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Final stage: ResultStage 56 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95)
[2025-12-09T20:00:35.334+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 55)
[2025-12-09T20:00:35.334+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 55)
[2025-12-09T20:00:35.335+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting ShuffleMapStage 55 (MapPartitionsRDD[126] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74), which has no missing parents
[2025-12-09T20:00:35.337+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_39 stored as values in memory (estimated size 8.5 KiB, free 433.9 MiB)
[2025-12-09T20:00:35.341+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 4.7 KiB, free 433.9 MiB)
[2025-12-09T20:00:35.342+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on ead6510418b0:35171 (size: 4.7 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.343+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:35.344+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 55 (MapPartitionsRDD[126] at keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:35.344+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Adding task set 55.0 with 1 tasks resource profile 0
[2025-12-09T20:00:35.346+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Starting task 0.0 in stage 55.0 (TID 184) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7503 bytes)
[2025-12-09T20:00:35.357+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_39_piece0 in memory on spark-worker:33701 (size: 4.7 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Finished task 0.0 in stage 55.0 (TID 184) in 22 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:35.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Removed TaskSet 55.0, whose tasks have all completed, from pool
[2025-12-09T20:00:35.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: ShuffleMapStage 55 (keyBy at SparkHoodieMetadataBulkInsertPartitioner.java:74) finished in 0.033 s
[2025-12-09T20:00:35.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:35.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:35.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: waiting: Set(ResultStage 56)
[2025-12-09T20:00:35.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:35.369+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting ResultStage 56 (MapPartitionsRDD[129] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81), which has no missing parents
[2025-12-09T20:00:35.371+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_40 stored as values in memory (estimated size 7.3 KiB, free 433.9 MiB)
[2025-12-09T20:00:35.373+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 3.9 KiB, free 433.9 MiB)
[2025-12-09T20:00:35.373+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on ead6510418b0:35171 (size: 3.9 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.374+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:35.374+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 56 (MapPartitionsRDD[129] at mapPartitions at SparkHoodieMetadataBulkInsertPartitioner.java:81) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:35.374+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Adding task set 56.0 with 1 tasks resource profile 0
[2025-12-09T20:00:35.377+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Starting task 0.0 in stage 56.0 (TID 185) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:35.390+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_40_piece0 in memory on spark-worker:33701 (size: 3.9 KiB, free: 434.3 MiB)
[2025-12-09T20:00:35.395+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 172.19.0.9:46230
[2025-12-09T20:00:35.404+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Finished task 0.0 in stage 56.0 (TID 185) in 28 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:35.405+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Removed TaskSet 56.0, whose tasks have all completed, from pool
[2025-12-09T20:00:35.405+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: ResultStage 56 (collect at SparkHoodieMetadataBulkInsertPartitioner.java:95) finished in 0.035 s
[2025-12-09T20:00:35.405+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:35.406+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 56: Stage finished
[2025-12-09T20:00:35.406+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 30 finished: collect at SparkHoodieMetadataBulkInsertPartitioner.java:95, took 0.073420 s
[2025-12-09T20:00:35.417+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-12-09T20:00:35.418+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 00000000000000010
[2025-12-09T20:00:35.436+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
[2025-12-09T20:00:35.437+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Got job 31 (collect at HoodieJavaRDD.java:177) with 1 output partitions
[2025-12-09T20:00:35.438+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Final stage: ResultStage 58 (collect at HoodieJavaRDD.java:177)
[2025-12-09T20:00:35.438+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 57)
[2025-12-09T20:00:35.439+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:35.439+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting ResultStage 58 (MapPartitionsRDD[132] at map at HoodieJavaRDD.java:125), which has no missing parents
[2025-12-09T20:00:35.448+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_41 stored as values in memory (estimated size 357.4 KiB, free 433.5 MiB)
[2025-12-09T20:00:35.451+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_41_piece0 stored as bytes in memory (estimated size 123.6 KiB, free 433.4 MiB)
[2025-12-09T20:00:35.452+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on ead6510418b0:35171 (size: 123.6 KiB, free: 434.1 MiB)
[2025-12-09T20:00:35.452+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Created broadcast 41 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:35.453+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 58 (MapPartitionsRDD[132] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:35.453+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Adding task set 58.0 with 1 tasks resource profile 0
[2025-12-09T20:00:35.454+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Starting task 0.0 in stage 58.0 (TID 186) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:35.464+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_41_piece0 in memory on spark-worker:33701 (size: 123.6 KiB, free: 434.1 MiB)
[2025-12-09T20:00:35.588+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added rdd_131_0 in memory on spark-worker:33701 (size: 290.0 B, free: 434.1 MiB)
[2025-12-09T20:00:35.593+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Finished task 0.0 in stage 58.0 (TID 186) in 139 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:35.593+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Removed TaskSet 58.0, whose tasks have all completed, from pool
[2025-12-09T20:00:35.594+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: ResultStage 58 (collect at HoodieJavaRDD.java:177) finished in 0.154 s
[2025-12-09T20:00:35.594+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:35.594+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 58: Stage finished
[2025-12-09T20:00:35.594+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 31 finished: collect at HoodieJavaRDD.java:177, took 0.157615 s
[2025-12-09T20:00:35.594+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO CommitUtils: Creating  metadata for BULK_INSERT numWriteStats:1 numReplaceFileIds:0
[2025-12-09T20:00:35.594+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BaseSparkCommitActionExecutor: Committing 00000000000000010, action Type deltacommit, operation Type BULK_INSERT
[2025-12-09T20:00:35.612+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-12-09T20:00:35.613+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Got job 32 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-12-09T20:00:35.613+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Final stage: ResultStage 59 (collect at HoodieSparkEngineContext.java:150)
[2025-12-09T20:00:35.613+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:35.613+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:35.613+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting ResultStage 59 (MapPartitionsRDD[134] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-12-09T20:00:35.617+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_42 stored as values in memory (estimated size 100.7 KiB, free 433.3 MiB)
[2025-12-09T20:00:35.619+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_42_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 433.3 MiB)
[2025-12-09T20:00:35.619+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on ead6510418b0:35171 (size: 35.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:35.620+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Created broadcast 42 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:35.620+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 59 (MapPartitionsRDD[134] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:35.620+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Adding task set 59.0 with 1 tasks resource profile 0
[2025-12-09T20:00:35.621+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Starting task 0.0 in stage 59.0 (TID 187) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7344 bytes)
[2025-12-09T20:00:35.630+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_42_piece0 in memory on spark-worker:33701 (size: 35.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:35.642+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Finished task 0.0 in stage 59.0 (TID 187) in 21 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:35.642+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Removed TaskSet 59.0, whose tasks have all completed, from pool
[2025-12-09T20:00:35.642+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: ResultStage 59 (collect at HoodieSparkEngineContext.java:150) finished in 0.028 s
[2025-12-09T20:00:35.643+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 32 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:35.643+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 59: Stage finished
[2025-12-09T20:00:35.643+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 32 finished: collect at HoodieSparkEngineContext.java:150, took 0.031053 s
[2025-12-09T20:00:35.644+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Marking instant complete [==>00000000000000010__deltacommit__INFLIGHT]
[2025-12-09T20:00:35.645+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/00000000000000010.deltacommit.inflight
[2025-12-09T20:00:35.667+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/00000000000000010.deltacommit
[2025-12-09T20:00:35.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Completed [==>00000000000000010__deltacommit__INFLIGHT]
[2025-12-09T20:00:35.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BaseSparkCommitActionExecutor: Committed 00000000000000010
[2025-12-09T20:00:35.696+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-12-09T20:00:35.697+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Got job 33 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
[2025-12-09T20:00:35.697+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Final stage: ResultStage 60 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-12-09T20:00:35.698+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:35.698+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:35.698+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting ResultStage 60 (MapPartitionsRDD[136] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-12-09T20:00:35.703+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_43 stored as values in memory (estimated size 100.9 KiB, free 433.2 MiB)
[2025-12-09T20:00:35.707+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO MemoryStore: Block broadcast_43_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 433.1 MiB)
[2025-12-09T20:00:35.708+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on ead6510418b0:35171 (size: 36.0 KiB, free: 434.1 MiB)
[2025-12-09T20:00:35.709+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO SparkContext: Created broadcast 43 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:35.709+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 60 (MapPartitionsRDD[136] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:35.709+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Adding task set 60.0 with 1 tasks resource profile 0
[2025-12-09T20:00:35.711+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Starting task 0.0 in stage 60.0 (TID 188) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7344 bytes)
[2025-12-09T20:00:35.720+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO BlockManagerInfo: Added broadcast_43_piece0 in memory on spark-worker:33701 (size: 36.0 KiB, free: 434.1 MiB)
[2025-12-09T20:00:35.754+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSetManager: Finished task 0.0 in stage 60.0 (TID 188) in 44 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:35.755+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Removed TaskSet 60.0, whose tasks have all completed, from pool
[2025-12-09T20:00:35.755+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: ResultStage 60 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.056 s
[2025-12-09T20:00:35.755+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:35.756+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO TaskSchedulerImpl: Killing all running tasks in stage 60: Stage finished
[2025-12-09T20:00:35.756+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO DAGScheduler: Job 33 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.058989 s
[2025-12-09T20:00:35.780+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/.temp/00000000000000010
[2025-12-09T20:00:35.781+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.788+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.795+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.795+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.800+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:35.811+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.818+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:35.818+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:35.825+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:35.830+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:35.895+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: MDT s3a://huditest/silver/table_name=orders partition FILES has been enabled
[2025-12-09T20:00:35.895+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:35.899+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:35.903+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:35.904+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:35.909+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.915+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:35.919+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:35.920+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:35.920+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:35.920+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieBackedTableMetadataWriter: Initializing FILES index in metadata table took 863 in ms
[2025-12-09T20:00:35.920+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.926+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.933+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.938+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:35.945+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.951+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:35.952+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:35.952+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.958+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.965+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.966+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.970+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:35.977+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:35.983+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:35.983+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:35.988+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:35.989+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieBackedTableMetadataWriter: Latest deltacommit time found is 00000000000000010, running clean operations.
[2025-12-09T20:00:35.993+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:35.993+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:35.998+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:35 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.003+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.003+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.008+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.015+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.020+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:36.020+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:36.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BaseHoodieWriteClient: Cleaner started
[2025-12-09T20:00:36.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.027+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.032+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.032+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.036+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.044+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.048+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:36.048+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:36.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time :00000000000000010002
[2025-12-09T20:00:36.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:36.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:36.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:36.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO CleanPlanner: No earliest commit to retain. No need to scan partitions !!
[2025-12-09T20:00:36.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO CleanPlanActionExecutor: Nothing to clean here. It is already clean
[2025-12-09T20:00:36.053+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.054+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.061+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.065+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.066+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.069+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.075+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.083+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:36.084+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:36.088+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__REQUESTED__20251209200034685]}
[2025-12-09T20:00:36.089+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BaseHoodieWriteClient: Scheduling table service COMPACT
[2025-12-09T20:00:36.089+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.096+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.102+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.102+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.107+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.113+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.118+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:36.118+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:36.118+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BaseHoodieWriteClient: Scheduling compaction at instant time :00000000000000010001
[2025-12-09T20:00:36.118+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO ScheduleCompactionActionExecutor: Checking if compaction needs to be run on s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.118+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.123+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.129+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.129+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:36.133+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.138+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.144+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:36.144+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:36.149+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.149+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.154+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:36.159+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.162+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__REQUESTED__20251209200034685]}
[2025-12-09T20:00:36.162+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTimelineArchiver: No Instants to archive
[2025-12-09T20:00:36.162+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieBackedTableMetadataWriter: All the table services operations on MDT completed successfully
[2025-12-09T20:00:36.162+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.168+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:36.173+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.173+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:36.179+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.185+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:36.191+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:36.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:36.192+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:36.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:36.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO AsyncCleanerService: Starting async clean service with instant time 20251209200036192...
[2025-12-09T20:00:36.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-12-09T20:00:36.193+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.199+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:36.203+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-12-09T20:00:36.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Registering RDD 137 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 15
[2025-12-09T20:00:36.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Registering RDD 141 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 14
[2025-12-09T20:00:36.206+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Got job 34 (countByKey at HoodieJavaPairRDD.java:105) with 4 output partitions
[2025-12-09T20:00:36.206+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Final stage: ResultStage 63 (countByKey at HoodieJavaPairRDD.java:105)
[2025-12-09T20:00:36.206+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 62)
[2025-12-09T20:00:36.207+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 62)
[2025-12-09T20:00:36.207+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.207+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.207+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ShuffleMapStage 61 (MapPartitionsRDD[137] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-12-09T20:00:36.211+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_44 stored as values in memory (estimated size 76.1 KiB, free 433.1 MiB)
[2025-12-09T20:00:36.212+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__REQUESTED__20251209200034685]}
[2025-12-09T20:00:36.212+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.222+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_44_piece0 stored as bytes in memory (estimated size 27.9 KiB, free 433.0 MiB)
[2025-12-09T20:00:36.222+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on ead6510418b0:35171 (size: 27.9 KiB, free: 434.0 MiB)
[2025-12-09T20:00:36.223+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_43_piece0 on ead6510418b0:35171 in memory (size: 36.0 KiB, free: 434.1 MiB)
[2025-12-09T20:00:36.224+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 44 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.225+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 61 (MapPartitionsRDD[137] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:36.225+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 61.0 with 4 tasks resource profile 0
[2025-12-09T20:00:36.225+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_43_piece0 on spark-worker:33701 in memory (size: 36.0 KiB, free: 434.1 MiB)
[2025-12-09T20:00:36.227+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 61.0 (TID 189) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 10183 bytes)
[2025-12-09T20:00:36.227+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 1.0 in stage 61.0 (TID 190) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 10139 bytes)
[2025-12-09T20:00:36.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:36.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 2.0 in stage 61.0 (TID 191) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 9989 bytes)
[2025-12-09T20:00:36.228+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 3.0 in stage 61.0 (TID 192) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 10165 bytes)
[2025-12-09T20:00:36.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_40_piece0 on ead6510418b0:35171 in memory (size: 3.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:36.233+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_40_piece0 on spark-worker:33701 in memory (size: 3.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:36.235+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.236+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:36.237+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_41_piece0 on ead6510418b0:35171 in memory (size: 123.6 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.239+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_41_piece0 on spark-worker:33701 in memory (size: 123.6 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.242+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_44_piece0 in memory on spark-worker:33701 (size: 27.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.245+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManager: Removing RDD 131
[2025-12-09T20:00:36.245+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.249+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_42_piece0 on ead6510418b0:35171 in memory (size: 35.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.250+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_42_piece0 on spark-worker:33701 in memory (size: 35.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.252+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:36.254+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_39_piece0 on ead6510418b0:35171 in memory (size: 4.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.256+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_39_piece0 on spark-worker:33701 in memory (size: 4.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.259+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.260+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:36.260+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:36.261+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:36.261+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:36.261+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BaseHoodieWriteClient: Cleaner started
[2025-12-09T20:00:36.261+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.261+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_38_piece0 on ead6510418b0:35171 in memory (size: 126.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.263+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_38_piece0 on spark-worker:33701 in memory (size: 126.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.268+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:36.276+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.276+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.280+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__REQUESTED__20251209200034685]}
[2025-12-09T20:00:36.281+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.286+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:36.293+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:36.294+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:36.302+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:36.312+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:36.315+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 3.0 in stage 61.0 (TID 192) in 86 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:36.317+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 61.0 (TID 189) in 91 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:36.318+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 2.0 in stage 61.0 (TID 191) in 90 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:36.318+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:36.318+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 1.0 in stage 61.0 (TID 190) in 91 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:36.318+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 61.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.318+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:36.319+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:36.319+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:36.319+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:36.319+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BaseHoodieWriteClient: Scheduling cleaning at instant time :20251209200036192
[2025-12-09T20:00:36.320+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ShuffleMapStage 61 (mapToPair at HoodieJavaRDD.java:149) finished in 0.111 s
[2025-12-09T20:00:36.320+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:36.320+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:36.320+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: waiting: Set(ResultStage 63, ShuffleMapStage 62)
[2025-12-09T20:00:36.320+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:36.320+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ShuffleMapStage 62 (MapPartitionsRDD[141] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:36.324+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_45 stored as values in memory (estimated size 26.9 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.325+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__REQUESTED__20251209200034685]}
[2025-12-09T20:00:36.328+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_45_piece0 stored as bytes in memory (estimated size 12.9 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.328+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on ead6510418b0:35171 (size: 12.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.329+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 45 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.329+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 62 (MapPartitionsRDD[141] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:36.330+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 62.0 with 4 tasks resource profile 0
[2025-12-09T20:00:36.331+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 62.0 (TID 193) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.332+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 1.0 in stage 62.0 (TID 194) (spark-worker, executor 0, partition 1, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.332+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 2.0 in stage 62.0 (TID 195) (spark-worker, executor 0, partition 2, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.333+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 3.0 in stage 62.0 (TID 196) (spark-worker, executor 0, partition 3, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.344+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_45_piece0 in memory on spark-worker:33701 (size: 12.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.351+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 172.19.0.9:46230
[2025-12-09T20:00:36.364+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added rdd_139_3 in memory on spark-worker:33701 (size: 2.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.366+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added rdd_139_2 in memory on spark-worker:33701 (size: 3.4 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.366+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added rdd_139_1 in memory on spark-worker:33701 (size: 2.4 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.367+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added rdd_139_0 in memory on spark-worker:33701 (size: 3.3 KiB, free: 434.3 MiB)
[2025-12-09T20:00:36.373+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 3.0 in stage 62.0 (TID 196) in 41 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:36.376+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 1.0 in stage 62.0 (TID 194) in 44 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:36.377+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 2.0 in stage 62.0 (TID 195) in 44 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:36.377+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 62.0 (TID 193) in 46 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:36.377+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 62.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.377+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ShuffleMapStage 62 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.056 s
[2025-12-09T20:00:36.378+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:36.378+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:36.378+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: waiting: Set(ResultStage 63)
[2025-12-09T20:00:36.378+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:36.378+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ResultStage 63 (ShuffledRDD[142] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:36.380+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_46 stored as values in memory (estimated size 5.5 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.383+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_46_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.383+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on ead6510418b0:35171 (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.384+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 46 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.384+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 63 (ShuffledRDD[142] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:36.384+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 63.0 with 4 tasks resource profile 0
[2025-12-09T20:00:36.386+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 63.0 (TID 197) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.387+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 1.0 in stage 63.0 (TID 198) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.387+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 2.0 in stage 63.0 (TID 199) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.388+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 3.0 in stage 63.0 (TID 200) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.399+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_46_piece0 in memory on spark-worker:33701 (size: 3.1 KiB, free: 434.3 MiB)
[2025-12-09T20:00:36.404+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 172.19.0.9:46230
[2025-12-09T20:00:36.412+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 2.0 in stage 63.0 (TID 199) in 25 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:36.414+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 3.0 in stage 63.0 (TID 200) in 27 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:36.415+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 1.0 in stage 63.0 (TID 198) in 27 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:36.417+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 63.0 (TID 197) in 32 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:36.418+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 63.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.418+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ResultStage 63 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.039 s
[2025-12-09T20:00:36.419+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 34 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:36.419+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 63: Stage finished
[2025-12-09T20:00:36.420+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 34 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.216514 s
[2025-12-09T20:00:36.458+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-12-09T20:00:36.460+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Got job 35 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-12-09T20:00:36.460+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Final stage: ResultStage 64 (collect at HoodieSparkEngineContext.java:150)
[2025-12-09T20:00:36.461+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:36.461+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:36.461+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ResultStage 64 (MapPartitionsRDD[144] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-12-09T20:00:36.472+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_47 stored as values in memory (estimated size 453.4 KiB, free 433.8 MiB)
[2025-12-09T20:00:36.476+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_47_piece0 stored as bytes in memory (estimated size 158.2 KiB, free 433.7 MiB)
[2025-12-09T20:00:36.477+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on ead6510418b0:35171 (size: 158.2 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.478+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 47 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.478+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 64 (MapPartitionsRDD[144] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:36.479+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 64.0 with 1 tasks resource profile 0
[2025-12-09T20:00:36.480+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 64.0 (TID 201) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7249 bytes)
[2025-12-09T20:00:36.489+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_47_piece0 in memory on spark-worker:33701 (size: 158.2 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 64.0 (TID 201) in 21 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:36.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 64.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ResultStage 64 (collect at HoodieSparkEngineContext.java:150) finished in 0.041 s
[2025-12-09T20:00:36.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 35 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:36.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 64: Stage finished
[2025-12-09T20:00:36.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 35 finished: collect at HoodieSparkEngineContext.java:150, took 0.044119 s
[2025-12-09T20:00:36.553+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:116
[2025-12-09T20:00:36.554+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Got job 36 (collect at HoodieSparkEngineContext.java:116) with 1 output partitions
[2025-12-09T20:00:36.554+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Final stage: ResultStage 65 (collect at HoodieSparkEngineContext.java:116)
[2025-12-09T20:00:36.554+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:36.554+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:36.554+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ResultStage 65 (MapPartitionsRDD[146] at map at HoodieSparkEngineContext.java:116), which has no missing parents
[2025-12-09T20:00:36.567+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_48 stored as values in memory (estimated size 453.2 KiB, free 433.2 MiB)
[2025-12-09T20:00:36.570+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_48_piece0 stored as bytes in memory (estimated size 157.8 KiB, free 433.1 MiB)
[2025-12-09T20:00:36.571+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on ead6510418b0:35171 (size: 157.8 KiB, free: 434.0 MiB)
[2025-12-09T20:00:36.571+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 48 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.571+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 65 (MapPartitionsRDD[146] at map at HoodieSparkEngineContext.java:116) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:36.572+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 65.0 with 1 tasks resource profile 0
[2025-12-09T20:00:36.573+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 65.0 (TID 202) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7246 bytes)
[2025-12-09T20:00:36.584+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_48_piece0 in memory on spark-worker:33701 (size: 157.8 KiB, free: 434.0 MiB)
[2025-12-09T20:00:36.596+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 65.0 (TID 202) in 24 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:36.596+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 65.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.597+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ResultStage 65 (collect at HoodieSparkEngineContext.java:116) finished in 0.041 s
[2025-12-09T20:00:36.597+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 36 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:36.597+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 65: Stage finished
[2025-12-09T20:00:36.597+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 36 finished: collect at HoodieSparkEngineContext.java:116, took 0.043959 s
[2025-12-09T20:00:36.600+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkHoodieBloomIndexHelper: Input parallelism: 4, Index parallelism: 4
[2025-12-09T20:00:36.606+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Starting job: countByKey at SparkHoodieBloomIndexHelper.java:197
[2025-12-09T20:00:36.608+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Registering RDD 149 (countByKey at SparkHoodieBloomIndexHelper.java:197) as input to shuffle 16
[2025-12-09T20:00:36.608+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Got job 37 (countByKey at SparkHoodieBloomIndexHelper.java:197) with 4 output partitions
[2025-12-09T20:00:36.608+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Final stage: ResultStage 68 (countByKey at SparkHoodieBloomIndexHelper.java:197)
[2025-12-09T20:00:36.608+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 67)
[2025-12-09T20:00:36.608+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 67)
[2025-12-09T20:00:36.609+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ShuffleMapStage 67 (MapPartitionsRDD[149] at countByKey at SparkHoodieBloomIndexHelper.java:197), which has no missing parents
[2025-12-09T20:00:36.611+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_49 stored as values in memory (estimated size 28.6 KiB, free 433.0 MiB)
[2025-12-09T20:00:36.620+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_49_piece0 stored as bytes in memory (estimated size 13.5 KiB, free 433.0 MiB)
[2025-12-09T20:00:36.621+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on ead6510418b0:35171 (size: 13.5 KiB, free: 434.0 MiB)
[2025-12-09T20:00:36.621+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_45_piece0 on ead6510418b0:35171 in memory (size: 12.9 KiB, free: 434.0 MiB)
[2025-12-09T20:00:36.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 49 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 67 (MapPartitionsRDD[149] at countByKey at SparkHoodieBloomIndexHelper.java:197) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:36.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 67.0 with 4 tasks resource profile 0
[2025-12-09T20:00:36.624+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 67.0 (TID 203) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.625+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 1.0 in stage 67.0 (TID 204) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.625+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 2.0 in stage 67.0 (TID 205) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.626+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_45_piece0 on spark-worker:33701 in memory (size: 12.9 KiB, free: 434.0 MiB)
[2025-12-09T20:00:36.626+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 3.0 in stage 67.0 (TID 206) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.629+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_48_piece0 on ead6510418b0:35171 in memory (size: 157.8 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.631+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_48_piece0 on spark-worker:33701 in memory (size: 157.8 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.634+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_44_piece0 on ead6510418b0:35171 in memory (size: 27.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.636+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_44_piece0 on spark-worker:33701 in memory (size: 27.9 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.636+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_49_piece0 in memory on spark-worker:33701 (size: 13.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.639+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_47_piece0 on ead6510418b0:35171 in memory (size: 158.2 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.641+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_47_piece0 on spark-worker:33701 in memory (size: 158.2 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.646+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_46_piece0 on ead6510418b0:35171 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.646+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 67.0 (TID 203) in 22 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:36.647+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_46_piece0 on spark-worker:33701 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.648+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 2.0 in stage 67.0 (TID 205) in 24 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:36.648+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 3.0 in stage 67.0 (TID 206) in 23 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:36.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 1.0 in stage 67.0 (TID 204) in 25 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:36.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 67.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ShuffleMapStage 67 (countByKey at SparkHoodieBloomIndexHelper.java:197) finished in 0.040 s
[2025-12-09T20:00:36.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:36.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:36.650+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: waiting: Set(ResultStage 68)
[2025-12-09T20:00:36.650+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:36.650+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ResultStage 68 (ShuffledRDD[150] at countByKey at SparkHoodieBloomIndexHelper.java:197), which has no missing parents
[2025-12-09T20:00:36.652+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_50 stored as values in memory (estimated size 5.5 KiB, free 434.4 MiB)
[2025-12-09T20:00:36.654+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_50_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.4 MiB)
[2025-12-09T20:00:36.656+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on ead6510418b0:35171 (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.656+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 50 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.657+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 68 (ShuffledRDD[150] at countByKey at SparkHoodieBloomIndexHelper.java:197) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:36.657+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 68.0 with 4 tasks resource profile 0
[2025-12-09T20:00:36.659+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 68.0 (TID 207) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.660+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 1.0 in stage 68.0 (TID 208) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.660+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 2.0 in stage 68.0 (TID 209) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.660+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 3.0 in stage 68.0 (TID 210) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.671+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_50_piece0 in memory on spark-worker:33701 (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.677+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 16 to 172.19.0.9:46230
[2025-12-09T20:00:36.686+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 3.0 in stage 68.0 (TID 210) in 26 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:36.687+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 2.0 in stage 68.0 (TID 209) in 27 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:36.688+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 68.0 (TID 207) in 28 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:36.688+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 1.0 in stage 68.0 (TID 208) in 28 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:36.688+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 68.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.688+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ResultStage 68 (countByKey at SparkHoodieBloomIndexHelper.java:197) finished in 0.038 s
[2025-12-09T20:00:36.688+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 37 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:36.689+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 68: Stage finished
[2025-12-09T20:00:36.689+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 37 finished: countByKey at SparkHoodieBloomIndexHelper.java:197, took 0.082714 s
[2025-12-09T20:00:36.690+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BucketizedBloomCheckPartitioner: TotalBuckets 0, min_buckets/partition 1
[2025-12-09T20:00:36.718+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapPartitionsRDD: Removing RDD 139 from persistence list
[2025-12-09T20:00:36.719+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManager: Removing RDD 139
[2025-12-09T20:00:36.720+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapPartitionsRDD: Removing RDD 157 from persistence list
[2025-12-09T20:00:36.720+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManager: Removing RDD 157
[2025-12-09T20:00:36.721+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:36.729+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:36.738+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-12-09T20:00:36.740+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Registering RDD 151 (mapToPair at SparkHoodieBloomIndexHelper.java:166) as input to shuffle 20
[2025-12-09T20:00:36.741+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Registering RDD 158 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 17
[2025-12-09T20:00:36.742+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Registering RDD 157 (flatMapToPair at SparkHoodieBloomIndexHelper.java:177) as input to shuffle 18
[2025-12-09T20:00:36.742+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Registering RDD 167 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 19
[2025-12-09T20:00:36.742+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Got job 38 (countByKey at HoodieJavaPairRDD.java:105) with 4 output partitions
[2025-12-09T20:00:36.742+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Final stage: ResultStage 74 (countByKey at HoodieJavaPairRDD.java:105)
[2025-12-09T20:00:36.742+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 73)
[2025-12-09T20:00:36.742+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 73)
[2025-12-09T20:00:36.743+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ShuffleMapStage 71 (MapPartitionsRDD[158] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-12-09T20:00:36.745+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_51 stored as values in memory (estimated size 26.7 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.747+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_51_piece0 stored as bytes in memory (estimated size 12.8 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.748+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on ead6510418b0:35171 (size: 12.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 51 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 71 (MapPartitionsRDD[158] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:36.750+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 71.0 with 4 tasks resource profile 0
[2025-12-09T20:00:36.751+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 71.0 (TID 211) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.751+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 1.0 in stage 71.0 (TID 212) (spark-worker, executor 0, partition 1, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.751+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 2.0 in stage 71.0 (TID 213) (spark-worker, executor 0, partition 2, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.752+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 3.0 in stage 71.0 (TID 214) (spark-worker, executor 0, partition 3, NODE_LOCAL, 7174 bytes)
[2025-12-09T20:00:36.763+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_51_piece0 in memory on spark-worker:33701 (size: 12.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.769+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 172.19.0.9:46230
[2025-12-09T20:00:36.781+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 3.0 in stage 71.0 (TID 214) in 30 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:36.786+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 1.0 in stage 71.0 (TID 212) in 35 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:36.787+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 71.0 (TID 211) in 36 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:36.787+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 2.0 in stage 71.0 (TID 213) in 35 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:36.787+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 71.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.787+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ShuffleMapStage 71 (mapToPair at HoodieJavaRDD.java:149) finished in 0.043 s
[2025-12-09T20:00:36.788+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:36.788+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:36.788+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: waiting: Set(ResultStage 74, ShuffleMapStage 73)
[2025-12-09T20:00:36.788+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:36.788+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ShuffleMapStage 73 (MapPartitionsRDD[167] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:36.790+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_52 stored as values in memory (estimated size 10.0 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.794+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_52_piece0 stored as bytes in memory (estimated size 5.1 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.794+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on ead6510418b0:35171 (size: 5.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.795+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 52 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.795+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 73 (MapPartitionsRDD[167] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:36.795+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 73.0 with 4 tasks resource profile 0
[2025-12-09T20:00:36.796+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 73.0 (TID 215) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:36.796+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 1.0 in stage 73.0 (TID 216) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:36.796+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 2.0 in stage 73.0 (TID 217) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:36.796+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 3.0 in stage 73.0 (TID 218) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:36.809+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_52_piece0 in memory on spark-worker:33701 (size: 5.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.815+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 17 to 172.19.0.9:46230
[2025-12-09T20:00:36.821+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 18 to 172.19.0.9:46230
[2025-12-09T20:00:36.833+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added rdd_165_3 in memory on spark-worker:33701 (size: 2.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.833+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added rdd_165_2 in memory on spark-worker:33701 (size: 3.4 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.834+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added rdd_165_0 in memory on spark-worker:33701 (size: 3.3 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.834+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added rdd_165_1 in memory on spark-worker:33701 (size: 2.4 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.849+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 2.0 in stage 73.0 (TID 217) in 53 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:36.851+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 3.0 in stage 73.0 (TID 218) in 55 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:36.851+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 1.0 in stage 73.0 (TID 216) in 55 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:36.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 73.0 (TID 215) in 55 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:36.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 73.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ShuffleMapStage 73 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.063 s
[2025-12-09T20:00:36.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:36.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:36.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: waiting: Set(ResultStage 74)
[2025-12-09T20:00:36.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:36.852+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ResultStage 74 (ShuffledRDD[168] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:36.853+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_53 stored as values in memory (estimated size 5.5 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.855+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_53_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 434.3 MiB)
[2025-12-09T20:00:36.856+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on ead6510418b0:35171 (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.856+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 53 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.857+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 4 missing tasks from ResultStage 74 (ShuffledRDD[168] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:36.857+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 74.0 with 4 tasks resource profile 0
[2025-12-09T20:00:36.860+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 1.0 in stage 74.0 (TID 219) (spark-worker, executor 0, partition 1, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.860+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 74.0 (TID 220) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.860+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 2.0 in stage 74.0 (TID 221) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.860+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 3.0 in stage 74.0 (TID 222) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:36.871+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_53_piece0 in memory on spark-worker:33701 (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.876+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 19 to 172.19.0.9:46230
[2025-12-09T20:00:36.883+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 2.0 in stage 74.0 (TID 221) in 23 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:36.885+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 74.0 (TID 220) in 25 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:36.885+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 3.0 in stage 74.0 (TID 222) in 25 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:36.886+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 1.0 in stage 74.0 (TID 219) in 27 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:36.886+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 74.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ResultStage 74 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.034 s
[2025-12-09T20:00:36.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 38 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:36.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 74: Stage finished
[2025-12-09T20:00:36.887+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 38 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.149090 s
[2025-12-09T20:00:36.888+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO UpsertPartitioner: AvgRecordSize => 1024
[2025-12-09T20:00:36.931+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
[2025-12-09T20:00:36.931+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_53_piece0 on ead6510418b0:35171 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Got job 39 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
[2025-12-09T20:00:36.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Final stage: ResultStage 75 (collectAsMap at UpsertPartitioner.java:282)
[2025-12-09T20:00:36.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:36.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:36.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting ResultStage 75 (MapPartitionsRDD[170] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
[2025-12-09T20:00:36.933+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_53_piece0 on spark-worker:33701 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.936+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_52_piece0 on ead6510418b0:35171 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.938+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_52_piece0 on spark-worker:33701 in memory (size: 5.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.942+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_50_piece0 on ead6510418b0:35171 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.943+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_50_piece0 on spark-worker:33701 in memory (size: 3.1 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.946+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_54 stored as values in memory (estimated size 454.2 KiB, free 433.9 MiB)
[2025-12-09T20:00:36.946+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_51_piece0 on ead6510418b0:35171 in memory (size: 12.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.947+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_51_piece0 on spark-worker:33701 in memory (size: 12.8 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.949+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO MemoryStore: Block broadcast_54_piece0 stored as bytes in memory (estimated size 159.0 KiB, free 433.8 MiB)
[2025-12-09T20:00:36.949+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on ead6510418b0:35171 (size: 159.0 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.949+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_49_piece0 on ead6510418b0:35171 in memory (size: 13.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.950+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO SparkContext: Created broadcast 54 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:36.950+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 75 (MapPartitionsRDD[170] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:36.951+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Adding task set 75.0 with 1 tasks resource profile 0
[2025-12-09T20:00:36.951+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Removed broadcast_49_piece0 on spark-worker:33701 in memory (size: 13.5 KiB, free: 434.4 MiB)
[2025-12-09T20:00:36.952+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Starting task 0.0 in stage 75.0 (TID 223) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7249 bytes)
[2025-12-09T20:00:36.959+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO BlockManagerInfo: Added broadcast_54_piece0 in memory on spark-worker:33701 (size: 159.0 KiB, free: 434.2 MiB)
[2025-12-09T20:00:36.970+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSetManager: Finished task 0.0 in stage 75.0 (TID 223) in 18 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:36.970+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Removed TaskSet 75.0, whose tasks have all completed, from pool
[2025-12-09T20:00:36.970+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: ResultStage 75 (collectAsMap at UpsertPartitioner.java:282) finished in 0.038 s
[2025-12-09T20:00:36.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 39 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:36.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO TaskSchedulerImpl: Killing all running tasks in stage 75: Stage finished
[2025-12-09T20:00:36.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO DAGScheduler: Job 39 finished: collectAsMap at UpsertPartitioner.java:282, took 0.039913 s
[2025-12-09T20:00:36.972+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:36.978+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:36.978+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO UpsertPartitioner: Total Buckets: 1
[2025-12-09T20:00:36.978+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:36 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/20251209200034599.commit.requested
[2025-12-09T20:00:37.003+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/20251209200034599.inflight
[2025-12-09T20:00:37.020+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-12-09T20:00:37.021+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseCommitActionExecutor: Auto commit disabled for 20251209200034599
[2025-12-09T20:00:37.025+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Starting job: count at HoodieSparkSqlWriter.scala:1050
[2025-12-09T20:00:37.027+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Registering RDD 171 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 21
[2025-12-09T20:00:37.028+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Got job 40 (count at HoodieSparkSqlWriter.scala:1050) with 1 output partitions
[2025-12-09T20:00:37.028+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Final stage: ResultStage 81 (count at HoodieSparkSqlWriter.scala:1050)
[2025-12-09T20:00:37.028+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 80)
[2025-12-09T20:00:37.028+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 80)
[2025-12-09T20:00:37.029+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting ShuffleMapStage 80 (MapPartitionsRDD[171] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-12-09T20:00:37.042+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_55 stored as values in memory (estimated size 459.4 KiB, free 433.4 MiB)
[2025-12-09T20:00:37.045+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_55_piece0 stored as bytes in memory (estimated size 160.8 KiB, free 433.2 MiB)
[2025-12-09T20:00:37.046+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on ead6510418b0:35171 (size: 160.8 KiB, free: 434.1 MiB)
[2025-12-09T20:00:37.046+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Created broadcast 55 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:37.047+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting 4 missing tasks from ShuffleMapStage 80 (MapPartitionsRDD[171] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0, 1, 2, 3))
[2025-12-09T20:00:37.047+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Adding task set 80.0 with 4 tasks resource profile 0
[2025-12-09T20:00:37.048+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 0.0 in stage 80.0 (TID 224) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:37.048+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 1.0 in stage 80.0 (TID 225) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:37.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 2.0 in stage 80.0 (TID 226) (spark-worker, executor 0, partition 2, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:37.049+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 3.0 in stage 80.0 (TID 227) (spark-worker, executor 0, partition 3, PROCESS_LOCAL, 7237 bytes)
[2025-12-09T20:00:37.056+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_55_piece0 in memory on spark-worker:33701 (size: 160.8 KiB, free: 434.1 MiB)
[2025-12-09T20:00:37.076+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 0.0 in stage 80.0 (TID 224) in 28 ms on spark-worker (executor 0) (1/4)
[2025-12-09T20:00:37.078+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 2.0 in stage 80.0 (TID 226) in 30 ms on spark-worker (executor 0) (2/4)
[2025-12-09T20:00:37.079+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 3.0 in stage 80.0 (TID 227) in 30 ms on spark-worker (executor 0) (3/4)
[2025-12-09T20:00:37.079+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 1.0 in stage 80.0 (TID 225) in 30 ms on spark-worker (executor 0) (4/4)
[2025-12-09T20:00:37.079+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Removed TaskSet 80.0, whose tasks have all completed, from pool
[2025-12-09T20:00:37.079+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: ShuffleMapStage 80 (mapToPair at HoodieJavaRDD.java:149) finished in 0.049 s
[2025-12-09T20:00:37.079+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:37.080+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:37.080+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: waiting: Set(ResultStage 81)
[2025-12-09T20:00:37.080+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:37.080+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting ResultStage 81 (MapPartitionsRDD[176] at filter at HoodieSparkSqlWriter.scala:1050), which has no missing parents
[2025-12-09T20:00:37.094+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_56 stored as values in memory (estimated size 476.5 KiB, free 432.7 MiB)
[2025-12-09T20:00:37.096+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_56_piece0 stored as bytes in memory (estimated size 168.3 KiB, free 432.6 MiB)
[2025-12-09T20:00:37.097+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on ead6510418b0:35171 (size: 168.3 KiB, free: 433.9 MiB)
[2025-12-09T20:00:37.097+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Created broadcast 56 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:37.097+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 81 (MapPartitionsRDD[176] at filter at HoodieSparkSqlWriter.scala:1050) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:37.098+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Adding task set 81.0 with 1 tasks resource profile 0
[2025-12-09T20:00:37.098+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 0.0 in stage 81.0 (TID 228) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:37.105+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_56_piece0 in memory on spark-worker:33701 (size: 168.3 KiB, free: 433.9 MiB)
[2025-12-09T20:00:37.119+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 21 to 172.19.0.9:46230
[2025-12-09T20:00:37.183+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MarkerHandler: Request: create marker: 3f3976bb-2437-4eea-af6d-c53e0ae375e6-0_0-81-228_20251209200034599.parquet.marker.CREATE
[2025-12-09T20:00:37.353+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added rdd_175_0 in memory on spark-worker:33701 (size: 311.0 B, free: 433.9 MiB)
[2025-12-09T20:00:37.357+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 0.0 in stage 81.0 (TID 228) in 259 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:37.358+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Removed TaskSet 81.0, whose tasks have all completed, from pool
[2025-12-09T20:00:37.359+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: ResultStage 81 (count at HoodieSparkSqlWriter.scala:1050) finished in 0.278 s
[2025-12-09T20:00:37.359+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Job 40 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:37.359+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 81: Stage finished
[2025-12-09T20:00:37.360+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Job 40 finished: count at HoodieSparkSqlWriter.scala:1050, took 0.334660 s
[2025-12-09T20:00:37.360+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieSparkSqlWriter$: Proceeding to commit the write.
[2025-12-09T20:00:37.388+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Starting job: collect at SparkRDDWriteClient.java:103
[2025-12-09T20:00:37.389+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Got job 41 (collect at SparkRDDWriteClient.java:103) with 1 output partitions
[2025-12-09T20:00:37.390+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Final stage: ResultStage 87 (collect at SparkRDDWriteClient.java:103)
[2025-12-09T20:00:37.390+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 86)
[2025-12-09T20:00:37.390+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:37.391+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting ResultStage 87 (MapPartitionsRDD[177] at map at SparkRDDWriteClient.java:103), which has no missing parents
[2025-12-09T20:00:37.403+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_57 stored as values in memory (estimated size 477.2 KiB, free 432.1 MiB)
[2025-12-09T20:00:37.406+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_57_piece0 stored as bytes in memory (estimated size 168.6 KiB, free 431.9 MiB)
[2025-12-09T20:00:37.407+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on ead6510418b0:35171 (size: 168.6 KiB, free: 433.8 MiB)
[2025-12-09T20:00:37.408+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Created broadcast 57 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:37.408+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 87 (MapPartitionsRDD[177] at map at SparkRDDWriteClient.java:103) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:37.408+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Adding task set 87.0 with 1 tasks resource profile 0
[2025-12-09T20:00:37.410+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 0.0 in stage 87.0 (TID 229) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7185 bytes)
[2025-12-09T20:00:37.421+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_57_piece0 in memory on spark-worker:33701 (size: 168.6 KiB, free: 433.7 MiB)
[2025-12-09T20:00:37.435+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 0.0 in stage 87.0 (TID 229) in 26 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:37.436+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Removed TaskSet 87.0, whose tasks have all completed, from pool
[2025-12-09T20:00:37.436+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: ResultStage 87 (collect at SparkRDDWriteClient.java:103) finished in 0.045 s
[2025-12-09T20:00:37.437+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Job 41 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:37.437+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 87: Stage finished
[2025-12-09T20:00:37.437+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Job 41 finished: collect at SparkRDDWriteClient.java:103, took 0.049287 s
[2025-12-09T20:00:37.437+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseHoodieWriteClient: Committing 20251209200034599 action commit
[2025-12-09T20:00:37.438+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.444+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:37.448+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.449+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.453+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__INFLIGHT__20251209200036993]}
[2025-12-09T20:00:37.453+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.459+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:37.465+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.465+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:37.472+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.478+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:37.482+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:37.483+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:37.483+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:37.483+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:37.484+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:37.484+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO CommitUtils: Creating  metadata for UPSERT numWriteStats:1 numReplaceFileIds:0
[2025-12-09T20:00:37.484+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.491+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:37.497+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.498+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.502+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__commit__INFLIGHT__20251209200036993]}
[2025-12-09T20:00:37.503+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.509+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:37.516+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.516+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:37.520+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.527+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:37.532+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:37.532+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:37.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:37.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:37.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:37.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseHoodieWriteClient: Committing 20251209200034599 action commit
[2025-12-09T20:00:37.545+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.551+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:37.555+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.557+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.565+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.572+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.577+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:37.577+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieBackedTableMetadataWriter: Async metadata indexing disabled and following partitions already initialized: [files]
[2025-12-09T20:00:37.577+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.583+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:37.587+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:37.588+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:37.593+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.598+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:37.601+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:37.602+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:37.602+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:37.605+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetadataUtil: Updating at 20251209200034599 from Commit/UPSERT. #partitions_updated=2, #files_added=1
[2025-12-09T20:00:37.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:37.622+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:37.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetadataUtil: Loading latest file slices for metadata table partition files
[2025-12-09T20:00:37.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:37.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:37.623+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AbstractTableFileSystemView: Building file system view for partition (files)
[2025-12-09T20:00:37.630+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-12-09T20:00:37.630+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseHoodieClient: Embedded Timeline Server is disabled. Not starting timeline service
[2025-12-09T20:00:37.630+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.636+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.640+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.640+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.645+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:37.649+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.653+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:37.653+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:37.653+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieBackedTableMetadataWriter: New commit at 20251209200034599 being applied to MDT.
[2025-12-09T20:00:37.654+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.658+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.663+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.664+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.667+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:37.667+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO CleanerUtils: Cleaned failed attempts if any
[2025-12-09T20:00:37.668+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.672+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.677+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.677+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.680+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[00000000000000010__deltacommit__COMPLETED__20251209200035657]}
[2025-12-09T20:00:37.684+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.688+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:37.689+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:37.689+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseHoodieWriteClient: Generate a new instant time: 20251209200034599 action: deltacommit
[2025-12-09T20:00:37.689+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Creating a new instant [==>20251209200034599__deltacommit__REQUESTED]
[2025-12-09T20:00:37.704+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.710+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.715+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.716+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:37.719+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[==>20251209200034599__deltacommit__REQUESTED__20251209200037695]}
[2025-12-09T20:00:37.722+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:37.727+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:37.727+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:37.727+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AsyncCleanerService: The HoodieWriteClient is not configured to auto & async clean. Async clean service will not start.
[2025-12-09T20:00:37.727+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AsyncArchiveService: The HoodieWriteClient is not configured to auto & async archive. Async archive service will not start.
[2025-12-09T20:00:37.728+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:37.728+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:37.735+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Starting job: countByKey at HoodieJavaPairRDD.java:105
[2025-12-09T20:00:37.736+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Registering RDD 188 (countByKey at HoodieJavaPairRDD.java:105) as input to shuffle 22
[2025-12-09T20:00:37.736+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Got job 42 (countByKey at HoodieJavaPairRDD.java:105) with 1 output partitions
[2025-12-09T20:00:37.736+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Final stage: ResultStage 89 (countByKey at HoodieJavaPairRDD.java:105)
[2025-12-09T20:00:37.737+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 88)
[2025-12-09T20:00:37.737+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 88)
[2025-12-09T20:00:37.737+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting ShuffleMapStage 88 (MapPartitionsRDD[188] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:37.738+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_58 stored as values in memory (estimated size 10.1 KiB, free 431.9 MiB)
[2025-12-09T20:00:37.740+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_58_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 431.9 MiB)
[2025-12-09T20:00:37.741+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on ead6510418b0:35171 (size: 5.5 KiB, free: 433.8 MiB)
[2025-12-09T20:00:37.741+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Created broadcast 58 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:37.742+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 88 (MapPartitionsRDD[188] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:37.742+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Adding task set 88.0 with 1 tasks resource profile 0
[2025-12-09T20:00:37.744+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 0.0 in stage 88.0 (TID 230) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7688 bytes)
[2025-12-09T20:00:37.752+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_58_piece0 in memory on spark-worker:33701 (size: 5.5 KiB, free: 433.7 MiB)
[2025-12-09T20:00:37.757+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added rdd_186_0 in memory on spark-worker:33701 (size: 354.0 B, free: 433.7 MiB)
[2025-12-09T20:00:37.765+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 0.0 in stage 88.0 (TID 230) in 22 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:37.765+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Removed TaskSet 88.0, whose tasks have all completed, from pool
[2025-12-09T20:00:37.766+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: ShuffleMapStage 88 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.028 s
[2025-12-09T20:00:37.766+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:37.766+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:37.766+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: waiting: Set(ResultStage 89)
[2025-12-09T20:00:37.766+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:37.766+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting ResultStage 89 (ShuffledRDD[189] at countByKey at HoodieJavaPairRDD.java:105), which has no missing parents
[2025-12-09T20:00:37.767+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_59 stored as values in memory (estimated size 5.5 KiB, free 431.9 MiB)
[2025-12-09T20:00:37.778+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_59_piece0 stored as bytes in memory (estimated size 3.1 KiB, free 431.9 MiB)
[2025-12-09T20:00:37.778+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on ead6510418b0:35171 (size: 3.1 KiB, free: 433.8 MiB)
[2025-12-09T20:00:37.779+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Created broadcast 59 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:37.779+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Removed broadcast_54_piece0 on ead6510418b0:35171 in memory (size: 159.0 KiB, free: 433.9 MiB)
[2025-12-09T20:00:37.780+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 89 (ShuffledRDD[189] at countByKey at HoodieJavaPairRDD.java:105) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:37.780+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Adding task set 89.0 with 1 tasks resource profile 0
[2025-12-09T20:00:37.781+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Removed broadcast_54_piece0 on spark-worker:33701 in memory (size: 159.0 KiB, free: 433.9 MiB)
[2025-12-09T20:00:37.782+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 0.0 in stage 89.0 (TID 231) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:37.785+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Removed broadcast_56_piece0 on ead6510418b0:35171 in memory (size: 168.3 KiB, free: 434.1 MiB)
[2025-12-09T20:00:37.786+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Removed broadcast_56_piece0 on spark-worker:33701 in memory (size: 168.3 KiB, free: 434.1 MiB)
[2025-12-09T20:00:37.788+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Removed broadcast_55_piece0 on ead6510418b0:35171 in memory (size: 160.8 KiB, free: 434.2 MiB)
[2025-12-09T20:00:37.790+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Removed broadcast_55_piece0 on spark-worker:33701 in memory (size: 160.8 KiB, free: 434.2 MiB)
[2025-12-09T20:00:37.790+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_59_piece0 in memory on spark-worker:33701 (size: 3.1 KiB, free: 434.2 MiB)
[2025-12-09T20:00:37.793+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Removed broadcast_57_piece0 on ead6510418b0:35171 in memory (size: 168.6 KiB, free: 434.4 MiB)
[2025-12-09T20:00:37.794+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 22 to 172.19.0.9:46230
[2025-12-09T20:00:37.794+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Removed broadcast_57_piece0 on spark-worker:33701 in memory (size: 168.6 KiB, free: 434.4 MiB)
[2025-12-09T20:00:37.804+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 0.0 in stage 89.0 (TID 231) in 23 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:37.805+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Removed TaskSet 89.0, whose tasks have all completed, from pool
[2025-12-09T20:00:37.805+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: ResultStage 89 (countByKey at HoodieJavaPairRDD.java:105) finished in 0.038 s
[2025-12-09T20:00:37.805+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Job 42 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:37.805+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 89: Stage finished
[2025-12-09T20:00:37.805+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Job 42 finished: countByKey at HoodieJavaPairRDD.java:105, took 0.070097 s
[2025-12-09T20:00:37.806+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO UpsertPartitioner: AvgRecordSize => 1024
[2025-12-09T20:00:37.834+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Starting job: collectAsMap at UpsertPartitioner.java:282
[2025-12-09T20:00:37.835+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Got job 43 (collectAsMap at UpsertPartitioner.java:282) with 1 output partitions
[2025-12-09T20:00:37.835+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Final stage: ResultStage 90 (collectAsMap at UpsertPartitioner.java:282)
[2025-12-09T20:00:37.836+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:37.836+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:37.836+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting ResultStage 90 (MapPartitionsRDD[191] at mapToPair at UpsertPartitioner.java:281), which has no missing parents
[2025-12-09T20:00:37.844+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_60 stored as values in memory (estimated size 352.5 KiB, free 434.0 MiB)
[2025-12-09T20:00:37.847+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_60_piece0 stored as bytes in memory (estimated size 123.7 KiB, free 433.9 MiB)
[2025-12-09T20:00:37.847+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on ead6510418b0:35171 (size: 123.7 KiB, free: 434.3 MiB)
[2025-12-09T20:00:37.847+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Created broadcast 60 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:37.848+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 90 (MapPartitionsRDD[191] at mapToPair at UpsertPartitioner.java:281) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:37.848+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Adding task set 90.0 with 1 tasks resource profile 0
[2025-12-09T20:00:37.849+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 0.0 in stage 90.0 (TID 232) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7253 bytes)
[2025-12-09T20:00:37.857+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_60_piece0 in memory on spark-worker:33701 (size: 123.7 KiB, free: 434.3 MiB)
[2025-12-09T20:00:37.873+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 0.0 in stage 90.0 (TID 232) in 25 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:37.874+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Removed TaskSet 90.0, whose tasks have all completed, from pool
[2025-12-09T20:00:37.874+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: ResultStage 90 (collectAsMap at UpsertPartitioner.java:282) finished in 0.038 s
[2025-12-09T20:00:37.874+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Job 43 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:37.874+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Killing all running tasks in stage 90: Stage finished
[2025-12-09T20:00:37.875+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Job 43 finished: collectAsMap at UpsertPartitioner.java:282, took 0.040372 s
[2025-12-09T20:00:37.876+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:37.876+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:37.876+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO UpsertPartitioner: Total Buckets: 1
[2025-12-09T20:00:37.876+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/20251209200034599.deltacommit.requested
[2025-12-09T20:00:37.894+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/20251209200034599.deltacommit.inflight
[2025-12-09T20:00:37.907+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseSparkCommitActionExecutor: no validators configured.
[2025-12-09T20:00:37.908+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BaseCommitActionExecutor: Auto commit enabled: Committing 20251209200034599
[2025-12-09T20:00:37.931+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Starting job: collect at HoodieJavaRDD.java:177
[2025-12-09T20:00:37.931+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Registering RDD 192 (mapToPair at HoodieJavaRDD.java:149) as input to shuffle 23
[2025-12-09T20:00:37.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Got job 44 (collect at HoodieJavaRDD.java:177) with 1 output partitions
[2025-12-09T20:00:37.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Final stage: ResultStage 92 (collect at HoodieJavaRDD.java:177)
[2025-12-09T20:00:37.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 91)
[2025-12-09T20:00:37.932+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 91)
[2025-12-09T20:00:37.933+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting ShuffleMapStage 91 (MapPartitionsRDD[192] at mapToPair at HoodieJavaRDD.java:149), which has no missing parents
[2025-12-09T20:00:37.942+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_61 stored as values in memory (estimated size 357.3 KiB, free 433.6 MiB)
[2025-12-09T20:00:37.946+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_61_piece0 stored as bytes in memory (estimated size 123.9 KiB, free 433.4 MiB)
[2025-12-09T20:00:37.946+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on ead6510418b0:35171 (size: 123.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:37.947+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Created broadcast 61 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:37.947+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 91 (MapPartitionsRDD[192] at mapToPair at HoodieJavaRDD.java:149) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:37.947+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Adding task set 91.0 with 1 tasks resource profile 0
[2025-12-09T20:00:37.948+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 0.0 in stage 91.0 (TID 233) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7688 bytes)
[2025-12-09T20:00:37.956+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_61_piece0 in memory on spark-worker:33701 (size: 123.9 KiB, free: 434.1 MiB)
[2025-12-09T20:00:37.970+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Finished task 0.0 in stage 91.0 (TID 233) in 22 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:37.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Removed TaskSet 91.0, whose tasks have all completed, from pool
[2025-12-09T20:00:37.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: ShuffleMapStage 91 (mapToPair at HoodieJavaRDD.java:149) finished in 0.038 s
[2025-12-09T20:00:37.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: looking for newly runnable stages
[2025-12-09T20:00:37.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: running: Set()
[2025-12-09T20:00:37.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: waiting: Set(ResultStage 92)
[2025-12-09T20:00:37.971+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: failed: Set()
[2025-12-09T20:00:37.972+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting ResultStage 92 (MapPartitionsRDD[197] at map at HoodieJavaRDD.java:125), which has no missing parents
[2025-12-09T20:00:37.984+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_62 stored as values in memory (estimated size 459.3 KiB, free 433.0 MiB)
[2025-12-09T20:00:37.987+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO MemoryStore: Block broadcast_62_piece0 stored as bytes in memory (estimated size 162.6 KiB, free 432.8 MiB)
[2025-12-09T20:00:37.988+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on ead6510418b0:35171 (size: 162.6 KiB, free: 434.0 MiB)
[2025-12-09T20:00:37.989+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO SparkContext: Created broadcast 62 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:37.989+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 92 (MapPartitionsRDD[197] at map at HoodieJavaRDD.java:125) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:37.989+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSchedulerImpl: Adding task set 92.0 with 1 tasks resource profile 0
[2025-12-09T20:00:37.990+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO TaskSetManager: Starting task 0.0 in stage 92.0 (TID 234) (spark-worker, executor 0, partition 0, NODE_LOCAL, 7185 bytes)
[2025-12-09T20:00:37.999+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:37 INFO BlockManagerInfo: Added broadcast_62_piece0 in memory on spark-worker:33701 (size: 162.6 KiB, free: 434.0 MiB)
[2025-12-09T20:00:38.010+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 23 to 172.19.0.9:46230
[2025-12-09T20:00:38.106+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Added rdd_196_0 in memory on spark-worker:33701 (size: 367.0 B, free: 434.0 MiB)
[2025-12-09T20:00:38.111+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Finished task 0.0 in stage 92.0 (TID 234) in 121 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:38.112+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Removed TaskSet 92.0, whose tasks have all completed, from pool
[2025-12-09T20:00:38.112+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: ResultStage 92 (collect at HoodieJavaRDD.java:177) finished in 0.139 s
[2025-12-09T20:00:38.112+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Job 44 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:38.112+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 92: Stage finished
[2025-12-09T20:00:38.112+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Job 44 finished: collect at HoodieJavaRDD.java:177, took 0.181521 s
[2025-12-09T20:00:38.113+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO CommitUtils: Creating  metadata for UPSERT_PREPPED numWriteStats:1 numReplaceFileIds:0
[2025-12-09T20:00:38.113+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BaseSparkCommitActionExecutor: Committing 20251209200034599, action Type deltacommit, operation Type UPSERT_PREPPED
[2025-12-09T20:00:38.132+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO SparkContext: Starting job: collect at HoodieSparkEngineContext.java:150
[2025-12-09T20:00:38.133+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Got job 45 (collect at HoodieSparkEngineContext.java:150) with 1 output partitions
[2025-12-09T20:00:38.133+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Final stage: ResultStage 93 (collect at HoodieSparkEngineContext.java:150)
[2025-12-09T20:00:38.134+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:38.134+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:38.134+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Submitting ResultStage 93 (MapPartitionsRDD[199] at flatMap at HoodieSparkEngineContext.java:150), which has no missing parents
[2025-12-09T20:00:38.137+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MemoryStore: Block broadcast_63 stored as values in memory (estimated size 100.7 KiB, free 432.7 MiB)
[2025-12-09T20:00:38.140+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MemoryStore: Block broadcast_63_piece0 stored as bytes in memory (estimated size 35.9 KiB, free 432.7 MiB)
[2025-12-09T20:00:38.141+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on ead6510418b0:35171 (size: 35.9 KiB, free: 434.0 MiB)
[2025-12-09T20:00:38.142+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO SparkContext: Created broadcast 63 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:38.143+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 93 (MapPartitionsRDD[199] at flatMap at HoodieSparkEngineContext.java:150) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:38.143+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Adding task set 93.0 with 1 tasks resource profile 0
[2025-12-09T20:00:38.144+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Starting task 0.0 in stage 93.0 (TID 235) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7344 bytes)
[2025-12-09T20:00:38.153+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Added broadcast_63_piece0 in memory on spark-worker:33701 (size: 35.9 KiB, free: 433.9 MiB)
[2025-12-09T20:00:38.164+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Finished task 0.0 in stage 93.0 (TID 235) in 19 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:38.164+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Removed TaskSet 93.0, whose tasks have all completed, from pool
[2025-12-09T20:00:38.164+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: ResultStage 93 (collect at HoodieSparkEngineContext.java:150) finished in 0.030 s
[2025-12-09T20:00:38.164+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Job 45 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:38.164+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 93: Stage finished
[2025-12-09T20:00:38.164+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Job 45 finished: collect at HoodieSparkEngineContext.java:150, took 0.032110 s
[2025-12-09T20:00:38.165+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Marking instant complete [==>20251209200034599__deltacommit__INFLIGHT]
[2025-12-09T20:00:38.165+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/20251209200034599.deltacommit.inflight
[2025-12-09T20:00:38.185+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/20251209200034599.deltacommit
[2025-12-09T20:00:38.185+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Completed [==>20251209200034599__deltacommit__INFLIGHT]
[2025-12-09T20:00:38.185+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BaseSparkCommitActionExecutor: Committed 20251209200034599
[2025-12-09T20:00:38.204+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-12-09T20:00:38.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Got job 46 (collectAsMap at HoodieSparkEngineContext.java:164) with 1 output partitions
[2025-12-09T20:00:38.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Final stage: ResultStage 94 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-12-09T20:00:38.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:38.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:38.205+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Submitting ResultStage 94 (MapPartitionsRDD[201] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-12-09T20:00:38.209+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MemoryStore: Block broadcast_64 stored as values in memory (estimated size 100.9 KiB, free 432.6 MiB)
[2025-12-09T20:00:38.219+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MemoryStore: Block broadcast_64_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 432.6 MiB)
[2025-12-09T20:00:38.219+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on ead6510418b0:35171 (size: 36.0 KiB, free: 433.9 MiB)
[2025-12-09T20:00:38.219+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_63_piece0 on ead6510418b0:35171 in memory (size: 35.9 KiB, free: 434.0 MiB)
[2025-12-09T20:00:38.220+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO SparkContext: Created broadcast 64 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:38.220+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 94 (MapPartitionsRDD[201] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0))
[2025-12-09T20:00:38.220+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Adding task set 94.0 with 1 tasks resource profile 0
[2025-12-09T20:00:38.221+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_63_piece0 on spark-worker:33701 in memory (size: 35.9 KiB, free: 434.0 MiB)
[2025-12-09T20:00:38.221+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Starting task 0.0 in stage 94.0 (TID 236) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7344 bytes)
[2025-12-09T20:00:38.224+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_62_piece0 on ead6510418b0:35171 in memory (size: 162.6 KiB, free: 434.1 MiB)
[2025-12-09T20:00:38.226+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_62_piece0 on spark-worker:33701 in memory (size: 162.6 KiB, free: 434.1 MiB)
[2025-12-09T20:00:38.229+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_59_piece0 on ead6510418b0:35171 in memory (size: 3.1 KiB, free: 434.1 MiB)
[2025-12-09T20:00:38.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Added broadcast_64_piece0 in memory on spark-worker:33701 (size: 36.0 KiB, free: 434.1 MiB)
[2025-12-09T20:00:38.230+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_59_piece0 on spark-worker:33701 in memory (size: 3.1 KiB, free: 434.1 MiB)
[2025-12-09T20:00:38.235+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_60_piece0 on ead6510418b0:35171 in memory (size: 123.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:38.236+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_60_piece0 on spark-worker:33701 in memory (size: 123.7 KiB, free: 434.2 MiB)
[2025-12-09T20:00:38.239+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_58_piece0 on ead6510418b0:35171 in memory (size: 5.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:38.240+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_58_piece0 on spark-worker:33701 in memory (size: 5.5 KiB, free: 434.2 MiB)
[2025-12-09T20:00:38.245+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_61_piece0 on ead6510418b0:35171 in memory (size: 123.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:38.246+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Removed broadcast_61_piece0 on spark-worker:33701 in memory (size: 123.9 KiB, free: 434.4 MiB)
[2025-12-09T20:00:38.264+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Finished task 0.0 in stage 94.0 (TID 236) in 42 ms on spark-worker (executor 0) (1/1)
[2025-12-09T20:00:38.264+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Removed TaskSet 94.0, whose tasks have all completed, from pool
[2025-12-09T20:00:38.264+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: ResultStage 94 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.058 s
[2025-12-09T20:00:38.265+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Job 46 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:38.265+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 94: Stage finished
[2025-12-09T20:00:38.265+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Job 46 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.060222 s
[2025-12-09T20:00:38.284+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/.temp/20251209200034599
[2025-12-09T20:00:38.285+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:38.289+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:38.294+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:38.295+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders//.hoodie/metadata
[2025-12-09T20:00:38.298+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__deltacommit__COMPLETED__20251209200038176]}
[2025-12-09T20:00:38.302+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:38.308+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO FileSystemViewManager: Creating View Manager with storage type :MEMORY
[2025-12-09T20:00:38.309+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO FileSystemViewManager: Creating in-memory based Table View
[2025-12-09T20:00:38.313+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__deltacommit__COMPLETED__20251209200038176]}
[2025-12-09T20:00:38.313+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Marking instant complete [==>20251209200034599__commit__INFLIGHT]
[2025-12-09T20:00:38.313+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Checking for file exists ?s3a://huditest/silver/table_name=orders/.hoodie/20251209200034599.inflight
[2025-12-09T20:00:38.333+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Create new file for toInstant ?s3a://huditest/silver/table_name=orders/.hoodie/20251209200034599.commit
[2025-12-09T20:00:38.333+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Completed [==>20251209200034599__commit__INFLIGHT]
[2025-12-09T20:00:38.356+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO SparkContext: Starting job: collectAsMap at HoodieSparkEngineContext.java:164
[2025-12-09T20:00:38.357+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Got job 47 (collectAsMap at HoodieSparkEngineContext.java:164) with 2 output partitions
[2025-12-09T20:00:38.358+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Final stage: ResultStage 95 (collectAsMap at HoodieSparkEngineContext.java:164)
[2025-12-09T20:00:38.358+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Parents of final stage: List()
[2025-12-09T20:00:38.358+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Missing parents: List()
[2025-12-09T20:00:38.358+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Submitting ResultStage 95 (MapPartitionsRDD[203] at mapToPair at HoodieSparkEngineContext.java:161), which has no missing parents
[2025-12-09T20:00:38.363+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MemoryStore: Block broadcast_65 stored as values in memory (estimated size 100.9 KiB, free 434.2 MiB)
[2025-12-09T20:00:38.366+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MemoryStore: Block broadcast_65_piece0 stored as bytes in memory (estimated size 36.0 KiB, free 434.1 MiB)
[2025-12-09T20:00:38.366+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on ead6510418b0:35171 (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:38.367+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO SparkContext: Created broadcast 65 from broadcast at DAGScheduler.scala:1535
[2025-12-09T20:00:38.367+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Submitting 2 missing tasks from ResultStage 95 (MapPartitionsRDD[203] at mapToPair at HoodieSparkEngineContext.java:161) (first 15 tasks are for partitions Vector(0, 1))
[2025-12-09T20:00:38.367+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Adding task set 95.0 with 2 tasks resource profile 0
[2025-12-09T20:00:38.368+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Starting task 0.0 in stage 95.0 (TID 237) (spark-worker, executor 0, partition 0, PROCESS_LOCAL, 7334 bytes)
[2025-12-09T20:00:38.369+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Starting task 1.0 in stage 95.0 (TID 238) (spark-worker, executor 0, partition 1, PROCESS_LOCAL, 7330 bytes)
[2025-12-09T20:00:38.378+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManagerInfo: Added broadcast_65_piece0 in memory on spark-worker:33701 (size: 36.0 KiB, free: 434.3 MiB)
[2025-12-09T20:00:38.403+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Finished task 0.0 in stage 95.0 (TID 237) in 35 ms on spark-worker (executor 0) (1/2)
[2025-12-09T20:00:38.434+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSetManager: Finished task 1.0 in stage 95.0 (TID 238) in 65 ms on spark-worker (executor 0) (2/2)
[2025-12-09T20:00:38.434+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Removed TaskSet 95.0, whose tasks have all completed, from pool
[2025-12-09T20:00:38.434+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: ResultStage 95 (collectAsMap at HoodieSparkEngineContext.java:164) finished in 0.075 s
[2025-12-09T20:00:38.434+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Job 47 is finished. Cancelling potential speculative or zombie tasks for this job
[2025-12-09T20:00:38.434+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TaskSchedulerImpl: Killing all running tasks in stage 95: Stage finished
[2025-12-09T20:00:38.435+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO DAGScheduler: Job 47 finished: collectAsMap at HoodieSparkEngineContext.java:164, took 0.078260 s
[2025-12-09T20:00:38.452+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO FSUtils: Removed directory at s3a://huditest/silver/table_name=orders/.hoodie/.temp/20251209200034599
[2025-12-09T20:00:38.453+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BaseHoodieWriteClient: Committed 20251209200034599
[2025-12-09T20:00:38.454+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MapPartitionsRDD: Removing RDD 165 from persistence list
[2025-12-09T20:00:38.455+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManager: Removing RDD 165
[2025-12-09T20:00:38.455+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MapPartitionsRDD: Removing RDD 175 from persistence list
[2025-12-09T20:00:38.456+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManager: Removing RDD 175
[2025-12-09T20:00:38.456+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO UnionRDD: Removing RDD 186 from persistence list
[2025-12-09T20:00:38.457+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManager: Removing RDD 186
[2025-12-09T20:00:38.457+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO MapPartitionsRDD: Removing RDD 196 from persistence list
[2025-12-09T20:00:38.458+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BlockManager: Removing RDD 196
[2025-12-09T20:00:38.459+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BaseHoodieWriteClient: Async cleaner has been spawned. Waiting for it to finish
[2025-12-09T20:00:38.459+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO AsyncCleanerService: Waiting for async clean service to finish
[2025-12-09T20:00:38.459+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BaseHoodieWriteClient: Async cleaner has finished
[2025-12-09T20:00:38.459+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:38.464+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:38.468+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:38.468+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Loading Active commit timeline for s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:38.472+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__commit__COMPLETED__20251209200038325]}
[2025-12-09T20:00:38.473+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:38.478+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:38.482+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:38.482+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:38.488+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:38.494+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:38.498+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__deltacommit__COMPLETED__20251209200038176]}
[2025-12-09T20:00:38.499+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:38.499+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:38.499+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO FileSystemViewManager: Creating View Manager with storage type :REMOTE_FIRST
[2025-12-09T20:00:38.499+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO FileSystemViewManager: Creating remote first table view
[2025-12-09T20:00:38.499+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BaseHoodieWriteClient: Start to archive synchronously.
[2025-12-09T20:00:38.504+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__commit__COMPLETED__20251209200038325]}
[2025-12-09T20:00:38.504+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:38.509+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/hoodie.properties
[2025-12-09T20:00:38.514+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Finished Loading Table of type COPY_ON_WRITE(version=1, baseFileFormat=PARQUET) from s3a://huditest/silver/table_name=orders/
[2025-12-09T20:00:38.514+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Loading HoodieTableMetaClient from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:38.518+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableConfig: Loading table properties from s3a://huditest/silver/table_name=orders/.hoodie/metadata/.hoodie/hoodie.properties
[2025-12-09T20:00:38.523+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTableMetaClient: Finished Loading Table of type MERGE_ON_READ(version=1, baseFileFormat=HFILE) from s3a://huditest/silver/table_name=orders/.hoodie/metadata
[2025-12-09T20:00:38.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__deltacommit__COMPLETED__20251209200038176]}
[2025-12-09T20:00:38.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:38.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:38.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTimelineArchiver: Not archiving as there is no compaction yet on the metadata table
[2025-12-09T20:00:38.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieTimelineArchiver: No Instants to archive
[2025-12-09T20:00:38.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO FileSystemViewManager: Creating remote view for basePath s3a://huditest/silver/table_name=orders. Server=ead6510418b0:35359, Timeout=300
[2025-12-09T20:00:38.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO FileSystemViewManager: Creating InMemory based view for basePath s3a://huditest/silver/table_name=orders
[2025-12-09T20:00:38.529+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:38.533+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:38.537+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__commit__COMPLETED__20251209200038325]}
[2025-12-09T20:00:38.537+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO RemoteHoodieTableFileSystemView: Sending request : (http://ead6510418b0:35359/v1/hoodie/view/refresh/?basepath=s3a%3A%2F%2Fhuditest%2Fsilver%2Ftable_name%3Dorders&lastinstantts=20251209200034599&timelinehash=21d90ccc72b1abcfeca936cf389cec35aab1d9c1632efcae8c92756e0a3394e4)
[2025-12-09T20:00:38.545+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__commit__COMPLETED__20251209200038325]}
[2025-12-09T20:00:38.545+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO AbstractTableFileSystemView: Took 1 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:38.549+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:38.553+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__commit__COMPLETED__20251209200038325]}
[2025-12-09T20:00:38.558+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieActiveTimeline: Loaded instants upto : Option{val=[20251209200034599__deltacommit__COMPLETED__20251209200038176]}
[2025-12-09T20:00:38.558+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO AbstractTableFileSystemView: Took 0 ms to read  0 instants, 0 replaced file groups
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO ClusteringUtils: Found 0 files in pending clustering operations
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Commit 20251209200034599 successful!
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Compaction Scheduled is Optional.empty
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Clustering Scheduled is Optional.empty
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Is Async Compaction Enabled ? false
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Config.inlineCompactionEnabled ? false
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Config.asyncClusteringEnabled ? false
[2025-12-09T20:00:38.559+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO HoodieSparkSqlWriter$: Closing write client
[2025-12-09T20:00:38.560+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO BaseHoodieClient: Stopping Timeline service !!
[2025-12-09T20:00:38.560+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO EmbeddedTimelineService: Closing Timeline server
[2025-12-09T20:00:38.560+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TimelineService: Closing Timeline Service
[2025-12-09T20:00:38.560+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO Javalin: Stopping Javalin ...
[2025-12-09T20:00:38.567+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO Javalin: Javalin has stopped
[2025-12-09T20:00:38.567+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO TimelineService: Closed Timeline Service
[2025-12-09T20:00:38.568+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO EmbeddedTimelineService: Closed Timeline server
[2025-12-09T20:00:38.568+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:38 INFO AsyncCleanerService: Shutting down async clean service...
[2025-12-09T20:00:39.614+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO SparkContext: Invoking stop() from shutdown hook
[2025-12-09T20:00:39.614+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO SparkContext: SparkContext is stopping with exitCode 0.
[2025-12-09T20:00:39.628+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO SparkUI: Stopped Spark web UI at http://ead6510418b0:4040
[2025-12-09T20:00:39.631+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO StandaloneSchedulerBackend: Shutting down all executors
[2025-12-09T20:00:39.631+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO StandaloneSchedulerBackend$StandaloneDriverEndpoint: Asking each executor to shut down
[2025-12-09T20:00:39.647+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
[2025-12-09T20:00:39.670+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO MemoryStore: MemoryStore cleared
[2025-12-09T20:00:39.670+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO BlockManager: BlockManager stopped
[2025-12-09T20:00:39.673+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO BlockManagerMaster: BlockManagerMaster stopped
[2025-12-09T20:00:39.676+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
[2025-12-09T20:00:39.723+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO SparkContext: Successfully stopped SparkContext
[2025-12-09T20:00:39.724+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO ShutdownHookManager: Shutdown hook called
[2025-12-09T20:00:39.725+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-8f39ccbf-354a-4551-930f-e76d9897db63
[2025-12-09T20:00:39.729+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-e4d45da3-19dc-4870-aa7a-566ca7d7d0b9/pyspark-3ba87c9d-3805-449d-b9e8-7cb245d9335e
[2025-12-09T20:00:39.734+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO ShutdownHookManager: Deleting directory /tmp/spark-e4d45da3-19dc-4870-aa7a-566ca7d7d0b9
[2025-12-09T20:00:39.749+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO MetricsSystemImpl: Stopping s3a-file-system metrics system...
[2025-12-09T20:00:39.750+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO MetricsSystemImpl: s3a-file-system metrics system stopped.
[2025-12-09T20:00:39.750+0000] {spark_submit.py:491} INFO - 25/12/09 20:00:39 INFO MetricsSystemImpl: s3a-file-system metrics system shutdown complete.
[2025-12-09T20:00:39.807+0000] {taskinstance.py:1398} INFO - Marking task as SUCCESS. dag_id=dag_create_hudi_tables, task_id=python_job, execution_date=20251208T000000, start_date=20251209T195958, end_date=20251209T200039
[2025-12-09T20:00:39.836+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-12-09T20:00:39.853+0000] {taskinstance.py:2776} INFO - 1 downstream tasks scheduled from follow-on schedule check
